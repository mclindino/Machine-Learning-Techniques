{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "\n",
    "def preprocessing(dataset, _predictors, _class):\n",
    "    \n",
    "    if dataset == 'census':\n",
    "        le = LabelEncoder()\n",
    "        _predictors[:,1] = le.fit_transform(_predictors[:,1])\n",
    "        _predictors[:,3] = le.fit_transform(_predictors[:,3])\n",
    "        _predictors[:,5] = le.fit_transform(_predictors[:,5])\n",
    "        _predictors[:,6] = le.fit_transform(_predictors[:,6])\n",
    "        _predictors[:,7] = le.fit_transform(_predictors[:,7])\n",
    "        _predictors[:,8] = le.fit_transform(_predictors[:,8])\n",
    "        _predictors[:,9] = le.fit_transform(_predictors[:,9])\n",
    "        _predictors[:,13] = le.fit_transform(_predictors[:,13])\n",
    "        \n",
    "        ct = ColumnTransformer([('one_hot_encoder', OneHotEncoder(categories='auto'), \n",
    "                                             [1,3,5,6,7,8,9,13])],   \n",
    "                                             remainder='passthrough')\n",
    "        \n",
    "        _predictors = ct.fit_transform(_predictors).toarray()\n",
    "        _class = le.fit_transform(_class)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        _predictors = scaler.fit_transform(_predictors)\n",
    "        \n",
    "    if dataset == 'credit_data':\n",
    "        scaler = StandardScaler()\n",
    "        _predictors = scaler.fit_transform(_predictors)\n",
    "        \n",
    "    return _predictors, _class\n",
    "\n",
    "def data_acquisition(dataset):\n",
    "    base = pd.read_csv('../dataset/' + dataset + '.csv')\n",
    "    \n",
    "    if dataset == 'census':\n",
    "        _predictors = base.iloc[:, 0:14].values\n",
    "        _class = base.iloc[:, 14].values\n",
    "    \n",
    "    if dataset == 'credit_data':\n",
    "        base.loc[base.age < 0, 'age'] = base['age'][base.age > 0].mean()\n",
    "        base.fillna(base.mean(), inplace=True)\n",
    "        \n",
    "        _predictors = base.iloc[:, 1:4].values\n",
    "        _class = base.iloc[:, 4].values\n",
    "        \n",
    "    return preprocessing(dataset, _predictors, _class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def dim_reduction(method, _predictors, _class=None, n_components=6, kernel=None):\n",
    "    if method == 'PCA':\n",
    "        pca = decomposition.PCA(n_components = n_components)\n",
    "        return pca.fit_transform(_predictors)\n",
    "    \n",
    "    elif method == 'KernelPCA':\n",
    "        kpca = KernlePCA(n_components = n_components, kernel = kernel)\n",
    "        return kpca.fit_transform(_predictors)\n",
    "    \n",
    "    elif method == 'LDA':\n",
    "        lda = LinearDiscriminantAnalysis(n_components = n_components)\n",
    "        return lda.fit_transform(_predictors, _class)\n",
    "    \n",
    "    else:\n",
    "        return _predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset split and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, _predictors, _class, split='nomal', test_size=None, random_state=0):\n",
    "    \n",
    "    if split == 'normal':\n",
    "        \n",
    "        _predictors_train, _predictors_test, _class_train, _class_test = model_selection.train_test_split(_predictors, \n",
    "                                                                                          _class, \n",
    "                                                                                          test_size=test_size, \n",
    "                                                                                          random_state=random_state)\n",
    "        model.fit(_predictors_train, _class_train)\n",
    "        predictions = model.predict(_predictors_test)\n",
    "        accuracy = accuracy_score(_class_test, predictions)\n",
    "        matrix = confusion_matrix(_class_test, predictions)\n",
    "        \n",
    "        return accuracy #, matrix\n",
    "\n",
    "    if split == 'cross_validation':\n",
    "        accuracy = model_selection.cross_val_score(model, _predictors, _class, cv = 10)\n",
    "        \n",
    "        return accuracy.mean()\n",
    "    \n",
    "    if split == 'stratified_fold':\n",
    "        kfold = model_selection.StratifiedKFold(n_splits = 10, shuffle = True, random_state = random_state)\n",
    "\n",
    "        accuracy = []\n",
    "        matrices = []\n",
    "\n",
    "        for train, test in kfold.split(_predictors, np.zeros(shape=(_predictors.shape[0], 1))):\n",
    "\n",
    "            model.fit(_predictors[train], _class[train])\n",
    "            predictions = model.predict(_predictors[test])\n",
    "            accuracy.append(accuracy_score(_class[test], predictions))\n",
    "            matrices.append(confusion_matrix(_class[test], predictions))\n",
    "\n",
    "        return np.asarray(accuracy).mean() #, np.mean(matrices, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_predictors, _class = data_acquisition('credit_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ZeroR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "counter = collections.Counter(_class)\n",
    "base_line = counter[0] / (counter[0] + counter[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "Naive_Bayes = GaussianNB()\n",
    "Naive_Bayes_accuracy = train(Naive_Bayes, _predictors, _class, split = 'cross_validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "Decision_Tree = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "Decision_Tree_accuracy = train(Decision_Tree, _predictors, _class, split = 'cross_validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "Random_Forest = RandomForestClassifier(n_estimators = 40, criterion = 'entropy', random_state = 0)\n",
    "Random_Forest_accuracy = train(Random_Forest, _predictors, _class, split = 'cross_validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "KNN_accuracy = train(KNN, _predictors, _class, split = 'cross_validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "Logistic_Regression = LogisticRegression(random_state = 0)\n",
    "Logistic_Regression_accuracy = train(Logistic_Regression, _predictors, _class, split = 'cross_validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "SVM = SVC(kernel = 'linear', random_state = 0)\n",
    "SVM_accuracy = train(SVM, _predictors, _class, split = 'cross_validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "MLP = MLPClassifier(verbose = False, \n",
    "                    max_iter = 2000, \n",
    "                    tol = 0.000010, \n",
    "                    solver = 'adam', \n",
    "                    hidden_layer_sizes = (100),\n",
    "                    activation = 'relu')\n",
    "MLP_accuracy = train(MLP, _predictors, _class, split = 'normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mBase Line\u001b[0;0m: \u001b[1;31m0.8585%\u001b[0;0m\n",
      "\u001b[1mNaive Bayes\u001b[0;0m: \u001b[1;31m0.9240%\u001b[0;0m\n",
      "\u001b[1mDecision Tree\u001b[0;0m: \u001b[1;31m0.9870%\u001b[0;0m\n",
      "\u001b[1mRandom Forest\u001b[0;0m: \u001b[1;31m0.9865%\u001b[0;0m\n",
      "\u001b[1mKNN\u001b[0;0m: \u001b[1;31m0.9800%\u001b[0;0m\n",
      "\u001b[1mLogistic Regression\u001b[0;0m: \u001b[1;31m0.9480%\u001b[0;0m\n",
      "\u001b[1mSVM\u001b[0;0m: \u001b[1;31m0.9475%\u001b[0;0m\n",
      "\u001b[1mMLP\u001b[0;0m: \u001b[1;31m0.9960%\u001b[0;0m\n"
     ]
    }
   ],
   "source": [
    "RED = \"\\033[1;31m\" \n",
    "RESET = '\\033[0;0m'\n",
    "NEGRITO = '\\033[1m'\n",
    "models = {'Base Line': base_line,'Naive Bayes': Naive_Bayes_accuracy, 'Decision Tree': Decision_Tree_accuracy,\n",
    "          'Random Forest': Random_Forest_accuracy, 'KNN': KNN_accuracy, 'Logistic Regression': Logistic_Regression_accuracy,\n",
    "          'SVM': SVM_accuracy, 'MLP': MLP_accuracy}\n",
    "\n",
    "for model in models:\n",
    "    print('{}{}{}: {}{:.4f}%{}'.format(NEGRITO,model,RESET,RED,models[model], RESET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

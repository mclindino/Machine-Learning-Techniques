{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "credit_data = pd.read_csv('../dataset/credit_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localizar problemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Localizar \n",
    "credit_data.loc[credit_data['age'] < 0]\n",
    "credit_data.loc[pd.isnull(credit_data['age'])]\n",
    "\n",
    "\n",
    "## Preencher os valores com a média\n",
    "credit_data.loc[credit_data.age < 0, 'age'] = credit_data['age'][credit_data.age > 0].mean()\n",
    "credit_data.fillna(credit_data.mean(), inplace=True)\n",
    "\n",
    "previsores = credit_data.iloc[:, 1:4].values\n",
    "classe = credit_data.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padronizar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "previsores = scaler.fit_transform(previsores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "def dataset_division(previsores, classse):\n",
    "    previsores_train, previsores_test, classe_train, classe_test = train_test_split(previsores, classe, test_size=0.25, random_state=0)\n",
    "    return previsores_train, previsores_test, classe_train, classe_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def cross_validation(model, previsores, classe):\n",
    "    \n",
    "    resultado = cross_val_score(Naive_Bayes, previsores, classe, cv = 10)\n",
    "    return resultado.mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação Cruzada Stratified Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def stratified_fold(model, previsores, classe, seed):\n",
    "    a = np.zeros(5)\n",
    "    b = np.zeros(shape=(previsores.shape[0], 1))\n",
    "\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = seed)\n",
    "\n",
    "    resultados = []\n",
    "    matrizes = []\n",
    "    for indice_treinamento, indice_teste in kfold.split(previsores, np.zeros(shape=(previsores.shape[0], 1))):\n",
    "\n",
    "        model.fit(previsores[indice_treinamento], classe[indice_treinamento])\n",
    "        previsoes = model.predict(previsores[indice_teste])\n",
    "        precisao = accuracy_score(classe[indice_teste], previsoes)\n",
    "        matrizes.append(confusion_matrix(classe[indice_teste], previsoes))\n",
    "        resultados.append(precisao)\n",
    "\n",
    "    matriz_final = np.mean(matrizes, axis = 0)\n",
    "    resultados = np.asarray(resultados)\n",
    "    \n",
    "    return resultados.mean(), matriz_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_model(model, name):\n",
    "    pickle.dump(model, open('../save-models/' + name + '.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name):\n",
    "    return pickle.load(open('../save-models/' + name + '.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes -> Tabela de Probabilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "Naive_Bayes = GaussianNB()\n",
    "\n",
    "##Divisao normal do dataset\n",
    "\n",
    "'''\n",
    "previsores_train, previsores_test, classe_train, classe_test = dataset_division(previsores, classe)\n",
    "Naive_Bayes.fit(previsores_train, classe_train)\n",
    "previsoes = Naive_Bayes.predict(previsores_test)\n",
    "Naive_Bayes_accuracy = accuracy_score(classe_test, previsoes)\n",
    "matriz = confusion_matrix(classe_test, previsoes)\n",
    "'''\n",
    "\n",
    "##Cross Validation\n",
    "'''\n",
    "Naive_Bayes_accuracy = cross_validation(Naive_Bayes, previsores, classe)\n",
    "'''\n",
    "\n",
    "##Stratified Fold\n",
    "Naive_Bayes_accuracy, _ = stratified_fold(Naive_Bayes, previsores, classe, 0)\n",
    "save_model(Naive_Bayes, 'Naive_Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árvore de Decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "Decision_Tree = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "\n",
    "##Divisao normal do dataset\n",
    "\n",
    "'''\n",
    "previsores_train, previsores_test, classe_train, classe_test = dataset_division(previsores, classe)\n",
    "Decision_Tree.fit(previsores_train, classe_train)\n",
    "previsoes = Decision_Tree.predict(previsores_test)\n",
    "Decision_Tree_accuracy = accuracy_score(classe_test, previsoes)\n",
    "matriz = confusion_matrix(classe_test, previsoes)\n",
    "'''\n",
    "\n",
    "##Cross Validation\n",
    "'''\n",
    "Decision_Tree_accuracy = cross_validation(Decision_Tree, previsores, classe)\n",
    "'''\n",
    "\n",
    "##Stratified Fold\n",
    "Decision_Tree_accuracy, _ = stratified_fold(Decision_Tree, previsores, classe, 0)\n",
    "save_model(Decision_Tree, 'Decision_Tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "Random_Forest = RandomForestClassifier(n_estimators = 40,\n",
    "                                      criterion = 'entropy',\n",
    "                                      random_state = 0)\n",
    "\n",
    "##Divisao normal do dataset\n",
    "\n",
    "'''\n",
    "previsores_train, previsores_test, classe_train, classe_test = dataset_division(previsores, classe)\n",
    "Random_Forest.fit(previsores_train, classe_train)\n",
    "previsoes = Random_Forest.predict(previsores_test)\n",
    "Random_Forest_accuracy = accuracy_score(classe_test, previsoes)\n",
    "matriz = confusion_matrix(classe_test, previsoes)\n",
    "'''\n",
    "\n",
    "##Cross Validation\n",
    "'''\n",
    "Random_Forest_accuracy = cross_validation(Random_Forest, previsores, classe)\n",
    "'''\n",
    "\n",
    "##Stratified Fold\n",
    "Random_Forest_accuracy, _ = stratified_fold(Random_Forest, previsores, classe, 0)\n",
    "save_model(Random_Forest, 'Random_Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regras -> CN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[income, age, loan | default]\n",
      "IF age>=34.9966813726351 THEN default=0 \n",
      "IF loan<=2495.13299137587 AND income>=20145.9885970689 THEN default=0 \n",
      "IF income<=34145.7995516248 AND loan>=4096.783714405129 THEN default=1 \n",
      "IF loan>=7708.31562497011 AND loan>=9698.582169129 THEN default=1 \n",
      "IF loan>=7708.31562497011 AND loan>=9595.28628892989 THEN default=0 \n",
      "IF loan>=7708.31562497011 AND age>=20.9909665295854 THEN default=1 \n",
      "IF income>=58132.4712652713 AND age>=20.3008601283655 THEN default=0 \n",
      "IF income<=22089.8374845274 AND age>=23.238169600255798 THEN default=1 \n",
      "IF loan<=3293.25087871026 AND income>=26617.0303151011 THEN default=0 \n",
      "IF age<=18.4162362311035 AND income>=42522.5757574663 THEN default=0 \n",
      "IF loan<=5836.56338145928 AND loan>=5836.56338145928 THEN default=1 \n",
      "IF loan<=5862.83302915672 AND loan>=5862.83302915672 THEN default=1 \n",
      "IF loan<=5883.660557890999 AND income>=45360.716274008206 THEN default=0 \n",
      "IF loan>=4849.33378471958 AND age>=26.854012909811 THEN default=1 \n",
      "IF loan>=4849.33378471958 AND loan>=7708.31562497011 THEN default=1 \n",
      "IF income>=57787.565658800304 THEN default=0 \n",
      "IF loan>=4849.33378471958 AND loan>=6289.25607587104 THEN default=1 \n",
      "IF loan>=4849.33378471958 AND income>=52841.5164374746 THEN default=0 \n",
      "IF income>=44471.873731818094 THEN default=1 \n",
      "IF income>=29775.1422203146 AND age>=27.217440214008104 THEN default=0 \n",
      "IF income>=40496.2558229454 THEN default=0 \n",
      "IF loan>=4625.19337762744 THEN default=1 \n",
      "IF income>=29775.1422203146 THEN default=0 \n",
      "IF income>=25363.331059925298 THEN default=1 \n",
      "IF income>=22925.8120805025 THEN default=0 \n",
      "IF income>=22800.796774681698 THEN default=1 \n",
      "IF income<=22089.8374845274 THEN default=0 \n",
      "IF TRUE THEN default=0 \n"
     ]
    }
   ],
   "source": [
    "import Orange\n",
    "\n",
    "base = Orange.data.Table('../dataset/credit_data.csv')\n",
    "print(base.domain)\n",
    "\n",
    "base_dividida = Orange.evaluation.testing.sample(base, n=0.25)\n",
    "base_treinamento = base_dividida[1]\n",
    "base_teste = base_dividida[0]\n",
    "\n",
    "cn2_learner = Orange.classification.rules.CN2Learner()\n",
    "classificador = cn2_learner(base_treinamento)\n",
    "\n",
    "for regras in classificador.rule_list:\n",
    "    print(regras)\n",
    "    \n",
    "resultado = Orange.evaluation.testing.TestOnTestData(base_treinamento, base_teste, [classificador])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[income, age, loan | default]\n",
      "Acurácia: 0.86%\n"
     ]
    }
   ],
   "source": [
    "import Orange\n",
    "\n",
    "base = Orange.data.Table('../dataset/credit_data.csv')\n",
    "print(base.domain)\n",
    "\n",
    "base_dividida = Orange.evaluation.testing.sample(base, n=0.25)\n",
    "base_treinamento = base_dividida[1]\n",
    "base_teste = base_dividida[0]\n",
    "\n",
    "classificador = Orange.classification.MajorityLearner()\n",
    "resultado = Orange.evaluation.testing.TestOnTestData(base_treinamento, base_teste, [classificador])\n",
    "print('Acurácia: %.2f%%' % Orange.evaluation.CA(resultado))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Line Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## O minimo que o modelo necessita aprender\n",
    "\n",
    "import collections\n",
    "previsores_train, previsores_test, classe_train, classe_test = dataset_division(previsores, classe)\n",
    "counter = collections.Counter(classe_test)\n",
    "base_line = counter[0] / (counter[0] + counter[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "\n",
    "##Divisao normal do dataset\n",
    "\n",
    "'''\n",
    "previsores_train, previsores_test, classe_train, classe_test = dataset_division(previsores, classe)\n",
    "KNN.fit(previsores_train, classe_train)\n",
    "previsoes = KNN.predict(previsores_test)\n",
    "KNN_accuracy = accuracy_score(classe_test, previsoes)\n",
    "matriz = confusion_matrix(classe_test, previsoes)\n",
    "'''\n",
    "\n",
    "##Cross Validation\n",
    "'''\n",
    "KNN_accuracy = cross_validation(KNN, previsores, classe)\n",
    "'''\n",
    "\n",
    "##Stratified Fold\n",
    "KNN_accuracy, _ = stratified_fold(KNN, previsores, classe, 0)\n",
    "save_model(KNN, 'KNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "Logistic_Regression = LogisticRegression(random_state = 0)\n",
    "\n",
    "##Divisao normal do dataset\n",
    "\n",
    "'''\n",
    "previsores_train, previsores_test, classe_train, classe_test = dataset_division(previsores, classe)\n",
    "Logistic_Regression.fit(previsores_train, classe_train)\n",
    "previsoes = Logistic_Regression.predict(previsores_test)\n",
    "Logistic_Regression_accuracy = accuracy_score(classe_test, previsoes)\n",
    "matriz = confusion_matrix(classe_test, previsoes)\n",
    "'''\n",
    "\n",
    "##Cross Validation\n",
    "'''\n",
    "Logistic_Regression_accuracy = cross_validation(Logistic_Regression, previsores, classe)\n",
    "'''\n",
    "\n",
    "##Stratified Fold\n",
    "Logistic_Regression_accuracy, _ = stratified_fold(Logistic_Regression, previsores, classe, 0)\n",
    "save_model(Logistic_Regression, 'Logistic_Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "SVM = SVC(kernel = 'rbf', random_state = 0, C = 2.0, probability = True)\n",
    "\n",
    "##Divisao normal do dataset\n",
    "\n",
    "'''\n",
    "previsores_train, previsores_test, classe_train, classe_test = dataset_division(previsores, classe)\n",
    "SVM.fit(previsores_train, classe_train)\n",
    "previsoes = SVM.predict(previsores_test)\n",
    "SVM_accuracy = accuracy_score(classe_test, previsoes)\n",
    "matriz = confusion_matrix(classe_test, previsoes)\n",
    "'''\n",
    "\n",
    "##Cross Validation\n",
    "'''\n",
    "SVM_accuracy = cross_validation(SVM, previsores, classe)\n",
    "'''\n",
    "\n",
    "##Stratified Fold\n",
    "SVM_accuracy, _ = stratified_fold(SVM, previsores, classe, 0)\n",
    "save_model(SVM, 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neurais com SkLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71882549\n",
      "Iteration 2, loss = 0.64142052\n",
      "Iteration 3, loss = 0.57786543\n",
      "Iteration 4, loss = 0.52504032\n",
      "Iteration 5, loss = 0.48107012\n",
      "Iteration 6, loss = 0.44368727\n",
      "Iteration 7, loss = 0.41148265\n",
      "Iteration 8, loss = 0.38262606\n",
      "Iteration 9, loss = 0.35717871\n",
      "Iteration 10, loss = 0.33413371\n",
      "Iteration 11, loss = 0.31346916\n",
      "Iteration 12, loss = 0.29456589\n",
      "Iteration 13, loss = 0.27720247\n",
      "Iteration 14, loss = 0.26148111\n",
      "Iteration 15, loss = 0.24726715\n",
      "Iteration 16, loss = 0.23406291\n",
      "Iteration 17, loss = 0.22243152\n",
      "Iteration 18, loss = 0.21138011\n",
      "Iteration 19, loss = 0.20172439\n",
      "Iteration 20, loss = 0.19274245\n",
      "Iteration 21, loss = 0.18443426\n",
      "Iteration 22, loss = 0.17709882\n",
      "Iteration 23, loss = 0.17033604\n",
      "Iteration 24, loss = 0.16407959\n",
      "Iteration 25, loss = 0.15832050\n",
      "Iteration 26, loss = 0.15305272\n",
      "Iteration 27, loss = 0.14837154\n",
      "Iteration 28, loss = 0.14387421\n",
      "Iteration 29, loss = 0.13984719\n",
      "Iteration 30, loss = 0.13610756\n",
      "Iteration 31, loss = 0.13260383\n",
      "Iteration 32, loss = 0.12945200\n",
      "Iteration 33, loss = 0.12636107\n",
      "Iteration 34, loss = 0.12353403\n",
      "Iteration 35, loss = 0.12087691\n",
      "Iteration 36, loss = 0.11838616\n",
      "Iteration 37, loss = 0.11604978\n",
      "Iteration 38, loss = 0.11379298\n",
      "Iteration 39, loss = 0.11173062\n",
      "Iteration 40, loss = 0.10963878\n",
      "Iteration 41, loss = 0.10779800\n",
      "Iteration 42, loss = 0.10598566\n",
      "Iteration 43, loss = 0.10421720\n",
      "Iteration 44, loss = 0.10254756\n",
      "Iteration 45, loss = 0.10091683\n",
      "Iteration 46, loss = 0.09936832\n",
      "Iteration 47, loss = 0.09789754\n",
      "Iteration 48, loss = 0.09664535\n",
      "Iteration 49, loss = 0.09510937\n",
      "Iteration 50, loss = 0.09381461\n",
      "Iteration 51, loss = 0.09259235\n",
      "Iteration 52, loss = 0.09127592\n",
      "Iteration 53, loss = 0.09017122\n",
      "Iteration 54, loss = 0.08890394\n",
      "Iteration 55, loss = 0.08777739\n",
      "Iteration 56, loss = 0.08673432\n",
      "Iteration 57, loss = 0.08562080\n",
      "Iteration 58, loss = 0.08457845\n",
      "Iteration 59, loss = 0.08364402\n",
      "Iteration 60, loss = 0.08278784\n",
      "Iteration 61, loss = 0.08183126\n",
      "Iteration 62, loss = 0.08089567\n",
      "Iteration 63, loss = 0.08018102\n",
      "Iteration 64, loss = 0.07916827\n",
      "Iteration 65, loss = 0.07834610\n",
      "Iteration 66, loss = 0.07784964\n",
      "Iteration 67, loss = 0.07704137\n",
      "Iteration 68, loss = 0.07605278\n",
      "Iteration 69, loss = 0.07522009\n",
      "Iteration 70, loss = 0.07455463\n",
      "Iteration 71, loss = 0.07376613\n",
      "Iteration 72, loss = 0.07309023\n",
      "Iteration 73, loss = 0.07242746\n",
      "Iteration 74, loss = 0.07172423\n",
      "Iteration 75, loss = 0.07100140\n",
      "Iteration 76, loss = 0.07037654\n",
      "Iteration 77, loss = 0.06968764\n",
      "Iteration 78, loss = 0.06906679\n",
      "Iteration 79, loss = 0.06841240\n",
      "Iteration 80, loss = 0.06776775\n",
      "Iteration 81, loss = 0.06715627\n",
      "Iteration 82, loss = 0.06659077\n",
      "Iteration 83, loss = 0.06601241\n",
      "Iteration 84, loss = 0.06541191\n",
      "Iteration 85, loss = 0.06491005\n",
      "Iteration 86, loss = 0.06420409\n",
      "Iteration 87, loss = 0.06366764\n",
      "Iteration 88, loss = 0.06324558\n",
      "Iteration 89, loss = 0.06262147\n",
      "Iteration 90, loss = 0.06209924\n",
      "Iteration 91, loss = 0.06148496\n",
      "Iteration 92, loss = 0.06101727\n",
      "Iteration 93, loss = 0.06046733\n",
      "Iteration 94, loss = 0.06000560\n",
      "Iteration 95, loss = 0.05946073\n",
      "Iteration 96, loss = 0.05897507\n",
      "Iteration 97, loss = 0.05848173\n",
      "Iteration 98, loss = 0.05801186\n",
      "Iteration 99, loss = 0.05760811\n",
      "Iteration 100, loss = 0.05716854\n",
      "Iteration 101, loss = 0.05659263\n",
      "Iteration 102, loss = 0.05616990\n",
      "Iteration 103, loss = 0.05566211\n",
      "Iteration 104, loss = 0.05519186\n",
      "Iteration 105, loss = 0.05475970\n",
      "Iteration 106, loss = 0.05438152\n",
      "Iteration 107, loss = 0.05382697\n",
      "Iteration 108, loss = 0.05343896\n",
      "Iteration 109, loss = 0.05303214\n",
      "Iteration 110, loss = 0.05260922\n",
      "Iteration 111, loss = 0.05215232\n",
      "Iteration 112, loss = 0.05178518\n",
      "Iteration 113, loss = 0.05137649\n",
      "Iteration 114, loss = 0.05092501\n",
      "Iteration 115, loss = 0.05065527\n",
      "Iteration 116, loss = 0.05025159\n",
      "Iteration 117, loss = 0.04977990\n",
      "Iteration 118, loss = 0.04939722\n",
      "Iteration 119, loss = 0.04897105\n",
      "Iteration 120, loss = 0.04863796\n",
      "Iteration 121, loss = 0.04841138\n",
      "Iteration 122, loss = 0.04793102\n",
      "Iteration 123, loss = 0.04760442\n",
      "Iteration 124, loss = 0.04725207\n",
      "Iteration 125, loss = 0.04692880\n",
      "Iteration 126, loss = 0.04657491\n",
      "Iteration 127, loss = 0.04622745\n",
      "Iteration 128, loss = 0.04595358\n",
      "Iteration 129, loss = 0.04556806\n",
      "Iteration 130, loss = 0.04524807\n",
      "Iteration 131, loss = 0.04510124\n",
      "Iteration 132, loss = 0.04461949\n",
      "Iteration 133, loss = 0.04431287\n",
      "Iteration 134, loss = 0.04408134\n",
      "Iteration 135, loss = 0.04373611\n",
      "Iteration 136, loss = 0.04358499\n",
      "Iteration 137, loss = 0.04314587\n",
      "Iteration 138, loss = 0.04284012\n",
      "Iteration 139, loss = 0.04254739\n",
      "Iteration 140, loss = 0.04225673\n",
      "Iteration 141, loss = 0.04203770\n",
      "Iteration 142, loss = 0.04169898\n",
      "Iteration 143, loss = 0.04157737\n",
      "Iteration 144, loss = 0.04118828\n",
      "Iteration 145, loss = 0.04089781\n",
      "Iteration 146, loss = 0.04072443\n",
      "Iteration 147, loss = 0.04044773\n",
      "Iteration 148, loss = 0.04012751\n",
      "Iteration 149, loss = 0.04011379\n",
      "Iteration 150, loss = 0.03962595\n",
      "Iteration 151, loss = 0.03934130\n",
      "Iteration 152, loss = 0.03915731\n",
      "Iteration 153, loss = 0.03895651\n",
      "Iteration 154, loss = 0.03875797\n",
      "Iteration 155, loss = 0.03845550\n",
      "Iteration 156, loss = 0.03822867\n",
      "Iteration 157, loss = 0.03798099\n",
      "Iteration 158, loss = 0.03775848\n",
      "Iteration 159, loss = 0.03766563\n",
      "Iteration 160, loss = 0.03732118\n",
      "Iteration 161, loss = 0.03708902\n",
      "Iteration 162, loss = 0.03684339\n",
      "Iteration 163, loss = 0.03670112\n",
      "Iteration 164, loss = 0.03648273\n",
      "Iteration 165, loss = 0.03621517\n",
      "Iteration 166, loss = 0.03600552\n",
      "Iteration 167, loss = 0.03579194\n",
      "Iteration 168, loss = 0.03564146\n",
      "Iteration 169, loss = 0.03541617\n",
      "Iteration 170, loss = 0.03520950\n",
      "Iteration 171, loss = 0.03507206\n",
      "Iteration 172, loss = 0.03504274\n",
      "Iteration 173, loss = 0.03469834\n",
      "Iteration 174, loss = 0.03454590\n",
      "Iteration 175, loss = 0.03427894\n",
      "Iteration 176, loss = 0.03409771\n",
      "Iteration 177, loss = 0.03392398\n",
      "Iteration 178, loss = 0.03382240\n",
      "Iteration 179, loss = 0.03358390\n",
      "Iteration 180, loss = 0.03343295\n",
      "Iteration 181, loss = 0.03322341\n",
      "Iteration 182, loss = 0.03305689\n",
      "Iteration 183, loss = 0.03290306\n",
      "Iteration 184, loss = 0.03275748\n",
      "Iteration 185, loss = 0.03255414\n",
      "Iteration 186, loss = 0.03245308\n",
      "Iteration 187, loss = 0.03228629\n",
      "Iteration 188, loss = 0.03205419\n",
      "Iteration 189, loss = 0.03196053\n",
      "Iteration 190, loss = 0.03175172\n",
      "Iteration 191, loss = 0.03160903\n",
      "Iteration 192, loss = 0.03142472\n",
      "Iteration 193, loss = 0.03133482\n",
      "Iteration 194, loss = 0.03109398\n",
      "Iteration 195, loss = 0.03105176\n",
      "Iteration 196, loss = 0.03087139\n",
      "Iteration 197, loss = 0.03067639\n",
      "Iteration 198, loss = 0.03059804\n",
      "Iteration 199, loss = 0.03038898\n",
      "Iteration 200, loss = 0.03025858\n",
      "Iteration 201, loss = 0.03011218\n",
      "Iteration 202, loss = 0.02995029\n",
      "Iteration 203, loss = 0.02983086\n",
      "Iteration 204, loss = 0.02969175\n",
      "Iteration 205, loss = 0.02957695\n",
      "Iteration 206, loss = 0.02944236\n",
      "Iteration 207, loss = 0.02926475\n",
      "Iteration 208, loss = 0.02913041\n",
      "Iteration 209, loss = 0.02904317\n",
      "Iteration 210, loss = 0.02886298\n",
      "Iteration 211, loss = 0.02881208\n",
      "Iteration 212, loss = 0.02870307\n",
      "Iteration 213, loss = 0.02849662\n",
      "Iteration 214, loss = 0.02848847\n",
      "Iteration 215, loss = 0.02838000\n",
      "Iteration 216, loss = 0.02804116\n",
      "Iteration 217, loss = 0.02798089\n",
      "Iteration 218, loss = 0.02790730\n",
      "Iteration 219, loss = 0.02786989\n",
      "Iteration 220, loss = 0.02765714\n",
      "Iteration 221, loss = 0.02753018\n",
      "Iteration 222, loss = 0.02736942\n",
      "Iteration 223, loss = 0.02725669\n",
      "Iteration 224, loss = 0.02712914\n",
      "Iteration 225, loss = 0.02702922\n",
      "Iteration 226, loss = 0.02689493\n",
      "Iteration 227, loss = 0.02678808\n",
      "Iteration 228, loss = 0.02669799\n",
      "Iteration 229, loss = 0.02659190\n",
      "Iteration 230, loss = 0.02652339\n",
      "Iteration 231, loss = 0.02636982\n",
      "Iteration 232, loss = 0.02624820\n",
      "Iteration 233, loss = 0.02619182\n",
      "Iteration 234, loss = 0.02606436\n",
      "Iteration 235, loss = 0.02603467\n",
      "Iteration 236, loss = 0.02587812\n",
      "Iteration 237, loss = 0.02577194\n",
      "Iteration 238, loss = 0.02565208\n",
      "Iteration 239, loss = 0.02572982\n",
      "Iteration 240, loss = 0.02537849\n",
      "Iteration 241, loss = 0.02535838\n",
      "Iteration 242, loss = 0.02529299\n",
      "Iteration 243, loss = 0.02509937\n",
      "Iteration 244, loss = 0.02498024\n",
      "Iteration 245, loss = 0.02499606\n",
      "Iteration 246, loss = 0.02490978\n",
      "Iteration 247, loss = 0.02475354\n",
      "Iteration 248, loss = 0.02469656\n",
      "Iteration 249, loss = 0.02455235\n",
      "Iteration 250, loss = 0.02448232\n",
      "Iteration 251, loss = 0.02438998\n",
      "Iteration 252, loss = 0.02442093\n",
      "Iteration 253, loss = 0.02420434\n",
      "Iteration 254, loss = 0.02406851\n",
      "Iteration 255, loss = 0.02400956\n",
      "Iteration 256, loss = 0.02388940\n",
      "Iteration 257, loss = 0.02389491\n",
      "Iteration 258, loss = 0.02371414\n",
      "Iteration 259, loss = 0.02369797\n",
      "Iteration 260, loss = 0.02356765\n",
      "Iteration 261, loss = 0.02346349\n",
      "Iteration 262, loss = 0.02334008\n",
      "Iteration 263, loss = 0.02326896\n",
      "Iteration 264, loss = 0.02319180\n",
      "Iteration 265, loss = 0.02311925\n",
      "Iteration 266, loss = 0.02301316\n",
      "Iteration 267, loss = 0.02299235\n",
      "Iteration 268, loss = 0.02285869\n",
      "Iteration 269, loss = 0.02273475\n",
      "Iteration 270, loss = 0.02269572\n",
      "Iteration 271, loss = 0.02263577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 272, loss = 0.02256948\n",
      "Iteration 273, loss = 0.02244697\n",
      "Iteration 274, loss = 0.02236782\n",
      "Iteration 275, loss = 0.02235175\n",
      "Iteration 276, loss = 0.02221423\n",
      "Iteration 277, loss = 0.02214499\n",
      "Iteration 278, loss = 0.02205377\n",
      "Iteration 279, loss = 0.02204580\n",
      "Iteration 280, loss = 0.02191850\n",
      "Iteration 281, loss = 0.02189174\n",
      "Iteration 282, loss = 0.02173431\n",
      "Iteration 283, loss = 0.02169737\n",
      "Iteration 284, loss = 0.02162650\n",
      "Iteration 285, loss = 0.02151730\n",
      "Iteration 286, loss = 0.02147292\n",
      "Iteration 287, loss = 0.02141405\n",
      "Iteration 288, loss = 0.02128757\n",
      "Iteration 289, loss = 0.02124430\n",
      "Iteration 290, loss = 0.02114530\n",
      "Iteration 291, loss = 0.02110396\n",
      "Iteration 292, loss = 0.02107927\n",
      "Iteration 293, loss = 0.02095106\n",
      "Iteration 294, loss = 0.02090174\n",
      "Iteration 295, loss = 0.02086652\n",
      "Iteration 296, loss = 0.02074135\n",
      "Iteration 297, loss = 0.02065536\n",
      "Iteration 298, loss = 0.02061586\n",
      "Iteration 299, loss = 0.02053314\n",
      "Iteration 300, loss = 0.02054363\n",
      "Iteration 301, loss = 0.02052272\n",
      "Iteration 302, loss = 0.02035269\n",
      "Iteration 303, loss = 0.02028648\n",
      "Iteration 304, loss = 0.02025648\n",
      "Iteration 305, loss = 0.02014020\n",
      "Iteration 306, loss = 0.02011160\n",
      "Iteration 307, loss = 0.02000996\n",
      "Iteration 308, loss = 0.01998026\n",
      "Iteration 309, loss = 0.01994302\n",
      "Iteration 310, loss = 0.01984126\n",
      "Iteration 311, loss = 0.01991266\n",
      "Iteration 312, loss = 0.01974969\n",
      "Iteration 313, loss = 0.01973821\n",
      "Iteration 314, loss = 0.01964625\n",
      "Iteration 315, loss = 0.01964619\n",
      "Iteration 316, loss = 0.01948392\n",
      "Iteration 317, loss = 0.01939314\n",
      "Iteration 318, loss = 0.01933859\n",
      "Iteration 319, loss = 0.01928852\n",
      "Iteration 320, loss = 0.01930488\n",
      "Iteration 321, loss = 0.01916845\n",
      "Iteration 322, loss = 0.01909375\n",
      "Iteration 323, loss = 0.01902241\n",
      "Iteration 324, loss = 0.01906296\n",
      "Iteration 325, loss = 0.01889764\n",
      "Iteration 326, loss = 0.01886733\n",
      "Iteration 327, loss = 0.01879277\n",
      "Iteration 328, loss = 0.01875654\n",
      "Iteration 329, loss = 0.01879431\n",
      "Iteration 330, loss = 0.01874329\n",
      "Iteration 331, loss = 0.01862670\n",
      "Iteration 332, loss = 0.01855733\n",
      "Iteration 333, loss = 0.01855753\n",
      "Iteration 334, loss = 0.01850663\n",
      "Iteration 335, loss = 0.01844845\n",
      "Iteration 336, loss = 0.01831055\n",
      "Iteration 337, loss = 0.01822754\n",
      "Iteration 338, loss = 0.01823104\n",
      "Iteration 339, loss = 0.01827830\n",
      "Iteration 340, loss = 0.01809807\n",
      "Iteration 341, loss = 0.01800613\n",
      "Iteration 342, loss = 0.01802370\n",
      "Iteration 343, loss = 0.01791057\n",
      "Iteration 344, loss = 0.01798212\n",
      "Iteration 345, loss = 0.01781897\n",
      "Iteration 346, loss = 0.01782130\n",
      "Iteration 347, loss = 0.01783718\n",
      "Iteration 348, loss = 0.01776300\n",
      "Iteration 349, loss = 0.01762173\n",
      "Iteration 350, loss = 0.01756284\n",
      "Iteration 351, loss = 0.01751120\n",
      "Iteration 352, loss = 0.01752554\n",
      "Iteration 353, loss = 0.01738282\n",
      "Iteration 354, loss = 0.01744657\n",
      "Iteration 355, loss = 0.01742541\n",
      "Iteration 356, loss = 0.01734361\n",
      "Iteration 357, loss = 0.01731012\n",
      "Iteration 358, loss = 0.01723917\n",
      "Iteration 359, loss = 0.01712016\n",
      "Iteration 360, loss = 0.01706685\n",
      "Iteration 361, loss = 0.01707517\n",
      "Iteration 362, loss = 0.01698450\n",
      "Iteration 363, loss = 0.01690194\n",
      "Iteration 364, loss = 0.01684754\n",
      "Iteration 365, loss = 0.01681955\n",
      "Iteration 366, loss = 0.01677149\n",
      "Iteration 367, loss = 0.01676419\n",
      "Iteration 368, loss = 0.01668617\n",
      "Iteration 369, loss = 0.01663740\n",
      "Iteration 370, loss = 0.01665799\n",
      "Iteration 371, loss = 0.01658826\n",
      "Iteration 372, loss = 0.01652120\n",
      "Iteration 373, loss = 0.01647601\n",
      "Iteration 374, loss = 0.01638942\n",
      "Iteration 375, loss = 0.01637725\n",
      "Iteration 376, loss = 0.01630525\n",
      "Iteration 377, loss = 0.01647814\n",
      "Iteration 378, loss = 0.01619456\n",
      "Iteration 379, loss = 0.01637658\n",
      "Iteration 380, loss = 0.01624367\n",
      "Iteration 381, loss = 0.01615396\n",
      "Iteration 382, loss = 0.01602941\n",
      "Iteration 383, loss = 0.01608663\n",
      "Iteration 384, loss = 0.01612431\n",
      "Iteration 385, loss = 0.01592562\n",
      "Iteration 386, loss = 0.01600869\n",
      "Iteration 387, loss = 0.01583979\n",
      "Iteration 388, loss = 0.01588952\n",
      "Iteration 389, loss = 0.01613592\n",
      "Iteration 390, loss = 0.01571054\n",
      "Iteration 391, loss = 0.01565879\n",
      "Iteration 392, loss = 0.01561791\n",
      "Iteration 393, loss = 0.01556536\n",
      "Iteration 394, loss = 0.01554107\n",
      "Iteration 395, loss = 0.01552584\n",
      "Iteration 396, loss = 0.01542063\n",
      "Iteration 397, loss = 0.01539762\n",
      "Iteration 398, loss = 0.01542157\n",
      "Iteration 399, loss = 0.01544292\n",
      "Iteration 400, loss = 0.01526049\n",
      "Iteration 401, loss = 0.01525681\n",
      "Iteration 402, loss = 0.01527471\n",
      "Iteration 403, loss = 0.01513217\n",
      "Iteration 404, loss = 0.01516792\n",
      "Iteration 405, loss = 0.01511633\n",
      "Iteration 406, loss = 0.01506574\n",
      "Iteration 407, loss = 0.01507252\n",
      "Iteration 408, loss = 0.01499715\n",
      "Iteration 409, loss = 0.01500708\n",
      "Iteration 410, loss = 0.01487624\n",
      "Iteration 411, loss = 0.01483997\n",
      "Iteration 412, loss = 0.01485040\n",
      "Iteration 413, loss = 0.01473098\n",
      "Iteration 414, loss = 0.01483445\n",
      "Iteration 415, loss = 0.01466705\n",
      "Iteration 416, loss = 0.01469585\n",
      "Iteration 417, loss = 0.01462974\n",
      "Iteration 418, loss = 0.01467046\n",
      "Iteration 419, loss = 0.01454106\n",
      "Iteration 420, loss = 0.01472018\n",
      "Iteration 421, loss = 0.01449418\n",
      "Iteration 422, loss = 0.01448702\n",
      "Iteration 423, loss = 0.01443849\n",
      "Iteration 424, loss = 0.01449683\n",
      "Iteration 425, loss = 0.01450838\n",
      "Iteration 426, loss = 0.01435375\n",
      "Iteration 427, loss = 0.01430976\n",
      "Iteration 428, loss = 0.01435146\n",
      "Iteration 429, loss = 0.01424437\n",
      "Iteration 430, loss = 0.01417630\n",
      "Iteration 431, loss = 0.01415720\n",
      "Iteration 432, loss = 0.01413034\n",
      "Iteration 433, loss = 0.01403447\n",
      "Iteration 434, loss = 0.01401602\n",
      "Iteration 435, loss = 0.01391987\n",
      "Iteration 436, loss = 0.01400121\n",
      "Iteration 437, loss = 0.01389465\n",
      "Iteration 438, loss = 0.01389605\n",
      "Iteration 439, loss = 0.01385689\n",
      "Iteration 440, loss = 0.01382522\n",
      "Iteration 441, loss = 0.01382659\n",
      "Iteration 442, loss = 0.01371295\n",
      "Iteration 443, loss = 0.01372365\n",
      "Iteration 444, loss = 0.01372223\n",
      "Iteration 445, loss = 0.01364011\n",
      "Iteration 446, loss = 0.01360575\n",
      "Iteration 447, loss = 0.01364110\n",
      "Iteration 448, loss = 0.01355621\n",
      "Iteration 449, loss = 0.01348267\n",
      "Iteration 450, loss = 0.01351274\n",
      "Iteration 451, loss = 0.01370545\n",
      "Iteration 452, loss = 0.01348501\n",
      "Iteration 453, loss = 0.01336846\n",
      "Iteration 454, loss = 0.01331069\n",
      "Iteration 455, loss = 0.01334394\n",
      "Iteration 456, loss = 0.01323569\n",
      "Iteration 457, loss = 0.01330486\n",
      "Iteration 458, loss = 0.01324403\n",
      "Iteration 459, loss = 0.01317960\n",
      "Iteration 460, loss = 0.01309919\n",
      "Iteration 461, loss = 0.01327723\n",
      "Iteration 462, loss = 0.01310574\n",
      "Iteration 463, loss = 0.01306344\n",
      "Iteration 464, loss = 0.01303770\n",
      "Iteration 465, loss = 0.01298984\n",
      "Iteration 466, loss = 0.01295730\n",
      "Iteration 467, loss = 0.01294267\n",
      "Iteration 468, loss = 0.01287815\n",
      "Iteration 469, loss = 0.01282643\n",
      "Iteration 470, loss = 0.01285802\n",
      "Iteration 471, loss = 0.01280671\n",
      "Iteration 472, loss = 0.01285178\n",
      "Iteration 473, loss = 0.01271032\n",
      "Iteration 474, loss = 0.01269372\n",
      "Iteration 475, loss = 0.01269881\n",
      "Iteration 476, loss = 0.01267018\n",
      "Iteration 477, loss = 0.01264553\n",
      "Iteration 478, loss = 0.01254763\n",
      "Iteration 479, loss = 0.01270191\n",
      "Iteration 480, loss = 0.01249886\n",
      "Iteration 481, loss = 0.01253655\n",
      "Iteration 482, loss = 0.01246670\n",
      "Iteration 483, loss = 0.01243045\n",
      "Iteration 484, loss = 0.01243148\n",
      "Iteration 485, loss = 0.01247177\n",
      "Iteration 486, loss = 0.01233776\n",
      "Iteration 487, loss = 0.01230776\n",
      "Iteration 488, loss = 0.01267997\n",
      "Iteration 489, loss = 0.01223653\n",
      "Iteration 490, loss = 0.01235654\n",
      "Iteration 491, loss = 0.01227236\n",
      "Iteration 492, loss = 0.01221249\n",
      "Iteration 493, loss = 0.01213837\n",
      "Iteration 494, loss = 0.01207567\n",
      "Iteration 495, loss = 0.01209562\n",
      "Iteration 496, loss = 0.01208097\n",
      "Iteration 497, loss = 0.01200575\n",
      "Iteration 498, loss = 0.01202598\n",
      "Iteration 499, loss = 0.01210677\n",
      "Iteration 500, loss = 0.01204396\n",
      "Iteration 501, loss = 0.01195519\n",
      "Iteration 502, loss = 0.01193771\n",
      "Iteration 503, loss = 0.01188219\n",
      "Iteration 504, loss = 0.01180662\n",
      "Iteration 505, loss = 0.01178896\n",
      "Iteration 506, loss = 0.01177786\n",
      "Iteration 507, loss = 0.01180707\n",
      "Iteration 508, loss = 0.01178407\n",
      "Iteration 509, loss = 0.01172621\n",
      "Iteration 510, loss = 0.01177949\n",
      "Iteration 511, loss = 0.01163055\n",
      "Iteration 512, loss = 0.01162611\n",
      "Iteration 513, loss = 0.01173354\n",
      "Iteration 514, loss = 0.01160937\n",
      "Iteration 515, loss = 0.01175672\n",
      "Iteration 516, loss = 0.01162485\n",
      "Iteration 517, loss = 0.01167536\n",
      "Iteration 518, loss = 0.01144815\n",
      "Iteration 519, loss = 0.01146917\n",
      "Iteration 520, loss = 0.01140527\n",
      "Iteration 521, loss = 0.01137017\n",
      "Iteration 522, loss = 0.01135461\n",
      "Iteration 523, loss = 0.01140381\n",
      "Iteration 524, loss = 0.01136752\n",
      "Iteration 525, loss = 0.01131980\n",
      "Iteration 526, loss = 0.01129210\n",
      "Iteration 527, loss = 0.01123876\n",
      "Iteration 528, loss = 0.01121270\n",
      "Iteration 529, loss = 0.01122616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 530, loss = 0.01113192\n",
      "Iteration 531, loss = 0.01112003\n",
      "Iteration 532, loss = 0.01111650\n",
      "Iteration 533, loss = 0.01106406\n",
      "Iteration 534, loss = 0.01118014\n",
      "Iteration 535, loss = 0.01109338\n",
      "Iteration 536, loss = 0.01104608\n",
      "Iteration 537, loss = 0.01101460\n",
      "Iteration 538, loss = 0.01097842\n",
      "Iteration 539, loss = 0.01112882\n",
      "Iteration 540, loss = 0.01105256\n",
      "Iteration 541, loss = 0.01089900\n",
      "Iteration 542, loss = 0.01093325\n",
      "Iteration 543, loss = 0.01094348\n",
      "Iteration 544, loss = 0.01095696\n",
      "Iteration 545, loss = 0.01081725\n",
      "Iteration 546, loss = 0.01079489\n",
      "Iteration 547, loss = 0.01092867\n",
      "Iteration 548, loss = 0.01072678\n",
      "Iteration 549, loss = 0.01078968\n",
      "Iteration 550, loss = 0.01069220\n",
      "Iteration 551, loss = 0.01080991\n",
      "Iteration 552, loss = 0.01068289\n",
      "Iteration 553, loss = 0.01058236\n",
      "Iteration 554, loss = 0.01067224\n",
      "Iteration 555, loss = 0.01061986\n",
      "Iteration 556, loss = 0.01060231\n",
      "Iteration 557, loss = 0.01052156\n",
      "Iteration 558, loss = 0.01055898\n",
      "Iteration 559, loss = 0.01049462\n",
      "Iteration 560, loss = 0.01049812\n",
      "Iteration 561, loss = 0.01049240\n",
      "Iteration 562, loss = 0.01050359\n",
      "Iteration 563, loss = 0.01040185\n",
      "Iteration 564, loss = 0.01041552\n",
      "Iteration 565, loss = 0.01036240\n",
      "Iteration 566, loss = 0.01030308\n",
      "Iteration 567, loss = 0.01035437\n",
      "Iteration 568, loss = 0.01025934\n",
      "Iteration 569, loss = 0.01037784\n",
      "Iteration 570, loss = 0.01020735\n",
      "Iteration 571, loss = 0.01022624\n",
      "Iteration 572, loss = 0.01042066\n",
      "Iteration 573, loss = 0.01013209\n",
      "Iteration 574, loss = 0.01023527\n",
      "Iteration 575, loss = 0.01016490\n",
      "Iteration 576, loss = 0.01016626\n",
      "Iteration 577, loss = 0.01013785\n",
      "Iteration 578, loss = 0.01013211\n",
      "Iteration 579, loss = 0.00999134\n",
      "Iteration 580, loss = 0.01005039\n",
      "Iteration 581, loss = 0.01006924\n",
      "Iteration 582, loss = 0.01002579\n",
      "Iteration 583, loss = 0.00994208\n",
      "Iteration 584, loss = 0.00991389\n",
      "Iteration 585, loss = 0.01015970\n",
      "Iteration 586, loss = 0.00990303\n",
      "Iteration 587, loss = 0.00989619\n",
      "Iteration 588, loss = 0.00983285\n",
      "Iteration 589, loss = 0.00982703\n",
      "Iteration 590, loss = 0.00981124\n",
      "Iteration 591, loss = 0.00990162\n",
      "Iteration 592, loss = 0.00975902\n",
      "Iteration 593, loss = 0.00973929\n",
      "Iteration 594, loss = 0.00980021\n",
      "Iteration 595, loss = 0.00970256\n",
      "Iteration 596, loss = 0.00978200\n",
      "Iteration 597, loss = 0.00969963\n",
      "Iteration 598, loss = 0.00969701\n",
      "Iteration 599, loss = 0.00961874\n",
      "Iteration 600, loss = 0.00967831\n",
      "Iteration 601, loss = 0.00960652\n",
      "Iteration 602, loss = 0.00956474\n",
      "Iteration 603, loss = 0.00963306\n",
      "Iteration 604, loss = 0.00953408\n",
      "Iteration 605, loss = 0.00951540\n",
      "Iteration 606, loss = 0.00954483\n",
      "Iteration 607, loss = 0.00949363\n",
      "Iteration 608, loss = 0.00948029\n",
      "Iteration 609, loss = 0.00957294\n",
      "Iteration 610, loss = 0.00954866\n",
      "Iteration 611, loss = 0.00946351\n",
      "Iteration 612, loss = 0.00940255\n",
      "Iteration 613, loss = 0.00937222\n",
      "Iteration 614, loss = 0.00942675\n",
      "Iteration 615, loss = 0.00928961\n",
      "Iteration 616, loss = 0.00934980\n",
      "Iteration 617, loss = 0.00925871\n",
      "Iteration 618, loss = 0.00927226\n",
      "Iteration 619, loss = 0.00933707\n",
      "Iteration 620, loss = 0.00924807\n",
      "Iteration 621, loss = 0.00921422\n",
      "Iteration 622, loss = 0.00919772\n",
      "Iteration 623, loss = 0.00925991\n",
      "Iteration 624, loss = 0.00915923\n",
      "Iteration 625, loss = 0.00909771\n",
      "Iteration 626, loss = 0.00915932\n",
      "Iteration 627, loss = 0.00910613\n",
      "Iteration 628, loss = 0.00907896\n",
      "Iteration 629, loss = 0.00914931\n",
      "Iteration 630, loss = 0.00906949\n",
      "Iteration 631, loss = 0.00902799\n",
      "Iteration 632, loss = 0.00902383\n",
      "Iteration 633, loss = 0.00903397\n",
      "Iteration 634, loss = 0.00898044\n",
      "Iteration 635, loss = 0.00900525\n",
      "Iteration 636, loss = 0.00911846\n",
      "Iteration 637, loss = 0.00890573\n",
      "Iteration 638, loss = 0.00893288\n",
      "Iteration 639, loss = 0.00891667\n",
      "Iteration 640, loss = 0.00888737\n",
      "Iteration 641, loss = 0.00883048\n",
      "Iteration 642, loss = 0.00895455\n",
      "Iteration 643, loss = 0.00891833\n",
      "Iteration 644, loss = 0.00883215\n",
      "Iteration 645, loss = 0.00883262\n",
      "Iteration 646, loss = 0.00882895\n",
      "Iteration 647, loss = 0.00878826\n",
      "Iteration 648, loss = 0.00873231\n",
      "Iteration 649, loss = 0.00877247\n",
      "Iteration 650, loss = 0.00870076\n",
      "Iteration 651, loss = 0.00873601\n",
      "Iteration 652, loss = 0.00871157\n",
      "Iteration 653, loss = 0.00867132\n",
      "Iteration 654, loss = 0.00862412\n",
      "Iteration 655, loss = 0.00863949\n",
      "Iteration 656, loss = 0.00864651\n",
      "Iteration 657, loss = 0.00860043\n",
      "Iteration 658, loss = 0.00856263\n",
      "Iteration 659, loss = 0.00856498\n",
      "Iteration 660, loss = 0.00849820\n",
      "Iteration 661, loss = 0.00854403\n",
      "Iteration 662, loss = 0.00859102\n",
      "Iteration 663, loss = 0.00847903\n",
      "Iteration 664, loss = 0.00848640\n",
      "Iteration 665, loss = 0.00844442\n",
      "Iteration 666, loss = 0.00842749\n",
      "Iteration 667, loss = 0.00843896\n",
      "Iteration 668, loss = 0.00855116\n",
      "Iteration 669, loss = 0.00844694\n",
      "Iteration 670, loss = 0.00842628\n",
      "Iteration 671, loss = 0.00835899\n",
      "Iteration 672, loss = 0.00845869\n",
      "Iteration 673, loss = 0.00835019\n",
      "Iteration 674, loss = 0.00831890\n",
      "Iteration 675, loss = 0.00825545\n",
      "Iteration 676, loss = 0.00830070\n",
      "Iteration 677, loss = 0.00826755\n",
      "Iteration 678, loss = 0.00825667\n",
      "Iteration 679, loss = 0.00823698\n",
      "Iteration 680, loss = 0.00822157\n",
      "Iteration 681, loss = 0.00831722\n",
      "Iteration 682, loss = 0.00820705\n",
      "Iteration 683, loss = 0.00836411\n",
      "Iteration 684, loss = 0.00809961\n",
      "Iteration 685, loss = 0.00818790\n",
      "Iteration 686, loss = 0.00812860\n",
      "Iteration 687, loss = 0.00812302\n",
      "Iteration 688, loss = 0.00818006\n",
      "Iteration 689, loss = 0.00811131\n",
      "Iteration 690, loss = 0.00812945\n",
      "Iteration 691, loss = 0.00809622\n",
      "Iteration 692, loss = 0.00818152\n",
      "Iteration 693, loss = 0.00805446\n",
      "Iteration 694, loss = 0.00804819\n",
      "Iteration 695, loss = 0.00803134\n",
      "Iteration 696, loss = 0.00801095\n",
      "Iteration 697, loss = 0.00798473\n",
      "Iteration 698, loss = 0.00797183\n",
      "Iteration 699, loss = 0.00794604\n",
      "Iteration 700, loss = 0.00808896\n",
      "Iteration 701, loss = 0.00791549\n",
      "Iteration 702, loss = 0.00794027\n",
      "Iteration 703, loss = 0.00795547\n",
      "Iteration 704, loss = 0.00790771\n",
      "Iteration 705, loss = 0.00787296\n",
      "Iteration 706, loss = 0.00786910\n",
      "Iteration 707, loss = 0.00787330\n",
      "Iteration 708, loss = 0.00782703\n",
      "Iteration 709, loss = 0.00787940\n",
      "Iteration 710, loss = 0.00786185\n",
      "Iteration 711, loss = 0.00784774\n",
      "Iteration 712, loss = 0.00773104\n",
      "Iteration 713, loss = 0.00781931\n",
      "Iteration 714, loss = 0.00772922\n",
      "Iteration 715, loss = 0.00779939\n",
      "Iteration 716, loss = 0.00771211\n",
      "Iteration 717, loss = 0.00776127\n",
      "Iteration 718, loss = 0.00776634\n",
      "Iteration 719, loss = 0.00768103\n",
      "Iteration 720, loss = 0.00763559\n",
      "Iteration 721, loss = 0.00770271\n",
      "Iteration 722, loss = 0.00764661\n",
      "Iteration 723, loss = 0.00765128\n",
      "Iteration 724, loss = 0.00767358\n",
      "Iteration 725, loss = 0.00757569\n",
      "Iteration 726, loss = 0.00758724\n",
      "Iteration 727, loss = 0.00757404\n",
      "Iteration 728, loss = 0.00755486\n",
      "Iteration 729, loss = 0.00749759\n",
      "Iteration 730, loss = 0.00750282\n",
      "Iteration 731, loss = 0.00747853\n",
      "Iteration 732, loss = 0.00753121\n",
      "Iteration 733, loss = 0.00748123\n",
      "Iteration 734, loss = 0.00751179\n",
      "Iteration 735, loss = 0.00745370\n",
      "Iteration 736, loss = 0.00746189\n",
      "Iteration 737, loss = 0.00739360\n",
      "Iteration 738, loss = 0.00743704\n",
      "Iteration 739, loss = 0.00739173\n",
      "Iteration 740, loss = 0.00744523\n",
      "Iteration 741, loss = 0.00737514\n",
      "Iteration 742, loss = 0.00739118\n",
      "Iteration 743, loss = 0.00734344\n",
      "Iteration 744, loss = 0.00732602\n",
      "Iteration 745, loss = 0.00731289\n",
      "Iteration 746, loss = 0.00740043\n",
      "Iteration 747, loss = 0.00731585\n",
      "Iteration 748, loss = 0.00735352\n",
      "Iteration 749, loss = 0.00732434\n",
      "Iteration 750, loss = 0.00723712\n",
      "Iteration 751, loss = 0.00731882\n",
      "Iteration 752, loss = 0.00731492\n",
      "Iteration 753, loss = 0.00733200\n",
      "Iteration 754, loss = 0.00724243\n",
      "Iteration 755, loss = 0.00716185\n",
      "Iteration 756, loss = 0.00727766\n",
      "Iteration 757, loss = 0.00717220\n",
      "Iteration 758, loss = 0.00713392\n",
      "Iteration 759, loss = 0.00712953\n",
      "Iteration 760, loss = 0.00713966\n",
      "Iteration 761, loss = 0.00717569\n",
      "Iteration 762, loss = 0.00717862\n",
      "Iteration 763, loss = 0.00711906\n",
      "Iteration 764, loss = 0.00711140\n",
      "Iteration 765, loss = 0.00707687\n",
      "Iteration 766, loss = 0.00725216\n",
      "Iteration 767, loss = 0.00710945\n",
      "Iteration 768, loss = 0.00701349\n",
      "Iteration 769, loss = 0.00699115\n",
      "Iteration 770, loss = 0.00706396\n",
      "Iteration 771, loss = 0.00693356\n",
      "Iteration 772, loss = 0.00707525\n",
      "Iteration 773, loss = 0.00706368\n",
      "Iteration 774, loss = 0.00713363\n",
      "Iteration 775, loss = 0.00690745\n",
      "Iteration 776, loss = 0.00692826\n",
      "Iteration 777, loss = 0.00699370\n",
      "Iteration 778, loss = 0.00690988\n",
      "Iteration 779, loss = 0.00689620\n",
      "Iteration 780, loss = 0.00688803\n",
      "Iteration 781, loss = 0.00687455\n",
      "Iteration 782, loss = 0.00689158\n",
      "Iteration 783, loss = 0.00686643\n",
      "Iteration 784, loss = 0.00681733\n",
      "Iteration 785, loss = 0.00682108\n",
      "Iteration 786, loss = 0.00687429\n",
      "Iteration 787, loss = 0.00687702\n",
      "Iteration 788, loss = 0.00676575\n",
      "Iteration 789, loss = 0.00687439\n",
      "Iteration 790, loss = 0.00682032\n",
      "Iteration 791, loss = 0.00675344\n",
      "Iteration 792, loss = 0.00676866\n",
      "Iteration 793, loss = 0.00677186\n",
      "Iteration 794, loss = 0.00672906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 795, loss = 0.00671186\n",
      "Iteration 796, loss = 0.00672254\n",
      "Iteration 797, loss = 0.00667273\n",
      "Iteration 798, loss = 0.00665068\n",
      "Iteration 799, loss = 0.00669683\n",
      "Iteration 800, loss = 0.00664216\n",
      "Iteration 801, loss = 0.00662303\n",
      "Iteration 802, loss = 0.00666457\n",
      "Iteration 803, loss = 0.00664312\n",
      "Iteration 804, loss = 0.00665135\n",
      "Iteration 805, loss = 0.00670280\n",
      "Iteration 806, loss = 0.00664111\n",
      "Iteration 807, loss = 0.00658282\n",
      "Iteration 808, loss = 0.00656939\n",
      "Iteration 809, loss = 0.00669303\n",
      "Iteration 810, loss = 0.00664933\n",
      "Iteration 811, loss = 0.00646729\n",
      "Iteration 812, loss = 0.00658764\n",
      "Iteration 813, loss = 0.00661098\n",
      "Iteration 814, loss = 0.00653287\n",
      "Iteration 815, loss = 0.00654122\n",
      "Iteration 816, loss = 0.00664204\n",
      "Iteration 817, loss = 0.00648257\n",
      "Iteration 818, loss = 0.00644885\n",
      "Iteration 819, loss = 0.00645416\n",
      "Iteration 820, loss = 0.00646338\n",
      "Iteration 821, loss = 0.00646005\n",
      "Iteration 822, loss = 0.00639717\n",
      "Iteration 823, loss = 0.00644279\n",
      "Iteration 824, loss = 0.00637496\n",
      "Iteration 825, loss = 0.00640250\n",
      "Iteration 826, loss = 0.00642550\n",
      "Iteration 827, loss = 0.00634745\n",
      "Iteration 828, loss = 0.00633519\n",
      "Iteration 829, loss = 0.00644235\n",
      "Iteration 830, loss = 0.00644257\n",
      "Iteration 831, loss = 0.00633609\n",
      "Iteration 832, loss = 0.00628936\n",
      "Iteration 833, loss = 0.00632506\n",
      "Iteration 834, loss = 0.00634050\n",
      "Iteration 835, loss = 0.00630042\n",
      "Iteration 836, loss = 0.00630900\n",
      "Iteration 837, loss = 0.00625066\n",
      "Iteration 838, loss = 0.00624556\n",
      "Iteration 839, loss = 0.00632680\n",
      "Iteration 840, loss = 0.00632335\n",
      "Iteration 841, loss = 0.00625923\n",
      "Iteration 842, loss = 0.00629978\n",
      "Iteration 843, loss = 0.00627368\n",
      "Iteration 844, loss = 0.00624077\n",
      "Iteration 845, loss = 0.00624920\n",
      "Iteration 846, loss = 0.00617175\n",
      "Iteration 847, loss = 0.00616758\n",
      "Iteration 848, loss = 0.00627103\n",
      "Iteration 849, loss = 0.00615572\n",
      "Iteration 850, loss = 0.00616445\n",
      "Iteration 851, loss = 0.00611472\n",
      "Iteration 852, loss = 0.00608822\n",
      "Iteration 853, loss = 0.00611616\n",
      "Iteration 854, loss = 0.00605577\n",
      "Iteration 855, loss = 0.00612907\n",
      "Iteration 856, loss = 0.00607080\n",
      "Iteration 857, loss = 0.00604254\n",
      "Iteration 858, loss = 0.00604544\n",
      "Iteration 859, loss = 0.00622262\n",
      "Iteration 860, loss = 0.00604606\n",
      "Iteration 861, loss = 0.00608916\n",
      "Iteration 862, loss = 0.00623170\n",
      "Iteration 863, loss = 0.00613462\n",
      "Iteration 864, loss = 0.00600333\n",
      "Iteration 865, loss = 0.00599750\n",
      "Iteration 866, loss = 0.00599419\n",
      "Iteration 867, loss = 0.00599843\n",
      "Iteration 868, loss = 0.00599831\n",
      "Iteration 869, loss = 0.00601906\n",
      "Iteration 870, loss = 0.00623468\n",
      "Iteration 871, loss = 0.00596619\n",
      "Iteration 872, loss = 0.00595900\n",
      "Iteration 873, loss = 0.00592644\n",
      "Iteration 874, loss = 0.00594132\n",
      "Iteration 875, loss = 0.00592010\n",
      "Iteration 876, loss = 0.00591588\n",
      "Iteration 877, loss = 0.00590796\n",
      "Iteration 878, loss = 0.00587119\n",
      "Iteration 879, loss = 0.00594217\n",
      "Iteration 880, loss = 0.00591050\n",
      "Iteration 881, loss = 0.00589205\n",
      "Iteration 882, loss = 0.00583443\n",
      "Iteration 883, loss = 0.00586644\n",
      "Iteration 884, loss = 0.00579154\n",
      "Iteration 885, loss = 0.00587485\n",
      "Iteration 886, loss = 0.00582418\n",
      "Iteration 887, loss = 0.00578987\n",
      "Iteration 888, loss = 0.00577447\n",
      "Iteration 889, loss = 0.00575051\n",
      "Iteration 890, loss = 0.00579717\n",
      "Iteration 891, loss = 0.00572949\n",
      "Iteration 892, loss = 0.00580877\n",
      "Iteration 893, loss = 0.00571324\n",
      "Iteration 894, loss = 0.00572332\n",
      "Iteration 895, loss = 0.00569112\n",
      "Iteration 896, loss = 0.00574901\n",
      "Iteration 897, loss = 0.00578507\n",
      "Iteration 898, loss = 0.00565669\n",
      "Iteration 899, loss = 0.00570854\n",
      "Iteration 900, loss = 0.00566624\n",
      "Iteration 901, loss = 0.00565607\n",
      "Iteration 902, loss = 0.00564231\n",
      "Iteration 903, loss = 0.00565061\n",
      "Iteration 904, loss = 0.00568511\n",
      "Iteration 905, loss = 0.00564197\n",
      "Iteration 906, loss = 0.00563362\n",
      "Iteration 907, loss = 0.00565366\n",
      "Iteration 908, loss = 0.00565372\n",
      "Iteration 909, loss = 0.00554579\n",
      "Iteration 910, loss = 0.00564157\n",
      "Iteration 911, loss = 0.00559349\n",
      "Iteration 912, loss = 0.00559761\n",
      "Iteration 913, loss = 0.00561822\n",
      "Iteration 914, loss = 0.00556943\n",
      "Iteration 915, loss = 0.00552905\n",
      "Iteration 916, loss = 0.00551676\n",
      "Iteration 917, loss = 0.00551887\n",
      "Iteration 918, loss = 0.00554376\n",
      "Iteration 919, loss = 0.00555466\n",
      "Iteration 920, loss = 0.00552557\n",
      "Iteration 921, loss = 0.00559538\n",
      "Iteration 922, loss = 0.00556691\n",
      "Iteration 923, loss = 0.00548495\n",
      "Iteration 924, loss = 0.00549391\n",
      "Iteration 925, loss = 0.00545132\n",
      "Iteration 926, loss = 0.00543907\n",
      "Iteration 927, loss = 0.00539637\n",
      "Iteration 928, loss = 0.00547284\n",
      "Iteration 929, loss = 0.00548318\n",
      "Iteration 930, loss = 0.00547425\n",
      "Iteration 931, loss = 0.00549050\n",
      "Iteration 932, loss = 0.00544428\n",
      "Iteration 933, loss = 0.00538134\n",
      "Iteration 934, loss = 0.00540448\n",
      "Iteration 935, loss = 0.00541158\n",
      "Iteration 936, loss = 0.00535730\n",
      "Iteration 937, loss = 0.00558274\n",
      "Iteration 938, loss = 0.00531397\n",
      "Iteration 939, loss = 0.00534137\n",
      "Iteration 940, loss = 0.00536506\n",
      "Iteration 941, loss = 0.00533351\n",
      "Iteration 942, loss = 0.00531693\n",
      "Iteration 943, loss = 0.00531003\n",
      "Iteration 944, loss = 0.00525687\n",
      "Iteration 945, loss = 0.00533112\n",
      "Iteration 946, loss = 0.00535897\n",
      "Iteration 947, loss = 0.00525639\n",
      "Iteration 948, loss = 0.00529090\n",
      "Iteration 949, loss = 0.00526757\n",
      "Iteration 950, loss = 0.00529873\n",
      "Iteration 951, loss = 0.00526636\n",
      "Iteration 952, loss = 0.00542741\n",
      "Iteration 953, loss = 0.00524059\n",
      "Iteration 954, loss = 0.00520317\n",
      "Iteration 955, loss = 0.00519892\n",
      "Iteration 956, loss = 0.00527144\n",
      "Iteration 957, loss = 0.00518642\n",
      "Iteration 958, loss = 0.00522816\n",
      "Iteration 959, loss = 0.00522244\n",
      "Iteration 960, loss = 0.00522263\n",
      "Iteration 961, loss = 0.00516390\n",
      "Iteration 962, loss = 0.00514493\n",
      "Iteration 963, loss = 0.00516875\n",
      "Iteration 964, loss = 0.00523420\n",
      "Iteration 965, loss = 0.00513935\n",
      "Iteration 966, loss = 0.00512655\n",
      "Iteration 967, loss = 0.00511659\n",
      "Iteration 968, loss = 0.00514490\n",
      "Iteration 969, loss = 0.00512024\n",
      "Iteration 970, loss = 0.00517025\n",
      "Iteration 971, loss = 0.00508898\n",
      "Iteration 972, loss = 0.00508918\n",
      "Iteration 973, loss = 0.00518580\n",
      "Iteration 974, loss = 0.00509838\n",
      "Iteration 975, loss = 0.00505146\n",
      "Iteration 976, loss = 0.00509892\n",
      "Iteration 977, loss = 0.00504230\n",
      "Iteration 978, loss = 0.00507161\n",
      "Iteration 979, loss = 0.00513716\n",
      "Iteration 980, loss = 0.00504194\n",
      "Iteration 981, loss = 0.00508443\n",
      "Iteration 982, loss = 0.00506940\n",
      "Iteration 983, loss = 0.00509449\n",
      "Iteration 984, loss = 0.00504342\n",
      "Iteration 985, loss = 0.00506510\n",
      "Iteration 986, loss = 0.00497554\n",
      "Iteration 987, loss = 0.00499205\n",
      "Iteration 988, loss = 0.00499367\n",
      "Iteration 989, loss = 0.00504464\n",
      "Iteration 990, loss = 0.00491007\n",
      "Iteration 991, loss = 0.00497684\n",
      "Iteration 992, loss = 0.00497205\n",
      "Iteration 993, loss = 0.00495151\n",
      "Iteration 994, loss = 0.00495516\n",
      "Iteration 995, loss = 0.00489399\n",
      "Iteration 996, loss = 0.00495792\n",
      "Iteration 997, loss = 0.00506439\n",
      "Iteration 998, loss = 0.00492498\n",
      "Iteration 999, loss = 0.00488624\n",
      "Iteration 1000, loss = 0.00489953\n",
      "Iteration 1, loss = 0.65336690\n",
      "Iteration 2, loss = 0.57525923\n",
      "Iteration 3, loss = 0.51101009\n",
      "Iteration 4, loss = 0.46008118\n",
      "Iteration 5, loss = 0.41809179\n",
      "Iteration 6, loss = 0.38334170\n",
      "Iteration 7, loss = 0.35420058\n",
      "Iteration 8, loss = 0.32903300\n",
      "Iteration 9, loss = 0.30676852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lindino/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.28708999\n",
      "Iteration 11, loss = 0.26929431\n",
      "Iteration 12, loss = 0.25345871\n",
      "Iteration 13, loss = 0.23945528\n",
      "Iteration 14, loss = 0.22689338\n",
      "Iteration 15, loss = 0.21545199\n",
      "Iteration 16, loss = 0.20506208\n",
      "Iteration 17, loss = 0.19571224\n",
      "Iteration 18, loss = 0.18725640\n",
      "Iteration 19, loss = 0.17951448\n",
      "Iteration 20, loss = 0.17240686\n",
      "Iteration 21, loss = 0.16591300\n",
      "Iteration 22, loss = 0.15983829\n",
      "Iteration 23, loss = 0.15436588\n",
      "Iteration 24, loss = 0.14921719\n",
      "Iteration 25, loss = 0.14447270\n",
      "Iteration 26, loss = 0.14003422\n",
      "Iteration 27, loss = 0.13592375\n",
      "Iteration 28, loss = 0.13201724\n",
      "Iteration 29, loss = 0.12852091\n",
      "Iteration 30, loss = 0.12514693\n",
      "Iteration 31, loss = 0.12190548\n",
      "Iteration 32, loss = 0.11896618\n",
      "Iteration 33, loss = 0.11621695\n",
      "Iteration 34, loss = 0.11367520\n",
      "Iteration 35, loss = 0.11111325\n",
      "Iteration 36, loss = 0.10884430\n",
      "Iteration 37, loss = 0.10661277\n",
      "Iteration 38, loss = 0.10450240\n",
      "Iteration 39, loss = 0.10250653\n",
      "Iteration 40, loss = 0.10071132\n",
      "Iteration 41, loss = 0.09893334\n",
      "Iteration 42, loss = 0.09710368\n",
      "Iteration 43, loss = 0.09546900\n",
      "Iteration 44, loss = 0.09385951\n",
      "Iteration 45, loss = 0.09240847\n",
      "Iteration 46, loss = 0.09110868\n",
      "Iteration 47, loss = 0.08952311\n",
      "Iteration 48, loss = 0.08806227\n",
      "Iteration 49, loss = 0.08679846\n",
      "Iteration 50, loss = 0.08555700\n",
      "Iteration 51, loss = 0.08430580\n",
      "Iteration 52, loss = 0.08314043\n",
      "Iteration 53, loss = 0.08213995\n",
      "Iteration 54, loss = 0.08092276\n",
      "Iteration 55, loss = 0.07979828\n",
      "Iteration 56, loss = 0.07872000\n",
      "Iteration 57, loss = 0.07767834\n",
      "Iteration 58, loss = 0.07668786\n",
      "Iteration 59, loss = 0.07571389\n",
      "Iteration 60, loss = 0.07470941\n",
      "Iteration 61, loss = 0.07375618\n",
      "Iteration 62, loss = 0.07284405\n",
      "Iteration 63, loss = 0.07192749\n",
      "Iteration 64, loss = 0.07102794\n",
      "Iteration 65, loss = 0.07004132\n",
      "Iteration 66, loss = 0.06916301\n",
      "Iteration 67, loss = 0.06826579\n",
      "Iteration 68, loss = 0.06745873\n",
      "Iteration 69, loss = 0.06657165\n",
      "Iteration 70, loss = 0.06577698\n",
      "Iteration 71, loss = 0.06496548\n",
      "Iteration 72, loss = 0.06415774\n",
      "Iteration 73, loss = 0.06344394\n",
      "Iteration 74, loss = 0.06266794\n",
      "Iteration 75, loss = 0.06210359\n",
      "Iteration 76, loss = 0.06129322\n",
      "Iteration 77, loss = 0.06065810\n",
      "Iteration 78, loss = 0.05995346\n",
      "Iteration 79, loss = 0.05939000\n",
      "Iteration 80, loss = 0.05873297\n",
      "Iteration 81, loss = 0.05813630\n",
      "Iteration 82, loss = 0.05758634\n",
      "Iteration 83, loss = 0.05703938\n",
      "Iteration 84, loss = 0.05642899\n",
      "Iteration 85, loss = 0.05584945\n",
      "Iteration 86, loss = 0.05531145\n",
      "Iteration 87, loss = 0.05481050\n",
      "Iteration 88, loss = 0.05425818\n",
      "Iteration 89, loss = 0.05373139\n",
      "Iteration 90, loss = 0.05324527\n",
      "Iteration 91, loss = 0.05268699\n",
      "Iteration 92, loss = 0.05222935\n",
      "Iteration 93, loss = 0.05173891\n",
      "Iteration 94, loss = 0.05125417\n",
      "Iteration 95, loss = 0.05089545\n",
      "Iteration 96, loss = 0.05029792\n",
      "Iteration 97, loss = 0.04990593\n",
      "Iteration 98, loss = 0.04954039\n",
      "Iteration 99, loss = 0.04907628\n",
      "Iteration 100, loss = 0.04860000\n",
      "Iteration 101, loss = 0.04814834\n",
      "Iteration 102, loss = 0.04770959\n",
      "Iteration 103, loss = 0.04735475\n",
      "Iteration 104, loss = 0.04692250\n",
      "Iteration 105, loss = 0.04664408\n",
      "Iteration 106, loss = 0.04626611\n",
      "Iteration 107, loss = 0.04576822\n",
      "Iteration 108, loss = 0.04546410\n",
      "Iteration 109, loss = 0.04503524\n",
      "Iteration 110, loss = 0.04466444\n",
      "Iteration 111, loss = 0.04425578\n",
      "Iteration 112, loss = 0.04387843\n",
      "Iteration 113, loss = 0.04353030\n",
      "Iteration 114, loss = 0.04328519\n",
      "Iteration 115, loss = 0.04283105\n",
      "Iteration 116, loss = 0.04245963\n",
      "Iteration 117, loss = 0.04219822\n",
      "Iteration 118, loss = 0.04184385\n",
      "Iteration 119, loss = 0.04160200\n",
      "Iteration 120, loss = 0.04112000\n",
      "Iteration 121, loss = 0.04085192\n",
      "Iteration 122, loss = 0.04054304\n",
      "Iteration 123, loss = 0.04030299\n",
      "Iteration 124, loss = 0.03995729\n",
      "Iteration 125, loss = 0.03964572\n",
      "Iteration 126, loss = 0.03938452\n",
      "Iteration 127, loss = 0.03910007\n",
      "Iteration 128, loss = 0.03881261\n",
      "Iteration 129, loss = 0.03853036\n",
      "Iteration 130, loss = 0.03827252\n",
      "Iteration 131, loss = 0.03812213\n",
      "Iteration 132, loss = 0.03778669\n",
      "Iteration 133, loss = 0.03761151\n",
      "Iteration 134, loss = 0.03729676\n",
      "Iteration 135, loss = 0.03702734\n",
      "Iteration 136, loss = 0.03676511\n",
      "Iteration 137, loss = 0.03645699\n",
      "Iteration 138, loss = 0.03623782\n",
      "Iteration 139, loss = 0.03606513\n",
      "Iteration 140, loss = 0.03588338\n",
      "Iteration 141, loss = 0.03559657\n",
      "Iteration 142, loss = 0.03529678\n",
      "Iteration 143, loss = 0.03507281\n",
      "Iteration 144, loss = 0.03491388\n",
      "Iteration 145, loss = 0.03469986\n",
      "Iteration 146, loss = 0.03448927\n",
      "Iteration 147, loss = 0.03420606\n",
      "Iteration 148, loss = 0.03398130\n",
      "Iteration 149, loss = 0.03377706\n",
      "Iteration 150, loss = 0.03356657\n",
      "Iteration 151, loss = 0.03335302\n",
      "Iteration 152, loss = 0.03319199\n",
      "Iteration 153, loss = 0.03298123\n",
      "Iteration 154, loss = 0.03278550\n",
      "Iteration 155, loss = 0.03261917\n",
      "Iteration 156, loss = 0.03240148\n",
      "Iteration 157, loss = 0.03218538\n",
      "Iteration 158, loss = 0.03196852\n",
      "Iteration 159, loss = 0.03183690\n",
      "Iteration 160, loss = 0.03173340\n",
      "Iteration 161, loss = 0.03141865\n",
      "Iteration 162, loss = 0.03124135\n",
      "Iteration 163, loss = 0.03110282\n",
      "Iteration 164, loss = 0.03096525\n",
      "Iteration 165, loss = 0.03071898\n",
      "Iteration 166, loss = 0.03057085\n",
      "Iteration 167, loss = 0.03034911\n",
      "Iteration 168, loss = 0.03018056\n",
      "Iteration 169, loss = 0.03006905\n",
      "Iteration 170, loss = 0.02994760\n",
      "Iteration 171, loss = 0.02971231\n",
      "Iteration 172, loss = 0.02953431\n",
      "Iteration 173, loss = 0.02934298\n",
      "Iteration 174, loss = 0.02920508\n",
      "Iteration 175, loss = 0.02915279\n",
      "Iteration 176, loss = 0.02891165\n",
      "Iteration 177, loss = 0.02878773\n",
      "Iteration 178, loss = 0.02859540\n",
      "Iteration 179, loss = 0.02842569\n",
      "Iteration 180, loss = 0.02829167\n",
      "Iteration 181, loss = 0.02814584\n",
      "Iteration 182, loss = 0.02797951\n",
      "Iteration 183, loss = 0.02788759\n",
      "Iteration 184, loss = 0.02770555\n",
      "Iteration 185, loss = 0.02767200\n",
      "Iteration 186, loss = 0.02748391\n",
      "Iteration 187, loss = 0.02725287\n",
      "Iteration 188, loss = 0.02715433\n",
      "Iteration 189, loss = 0.02700038\n",
      "Iteration 190, loss = 0.02692050\n",
      "Iteration 191, loss = 0.02682912\n",
      "Iteration 192, loss = 0.02669048\n",
      "Iteration 193, loss = 0.02652659\n",
      "Iteration 194, loss = 0.02639600\n",
      "Iteration 195, loss = 0.02631495\n",
      "Iteration 196, loss = 0.02613620\n",
      "Iteration 197, loss = 0.02598415\n",
      "Iteration 198, loss = 0.02586019\n",
      "Iteration 199, loss = 0.02577436\n",
      "Iteration 200, loss = 0.02561202\n",
      "Iteration 201, loss = 0.02550275\n",
      "Iteration 202, loss = 0.02542988\n",
      "Iteration 203, loss = 0.02522022\n",
      "Iteration 204, loss = 0.02511614\n",
      "Iteration 205, loss = 0.02499367\n",
      "Iteration 206, loss = 0.02499589\n",
      "Iteration 207, loss = 0.02476686\n",
      "Iteration 208, loss = 0.02468077\n",
      "Iteration 209, loss = 0.02459509\n",
      "Iteration 210, loss = 0.02443453\n",
      "Iteration 211, loss = 0.02440164\n",
      "Iteration 212, loss = 0.02420978\n",
      "Iteration 213, loss = 0.02410202\n",
      "Iteration 214, loss = 0.02404500\n",
      "Iteration 215, loss = 0.02390587\n",
      "Iteration 216, loss = 0.02381520\n",
      "Iteration 217, loss = 0.02380406\n",
      "Iteration 218, loss = 0.02358567\n",
      "Iteration 219, loss = 0.02356099\n",
      "Iteration 220, loss = 0.02342902\n",
      "Iteration 221, loss = 0.02325016\n",
      "Iteration 222, loss = 0.02339383\n",
      "Iteration 223, loss = 0.02323481\n",
      "Iteration 224, loss = 0.02303799\n",
      "Iteration 225, loss = 0.02287702\n",
      "Iteration 226, loss = 0.02285248\n",
      "Iteration 227, loss = 0.02275798\n",
      "Iteration 228, loss = 0.02262131\n",
      "Iteration 229, loss = 0.02260457\n",
      "Iteration 230, loss = 0.02250013\n",
      "Iteration 231, loss = 0.02228988\n",
      "Iteration 232, loss = 0.02225064\n",
      "Iteration 233, loss = 0.02213917\n",
      "Iteration 234, loss = 0.02206099\n",
      "Iteration 235, loss = 0.02192588\n",
      "Iteration 236, loss = 0.02186974\n",
      "Iteration 237, loss = 0.02179008\n",
      "Iteration 238, loss = 0.02166674\n",
      "Iteration 239, loss = 0.02156560\n",
      "Iteration 240, loss = 0.02149654\n",
      "Iteration 241, loss = 0.02144944\n",
      "Iteration 242, loss = 0.02133541\n",
      "Iteration 243, loss = 0.02122941\n",
      "Iteration 244, loss = 0.02119784\n",
      "Iteration 245, loss = 0.02117069\n",
      "Iteration 246, loss = 0.02105476\n",
      "Iteration 247, loss = 0.02094083\n",
      "Iteration 248, loss = 0.02081148\n",
      "Iteration 249, loss = 0.02079850\n",
      "Iteration 250, loss = 0.02078972\n",
      "Iteration 251, loss = 0.02074879\n",
      "Iteration 252, loss = 0.02060797\n",
      "Iteration 253, loss = 0.02045214\n",
      "Iteration 254, loss = 0.02038758\n",
      "Iteration 255, loss = 0.02028046\n",
      "Iteration 256, loss = 0.02030381\n",
      "Iteration 257, loss = 0.02011785\n",
      "Iteration 258, loss = 0.02003152\n",
      "Iteration 259, loss = 0.02002079\n",
      "Iteration 260, loss = 0.01990493\n",
      "Iteration 261, loss = 0.01982873\n",
      "Iteration 262, loss = 0.01972903\n",
      "Iteration 263, loss = 0.01969596\n",
      "Iteration 264, loss = 0.01962821\n",
      "Iteration 265, loss = 0.01957267\n",
      "Iteration 266, loss = 0.01956835\n",
      "Iteration 267, loss = 0.01941425\n",
      "Iteration 268, loss = 0.01932608\n",
      "Iteration 269, loss = 0.01938490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 270, loss = 0.01924495\n",
      "Iteration 271, loss = 0.01912774\n",
      "Iteration 272, loss = 0.01905038\n",
      "Iteration 273, loss = 0.01897072\n",
      "Iteration 274, loss = 0.01912695\n",
      "Iteration 275, loss = 0.01884294\n",
      "Iteration 276, loss = 0.01893386\n",
      "Iteration 277, loss = 0.01879516\n",
      "Iteration 278, loss = 0.01866212\n",
      "Iteration 279, loss = 0.01860935\n",
      "Iteration 280, loss = 0.01853756\n",
      "Iteration 281, loss = 0.01846728\n",
      "Iteration 282, loss = 0.01839722\n",
      "Iteration 283, loss = 0.01835829\n",
      "Iteration 284, loss = 0.01825506\n",
      "Iteration 285, loss = 0.01824012\n",
      "Iteration 286, loss = 0.01819280\n",
      "Iteration 287, loss = 0.01811591\n",
      "Iteration 288, loss = 0.01803694\n",
      "Iteration 289, loss = 0.01800471\n",
      "Iteration 290, loss = 0.01796390\n",
      "Iteration 291, loss = 0.01780511\n",
      "Iteration 292, loss = 0.01781543\n",
      "Iteration 293, loss = 0.01776033\n",
      "Iteration 294, loss = 0.01765806\n",
      "Iteration 295, loss = 0.01760978\n",
      "Iteration 296, loss = 0.01751353\n",
      "Iteration 297, loss = 0.01748817\n",
      "Iteration 298, loss = 0.01750192\n",
      "Iteration 299, loss = 0.01739637\n",
      "Iteration 300, loss = 0.01737747\n",
      "Iteration 301, loss = 0.01730342\n",
      "Iteration 302, loss = 0.01720873\n",
      "Iteration 303, loss = 0.01715020\n",
      "Iteration 304, loss = 0.01708818\n",
      "Iteration 305, loss = 0.01704958\n",
      "Iteration 306, loss = 0.01696760\n",
      "Iteration 307, loss = 0.01692272\n",
      "Iteration 308, loss = 0.01687346\n",
      "Iteration 309, loss = 0.01685273\n",
      "Iteration 310, loss = 0.01675993\n",
      "Iteration 311, loss = 0.01678184\n",
      "Iteration 312, loss = 0.01668857\n",
      "Iteration 313, loss = 0.01663144\n",
      "Iteration 314, loss = 0.01656176\n",
      "Iteration 315, loss = 0.01651000\n",
      "Iteration 316, loss = 0.01649250\n",
      "Iteration 317, loss = 0.01641327\n",
      "Iteration 318, loss = 0.01646133\n",
      "Iteration 319, loss = 0.01630742\n",
      "Iteration 320, loss = 0.01624559\n",
      "Iteration 321, loss = 0.01621717\n",
      "Iteration 322, loss = 0.01616453\n",
      "Iteration 323, loss = 0.01609321\n",
      "Iteration 324, loss = 0.01604857\n",
      "Iteration 325, loss = 0.01599305\n",
      "Iteration 326, loss = 0.01597491\n",
      "Iteration 327, loss = 0.01592300\n",
      "Iteration 328, loss = 0.01596897\n",
      "Iteration 329, loss = 0.01587732\n",
      "Iteration 330, loss = 0.01582039\n",
      "Iteration 331, loss = 0.01574670\n",
      "Iteration 332, loss = 0.01569635\n",
      "Iteration 333, loss = 0.01564118\n",
      "Iteration 334, loss = 0.01565676\n",
      "Iteration 335, loss = 0.01550298\n",
      "Iteration 336, loss = 0.01554411\n",
      "Iteration 337, loss = 0.01543760\n",
      "Iteration 338, loss = 0.01548396\n",
      "Iteration 339, loss = 0.01530006\n",
      "Iteration 340, loss = 0.01546794\n",
      "Iteration 341, loss = 0.01525873\n",
      "Iteration 342, loss = 0.01518463\n",
      "Iteration 343, loss = 0.01518633\n",
      "Iteration 344, loss = 0.01508891\n",
      "Iteration 345, loss = 0.01507379\n",
      "Iteration 346, loss = 0.01507195\n",
      "Iteration 347, loss = 0.01504183\n",
      "Iteration 348, loss = 0.01491253\n",
      "Iteration 349, loss = 0.01493540\n",
      "Iteration 350, loss = 0.01484148\n",
      "Iteration 351, loss = 0.01479666\n",
      "Iteration 352, loss = 0.01473727\n",
      "Iteration 353, loss = 0.01473775\n",
      "Iteration 354, loss = 0.01465495\n",
      "Iteration 355, loss = 0.01467602\n",
      "Iteration 356, loss = 0.01460551\n",
      "Iteration 357, loss = 0.01460052\n",
      "Iteration 358, loss = 0.01457719\n",
      "Iteration 359, loss = 0.01448996\n",
      "Iteration 360, loss = 0.01441497\n",
      "Iteration 361, loss = 0.01438112\n",
      "Iteration 362, loss = 0.01435992\n",
      "Iteration 363, loss = 0.01432137\n",
      "Iteration 364, loss = 0.01423987\n",
      "Iteration 365, loss = 0.01433901\n",
      "Iteration 366, loss = 0.01417729\n",
      "Iteration 367, loss = 0.01422564\n",
      "Iteration 368, loss = 0.01426339\n",
      "Iteration 369, loss = 0.01407752\n",
      "Iteration 370, loss = 0.01407883\n",
      "Iteration 371, loss = 0.01409150\n",
      "Iteration 372, loss = 0.01395139\n",
      "Iteration 373, loss = 0.01394990\n",
      "Iteration 374, loss = 0.01389431\n",
      "Iteration 375, loss = 0.01387302\n",
      "Iteration 376, loss = 0.01385293\n",
      "Iteration 377, loss = 0.01375312\n",
      "Iteration 378, loss = 0.01378184\n",
      "Iteration 379, loss = 0.01373736\n",
      "Iteration 380, loss = 0.01368882\n",
      "Iteration 381, loss = 0.01367429\n",
      "Iteration 382, loss = 0.01355175\n",
      "Iteration 383, loss = 0.01361689\n",
      "Iteration 384, loss = 0.01350391\n",
      "Iteration 385, loss = 0.01348958\n",
      "Iteration 386, loss = 0.01351906\n",
      "Iteration 387, loss = 0.01349256\n",
      "Iteration 388, loss = 0.01335710\n",
      "Iteration 389, loss = 0.01351913\n",
      "Iteration 390, loss = 0.01322824\n",
      "Iteration 391, loss = 0.01325494\n",
      "Iteration 392, loss = 0.01322348\n",
      "Iteration 393, loss = 0.01318348\n",
      "Iteration 394, loss = 0.01315805\n",
      "Iteration 395, loss = 0.01313725\n",
      "Iteration 396, loss = 0.01315738\n",
      "Iteration 397, loss = 0.01307794\n",
      "Iteration 398, loss = 0.01297515\n",
      "Iteration 399, loss = 0.01295996\n",
      "Iteration 400, loss = 0.01310157\n",
      "Iteration 401, loss = 0.01294595\n",
      "Iteration 402, loss = 0.01287201\n",
      "Iteration 403, loss = 0.01297071\n",
      "Iteration 404, loss = 0.01286554\n",
      "Iteration 405, loss = 0.01284953\n",
      "Iteration 406, loss = 0.01274127\n",
      "Iteration 407, loss = 0.01267983\n",
      "Iteration 408, loss = 0.01270110\n",
      "Iteration 409, loss = 0.01269466\n",
      "Iteration 410, loss = 0.01263194\n",
      "Iteration 411, loss = 0.01268526\n",
      "Iteration 412, loss = 0.01254427\n",
      "Iteration 413, loss = 0.01250336\n",
      "Iteration 414, loss = 0.01253928\n",
      "Iteration 415, loss = 0.01253850\n",
      "Iteration 416, loss = 0.01248358\n",
      "Iteration 417, loss = 0.01245178\n",
      "Iteration 418, loss = 0.01237315\n",
      "Iteration 419, loss = 0.01230294\n",
      "Iteration 420, loss = 0.01247324\n",
      "Iteration 421, loss = 0.01231447\n",
      "Iteration 422, loss = 0.01225227\n",
      "Iteration 423, loss = 0.01224367\n",
      "Iteration 424, loss = 0.01218385\n",
      "Iteration 425, loss = 0.01224102\n",
      "Iteration 426, loss = 0.01210149\n",
      "Iteration 427, loss = 0.01222287\n",
      "Iteration 428, loss = 0.01205219\n",
      "Iteration 429, loss = 0.01213336\n",
      "Iteration 430, loss = 0.01198498\n",
      "Iteration 431, loss = 0.01200277\n",
      "Iteration 432, loss = 0.01197682\n",
      "Iteration 433, loss = 0.01196122\n",
      "Iteration 434, loss = 0.01193220\n",
      "Iteration 435, loss = 0.01194696\n",
      "Iteration 436, loss = 0.01181956\n",
      "Iteration 437, loss = 0.01186628\n",
      "Iteration 438, loss = 0.01179439\n",
      "Iteration 439, loss = 0.01174948\n",
      "Iteration 440, loss = 0.01173088\n",
      "Iteration 441, loss = 0.01172210\n",
      "Iteration 442, loss = 0.01169628\n",
      "Iteration 443, loss = 0.01173672\n",
      "Iteration 444, loss = 0.01167797\n",
      "Iteration 445, loss = 0.01156658\n",
      "Iteration 446, loss = 0.01156780\n",
      "Iteration 447, loss = 0.01151773\n",
      "Iteration 448, loss = 0.01154528\n",
      "Iteration 449, loss = 0.01148354\n",
      "Iteration 450, loss = 0.01151327\n",
      "Iteration 451, loss = 0.01145625\n",
      "Iteration 452, loss = 0.01139715\n",
      "Iteration 453, loss = 0.01136344\n",
      "Iteration 454, loss = 0.01143826\n",
      "Iteration 455, loss = 0.01136144\n",
      "Iteration 456, loss = 0.01134989\n",
      "Iteration 457, loss = 0.01132408\n",
      "Iteration 458, loss = 0.01122022\n",
      "Iteration 459, loss = 0.01121867\n",
      "Iteration 460, loss = 0.01133404\n",
      "Iteration 461, loss = 0.01113819\n",
      "Iteration 462, loss = 0.01117530\n",
      "Iteration 463, loss = 0.01112979\n",
      "Iteration 464, loss = 0.01112662\n",
      "Iteration 465, loss = 0.01114623\n",
      "Iteration 466, loss = 0.01102115\n",
      "Iteration 467, loss = 0.01101753\n",
      "Iteration 468, loss = 0.01100578\n",
      "Iteration 469, loss = 0.01107286\n",
      "Iteration 470, loss = 0.01095597\n",
      "Iteration 471, loss = 0.01098268\n",
      "Iteration 472, loss = 0.01092758\n",
      "Iteration 473, loss = 0.01090310\n",
      "Iteration 474, loss = 0.01088782\n",
      "Iteration 475, loss = 0.01085689\n",
      "Iteration 476, loss = 0.01078963\n",
      "Iteration 477, loss = 0.01078135\n",
      "Iteration 478, loss = 0.01080746\n",
      "Iteration 479, loss = 0.01075347\n",
      "Iteration 480, loss = 0.01078222\n",
      "Iteration 481, loss = 0.01064491\n",
      "Iteration 482, loss = 0.01062854\n",
      "Iteration 483, loss = 0.01073075\n",
      "Iteration 484, loss = 0.01070508\n",
      "Iteration 485, loss = 0.01059840\n",
      "Iteration 486, loss = 0.01052882\n",
      "Iteration 487, loss = 0.01054837\n",
      "Iteration 488, loss = 0.01055156\n",
      "Iteration 489, loss = 0.01047443\n",
      "Iteration 490, loss = 0.01048932\n",
      "Iteration 491, loss = 0.01052976\n",
      "Iteration 492, loss = 0.01041748\n",
      "Iteration 493, loss = 0.01044050\n",
      "Iteration 494, loss = 0.01060866\n",
      "Iteration 495, loss = 0.01036286\n",
      "Iteration 496, loss = 0.01033443\n",
      "Iteration 497, loss = 0.01039622\n",
      "Iteration 498, loss = 0.01029693\n",
      "Iteration 499, loss = 0.01026504\n",
      "Iteration 500, loss = 0.01028358\n",
      "Iteration 501, loss = 0.01025969\n",
      "Iteration 502, loss = 0.01023768\n",
      "Iteration 503, loss = 0.01013388\n",
      "Iteration 504, loss = 0.01019933\n",
      "Iteration 505, loss = 0.01012705\n",
      "Iteration 506, loss = 0.01011740\n",
      "Iteration 507, loss = 0.01010101\n",
      "Iteration 508, loss = 0.01008058\n",
      "Iteration 509, loss = 0.01010258\n",
      "Iteration 510, loss = 0.01007454\n",
      "Iteration 511, loss = 0.01004068\n",
      "Iteration 512, loss = 0.01003980\n",
      "Iteration 513, loss = 0.00999681\n",
      "Iteration 514, loss = 0.00998347\n",
      "Iteration 515, loss = 0.00991987\n",
      "Iteration 516, loss = 0.00989459\n",
      "Iteration 517, loss = 0.00985818\n",
      "Iteration 518, loss = 0.00986271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 519, loss = 0.00981671\n",
      "Iteration 520, loss = 0.00988230\n",
      "Iteration 521, loss = 0.00975868\n",
      "Iteration 522, loss = 0.00981986\n",
      "Iteration 523, loss = 0.00980365\n",
      "Iteration 524, loss = 0.00977104\n",
      "Iteration 525, loss = 0.00973810\n",
      "Iteration 526, loss = 0.00971819\n",
      "Iteration 527, loss = 0.00970600\n",
      "Iteration 528, loss = 0.00970362\n",
      "Iteration 529, loss = 0.00969539\n",
      "Iteration 530, loss = 0.00963105\n",
      "Iteration 531, loss = 0.00973251\n",
      "Iteration 532, loss = 0.00959289\n",
      "Iteration 533, loss = 0.00957758\n",
      "Iteration 534, loss = 0.00959935\n",
      "Iteration 535, loss = 0.00952948\n",
      "Iteration 536, loss = 0.00953439\n",
      "Iteration 537, loss = 0.00949586\n",
      "Iteration 538, loss = 0.00952943\n",
      "Iteration 539, loss = 0.00944244\n",
      "Iteration 540, loss = 0.00944496\n",
      "Iteration 541, loss = 0.00941872\n",
      "Iteration 542, loss = 0.00941302\n",
      "Iteration 543, loss = 0.00944338\n",
      "Iteration 544, loss = 0.00935939\n",
      "Iteration 545, loss = 0.00935962\n",
      "Iteration 546, loss = 0.00932145\n",
      "Iteration 547, loss = 0.00939229\n",
      "Iteration 548, loss = 0.00928258\n",
      "Iteration 549, loss = 0.00950251\n",
      "Iteration 550, loss = 0.00940200\n",
      "Iteration 551, loss = 0.00922666\n",
      "Iteration 552, loss = 0.00919903\n",
      "Iteration 553, loss = 0.00917509\n",
      "Iteration 554, loss = 0.00923821\n",
      "Iteration 555, loss = 0.00914479\n",
      "Iteration 556, loss = 0.00918701\n",
      "Iteration 557, loss = 0.00917868\n",
      "Iteration 558, loss = 0.00913990\n",
      "Iteration 559, loss = 0.00916823\n",
      "Iteration 560, loss = 0.00909068\n",
      "Iteration 561, loss = 0.00907480\n",
      "Iteration 562, loss = 0.00900670\n",
      "Iteration 563, loss = 0.00907462\n",
      "Iteration 564, loss = 0.00898397\n",
      "Iteration 565, loss = 0.00911788\n",
      "Iteration 566, loss = 0.00910792\n",
      "Iteration 567, loss = 0.00900761\n",
      "Iteration 568, loss = 0.00902333\n",
      "Iteration 569, loss = 0.00899978\n",
      "Iteration 570, loss = 0.00891215\n",
      "Iteration 571, loss = 0.00895707\n",
      "Iteration 572, loss = 0.00889552\n",
      "Iteration 573, loss = 0.00891473\n",
      "Iteration 574, loss = 0.00892591\n",
      "Iteration 575, loss = 0.00887254\n",
      "Iteration 576, loss = 0.00877574\n",
      "Iteration 577, loss = 0.00877764\n",
      "Iteration 578, loss = 0.00886260\n",
      "Iteration 579, loss = 0.00881795\n",
      "Iteration 580, loss = 0.00891581\n",
      "Iteration 581, loss = 0.00879536\n",
      "Iteration 582, loss = 0.00874762\n",
      "Iteration 583, loss = 0.00882450\n",
      "Iteration 584, loss = 0.00870713\n",
      "Iteration 585, loss = 0.00869404\n",
      "Iteration 586, loss = 0.00868465\n",
      "Iteration 587, loss = 0.00864437\n",
      "Iteration 588, loss = 0.00873114\n",
      "Iteration 589, loss = 0.00858789\n",
      "Iteration 590, loss = 0.00868167\n",
      "Iteration 591, loss = 0.00860467\n",
      "Iteration 592, loss = 0.00854628\n",
      "Iteration 593, loss = 0.00855958\n",
      "Iteration 594, loss = 0.00860472\n",
      "Iteration 595, loss = 0.00863575\n",
      "Iteration 596, loss = 0.00875651\n",
      "Iteration 597, loss = 0.00845691\n",
      "Iteration 598, loss = 0.00856959\n",
      "Iteration 599, loss = 0.00854210\n",
      "Iteration 600, loss = 0.00839329\n",
      "Iteration 601, loss = 0.00844581\n",
      "Iteration 602, loss = 0.00839563\n",
      "Iteration 603, loss = 0.00843426\n",
      "Iteration 604, loss = 0.00845550\n",
      "Iteration 605, loss = 0.00835791\n",
      "Iteration 606, loss = 0.00831276\n",
      "Iteration 607, loss = 0.00828717\n",
      "Iteration 608, loss = 0.00831697\n",
      "Iteration 609, loss = 0.00829927\n",
      "Iteration 610, loss = 0.00838096\n",
      "Iteration 611, loss = 0.00833296\n",
      "Iteration 612, loss = 0.00829643\n",
      "Iteration 613, loss = 0.00845581\n",
      "Iteration 614, loss = 0.00822877\n",
      "Iteration 615, loss = 0.00819329\n",
      "Iteration 616, loss = 0.00817508\n",
      "Iteration 617, loss = 0.00814376\n",
      "Iteration 618, loss = 0.00815773\n",
      "Iteration 619, loss = 0.00821617\n",
      "Iteration 620, loss = 0.00808865\n",
      "Iteration 621, loss = 0.00812696\n",
      "Iteration 622, loss = 0.00826359\n",
      "Iteration 623, loss = 0.00812435\n",
      "Iteration 624, loss = 0.00809844\n",
      "Iteration 625, loss = 0.00807280\n",
      "Iteration 626, loss = 0.00810888\n",
      "Iteration 627, loss = 0.00808456\n",
      "Iteration 628, loss = 0.00809022\n",
      "Iteration 629, loss = 0.00806944\n",
      "Iteration 630, loss = 0.00798983\n",
      "Iteration 631, loss = 0.00800032\n",
      "Iteration 632, loss = 0.00804317\n",
      "Iteration 633, loss = 0.00793655\n",
      "Iteration 634, loss = 0.00809943\n",
      "Iteration 635, loss = 0.00801322\n",
      "Iteration 636, loss = 0.00793096\n",
      "Iteration 637, loss = 0.00787720\n",
      "Iteration 638, loss = 0.00786068\n",
      "Iteration 639, loss = 0.00790374\n",
      "Iteration 640, loss = 0.00790634\n",
      "Iteration 641, loss = 0.00788844\n",
      "Iteration 642, loss = 0.00784161\n",
      "Iteration 643, loss = 0.00782451\n",
      "Iteration 644, loss = 0.00789736\n",
      "Iteration 645, loss = 0.00781515\n",
      "Iteration 646, loss = 0.00812253\n",
      "Iteration 647, loss = 0.00776652\n",
      "Iteration 648, loss = 0.00776079\n",
      "Iteration 649, loss = 0.00787469\n",
      "Iteration 650, loss = 0.00765781\n",
      "Iteration 651, loss = 0.00781092\n",
      "Iteration 652, loss = 0.00768385\n",
      "Iteration 653, loss = 0.00780024\n",
      "Iteration 654, loss = 0.00764045\n",
      "Iteration 655, loss = 0.00759985\n",
      "Iteration 656, loss = 0.00764963\n",
      "Iteration 657, loss = 0.00771357\n",
      "Iteration 658, loss = 0.00761266\n",
      "Iteration 659, loss = 0.00769939\n",
      "Iteration 660, loss = 0.00759183\n",
      "Iteration 661, loss = 0.00767524\n",
      "Iteration 662, loss = 0.00755866\n",
      "Iteration 663, loss = 0.00762938\n",
      "Iteration 664, loss = 0.00750782\n",
      "Iteration 665, loss = 0.00763258\n",
      "Iteration 666, loss = 0.00763976\n",
      "Iteration 667, loss = 0.00751963\n",
      "Iteration 668, loss = 0.00747081\n",
      "Iteration 669, loss = 0.00748902\n",
      "Iteration 670, loss = 0.00745929\n",
      "Iteration 671, loss = 0.00745097\n",
      "Iteration 672, loss = 0.00748070\n",
      "Iteration 673, loss = 0.00741781\n",
      "Iteration 674, loss = 0.00738840\n",
      "Iteration 675, loss = 0.00737425\n",
      "Iteration 676, loss = 0.00740872\n",
      "Iteration 677, loss = 0.00740999\n",
      "Iteration 678, loss = 0.00736580\n",
      "Iteration 679, loss = 0.00732368\n",
      "Iteration 680, loss = 0.00732613\n",
      "Iteration 681, loss = 0.00734741\n",
      "Iteration 682, loss = 0.00734022\n",
      "Iteration 683, loss = 0.00734348\n",
      "Iteration 684, loss = 0.00740084\n",
      "Iteration 685, loss = 0.00727867\n",
      "Iteration 686, loss = 0.00736169\n",
      "Iteration 687, loss = 0.00728288\n",
      "Iteration 688, loss = 0.00730600\n",
      "Iteration 689, loss = 0.00736152\n",
      "Iteration 690, loss = 0.00745113\n",
      "Iteration 691, loss = 0.00718039\n",
      "Iteration 692, loss = 0.00725157\n",
      "Iteration 693, loss = 0.00723944\n",
      "Iteration 694, loss = 0.00717905\n",
      "Iteration 695, loss = 0.00720313\n",
      "Iteration 696, loss = 0.00721523\n",
      "Iteration 697, loss = 0.00728164\n",
      "Iteration 698, loss = 0.00716001\n",
      "Iteration 699, loss = 0.00711298\n",
      "Iteration 700, loss = 0.00711207\n",
      "Iteration 701, loss = 0.00717476\n",
      "Iteration 702, loss = 0.00713175\n",
      "Iteration 703, loss = 0.00708528\n",
      "Iteration 704, loss = 0.00705340\n",
      "Iteration 705, loss = 0.00708900\n",
      "Iteration 706, loss = 0.00707277\n",
      "Iteration 707, loss = 0.00704214\n",
      "Iteration 708, loss = 0.00698953\n",
      "Iteration 709, loss = 0.00700108\n",
      "Iteration 710, loss = 0.00699954\n",
      "Iteration 711, loss = 0.00700656\n",
      "Iteration 712, loss = 0.00698735\n",
      "Iteration 713, loss = 0.00694847\n",
      "Iteration 714, loss = 0.00701638\n",
      "Iteration 715, loss = 0.00693505\n",
      "Iteration 716, loss = 0.00695441\n",
      "Iteration 717, loss = 0.00697319\n",
      "Iteration 718, loss = 0.00689737\n",
      "Iteration 719, loss = 0.00688716\n",
      "Iteration 720, loss = 0.00690947\n",
      "Iteration 721, loss = 0.00696613\n",
      "Iteration 722, loss = 0.00688834\n",
      "Iteration 723, loss = 0.00688105\n",
      "Iteration 724, loss = 0.00689739\n",
      "Iteration 725, loss = 0.00682506\n",
      "Iteration 726, loss = 0.00682006\n",
      "Iteration 727, loss = 0.00691379\n",
      "Iteration 728, loss = 0.00678622\n",
      "Iteration 729, loss = 0.00676375\n",
      "Iteration 730, loss = 0.00694620\n",
      "Iteration 731, loss = 0.00681835\n",
      "Iteration 732, loss = 0.00690433\n",
      "Iteration 733, loss = 0.00676119\n",
      "Iteration 734, loss = 0.00673247\n",
      "Iteration 735, loss = 0.00673607\n",
      "Iteration 736, loss = 0.00669397\n",
      "Iteration 737, loss = 0.00672177\n",
      "Iteration 738, loss = 0.00672091\n",
      "Iteration 739, loss = 0.00671656\n",
      "Iteration 740, loss = 0.00673580\n",
      "Iteration 741, loss = 0.00671638\n",
      "Iteration 742, loss = 0.00668660\n",
      "Iteration 743, loss = 0.00665020\n",
      "Iteration 744, loss = 0.00661108\n",
      "Iteration 745, loss = 0.00659425\n",
      "Iteration 746, loss = 0.00672792\n",
      "Iteration 747, loss = 0.00667399\n",
      "Iteration 748, loss = 0.00660696\n",
      "Iteration 749, loss = 0.00668739\n",
      "Iteration 750, loss = 0.00661200\n",
      "Iteration 751, loss = 0.00659015\n",
      "Iteration 752, loss = 0.00652201\n",
      "Iteration 753, loss = 0.00656502\n",
      "Iteration 754, loss = 0.00654855\n",
      "Iteration 755, loss = 0.00652774\n",
      "Iteration 756, loss = 0.00646685\n",
      "Iteration 757, loss = 0.00646434\n",
      "Iteration 758, loss = 0.00651598\n",
      "Iteration 759, loss = 0.00649455\n",
      "Iteration 760, loss = 0.00646705\n",
      "Iteration 761, loss = 0.00651398\n",
      "Iteration 762, loss = 0.00653319\n",
      "Iteration 763, loss = 0.00648720\n",
      "Iteration 764, loss = 0.00661692\n",
      "Iteration 765, loss = 0.00642582\n",
      "Iteration 766, loss = 0.00642825\n",
      "Iteration 767, loss = 0.00654757\n",
      "Iteration 768, loss = 0.00644456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 769, loss = 0.00638557\n",
      "Iteration 770, loss = 0.00645122\n",
      "Iteration 771, loss = 0.00641928\n",
      "Iteration 772, loss = 0.00637027\n",
      "Iteration 773, loss = 0.00639747\n",
      "Iteration 774, loss = 0.00634054\n",
      "Iteration 775, loss = 0.00636937\n",
      "Iteration 776, loss = 0.00637595\n",
      "Iteration 777, loss = 0.00632954\n",
      "Iteration 778, loss = 0.00630375\n",
      "Iteration 779, loss = 0.00631016\n",
      "Iteration 780, loss = 0.00635962\n",
      "Iteration 781, loss = 0.00626445\n",
      "Iteration 782, loss = 0.00624166\n",
      "Iteration 783, loss = 0.00623866\n",
      "Iteration 784, loss = 0.00632161\n",
      "Iteration 785, loss = 0.00633776\n",
      "Iteration 786, loss = 0.00622861\n",
      "Iteration 787, loss = 0.00622116\n",
      "Iteration 788, loss = 0.00618217\n",
      "Iteration 789, loss = 0.00622341\n",
      "Iteration 790, loss = 0.00621370\n",
      "Iteration 791, loss = 0.00620918\n",
      "Iteration 792, loss = 0.00619948\n",
      "Iteration 793, loss = 0.00614120\n",
      "Iteration 794, loss = 0.00616167\n",
      "Iteration 795, loss = 0.00620699\n",
      "Iteration 796, loss = 0.00618665\n",
      "Iteration 797, loss = 0.00615400\n",
      "Iteration 798, loss = 0.00608556\n",
      "Iteration 799, loss = 0.00618369\n",
      "Iteration 800, loss = 0.00616297\n",
      "Iteration 801, loss = 0.00605720\n",
      "Iteration 802, loss = 0.00614679\n",
      "Iteration 803, loss = 0.00611993\n",
      "Iteration 804, loss = 0.00605008\n",
      "Iteration 805, loss = 0.00610868\n",
      "Iteration 806, loss = 0.00612644\n",
      "Iteration 807, loss = 0.00610517\n",
      "Iteration 808, loss = 0.00605313\n",
      "Iteration 809, loss = 0.00602076\n",
      "Iteration 810, loss = 0.00602639\n",
      "Iteration 811, loss = 0.00605075\n",
      "Iteration 812, loss = 0.00598815\n",
      "Iteration 813, loss = 0.00598589\n",
      "Iteration 814, loss = 0.00600943\n",
      "Iteration 815, loss = 0.00595444\n",
      "Iteration 816, loss = 0.00604626\n",
      "Iteration 817, loss = 0.00596736\n",
      "Iteration 818, loss = 0.00599950\n",
      "Iteration 819, loss = 0.00601732\n",
      "Iteration 820, loss = 0.00596641\n",
      "Iteration 821, loss = 0.00598206\n",
      "Iteration 822, loss = 0.00595082\n",
      "Iteration 823, loss = 0.00596386\n",
      "Iteration 824, loss = 0.00589763\n",
      "Iteration 825, loss = 0.00587315\n",
      "Iteration 826, loss = 0.00586409\n",
      "Iteration 827, loss = 0.00584668\n",
      "Iteration 828, loss = 0.00591191\n",
      "Iteration 829, loss = 0.00586671\n",
      "Iteration 830, loss = 0.00585653\n",
      "Iteration 831, loss = 0.00588939\n",
      "Iteration 832, loss = 0.00586218\n",
      "Iteration 833, loss = 0.00582434\n",
      "Iteration 834, loss = 0.00584236\n",
      "Iteration 835, loss = 0.00582042\n",
      "Iteration 836, loss = 0.00597544\n",
      "Iteration 837, loss = 0.00578026\n",
      "Iteration 838, loss = 0.00581290\n",
      "Iteration 839, loss = 0.00581808\n",
      "Iteration 840, loss = 0.00579856\n",
      "Iteration 841, loss = 0.00577589\n",
      "Iteration 842, loss = 0.00571709\n",
      "Iteration 843, loss = 0.00574580\n",
      "Iteration 844, loss = 0.00575719\n",
      "Iteration 845, loss = 0.00573547\n",
      "Iteration 846, loss = 0.00574994\n",
      "Iteration 847, loss = 0.00574500\n",
      "Iteration 848, loss = 0.00571366\n",
      "Iteration 849, loss = 0.00568534\n",
      "Iteration 850, loss = 0.00566759\n",
      "Iteration 851, loss = 0.00573750\n",
      "Iteration 852, loss = 0.00568232\n",
      "Iteration 853, loss = 0.00573323\n",
      "Iteration 854, loss = 0.00562838\n",
      "Iteration 855, loss = 0.00565928\n",
      "Iteration 856, loss = 0.00571943\n",
      "Iteration 857, loss = 0.00565874\n",
      "Iteration 858, loss = 0.00557565\n",
      "Iteration 859, loss = 0.00573764\n",
      "Iteration 860, loss = 0.00568148\n",
      "Iteration 861, loss = 0.00565485\n",
      "Iteration 862, loss = 0.00562404\n",
      "Iteration 863, loss = 0.00555685\n",
      "Iteration 864, loss = 0.00557256\n",
      "Iteration 865, loss = 0.00557636\n",
      "Iteration 866, loss = 0.00556035\n",
      "Iteration 867, loss = 0.00556711\n",
      "Iteration 868, loss = 0.00558539\n",
      "Iteration 869, loss = 0.00550609\n",
      "Iteration 870, loss = 0.00561633\n",
      "Iteration 871, loss = 0.00553534\n",
      "Iteration 872, loss = 0.00555260\n",
      "Iteration 873, loss = 0.00556674\n",
      "Iteration 874, loss = 0.00557586\n",
      "Iteration 875, loss = 0.00547816\n",
      "Iteration 876, loss = 0.00546684\n",
      "Iteration 877, loss = 0.00549158\n",
      "Iteration 878, loss = 0.00549932\n",
      "Iteration 879, loss = 0.00547805\n",
      "Iteration 880, loss = 0.00556562\n",
      "Iteration 881, loss = 0.00550136\n",
      "Iteration 882, loss = 0.00556205\n",
      "Iteration 883, loss = 0.00540902\n",
      "Iteration 884, loss = 0.00544627\n",
      "Iteration 885, loss = 0.00546892\n",
      "Iteration 886, loss = 0.00541219\n",
      "Iteration 887, loss = 0.00542722\n",
      "Iteration 888, loss = 0.00556048\n",
      "Iteration 889, loss = 0.00540621\n",
      "Iteration 890, loss = 0.00545650\n",
      "Iteration 891, loss = 0.00537772\n",
      "Iteration 892, loss = 0.00535338\n",
      "Iteration 893, loss = 0.00541096\n",
      "Iteration 894, loss = 0.00533780\n",
      "Iteration 895, loss = 0.00533572\n",
      "Iteration 896, loss = 0.00536203\n",
      "Iteration 897, loss = 0.00536123\n",
      "Iteration 898, loss = 0.00534809\n",
      "Iteration 899, loss = 0.00535103\n",
      "Iteration 900, loss = 0.00528230\n",
      "Iteration 901, loss = 0.00534077\n",
      "Iteration 902, loss = 0.00535807\n",
      "Iteration 903, loss = 0.00529324\n",
      "Iteration 904, loss = 0.00533207\n",
      "Iteration 905, loss = 0.00530447\n",
      "Iteration 906, loss = 0.00525211\n",
      "Iteration 907, loss = 0.00530748\n",
      "Iteration 908, loss = 0.00526992\n",
      "Iteration 909, loss = 0.00535936\n",
      "Iteration 910, loss = 0.00532232\n",
      "Iteration 911, loss = 0.00527391\n",
      "Iteration 912, loss = 0.00522808\n",
      "Iteration 913, loss = 0.00525745\n",
      "Iteration 914, loss = 0.00528686\n",
      "Iteration 915, loss = 0.00522879\n",
      "Iteration 916, loss = 0.00522506\n",
      "Iteration 917, loss = 0.00522268\n",
      "Iteration 918, loss = 0.00520625\n",
      "Iteration 919, loss = 0.00523874\n",
      "Iteration 920, loss = 0.00519342\n",
      "Iteration 921, loss = 0.00518630\n",
      "Iteration 922, loss = 0.00517201\n",
      "Iteration 923, loss = 0.00514041\n",
      "Iteration 924, loss = 0.00518968\n",
      "Iteration 925, loss = 0.00518651\n",
      "Iteration 926, loss = 0.00510186\n",
      "Iteration 927, loss = 0.00513280\n",
      "Iteration 928, loss = 0.00510835\n",
      "Iteration 929, loss = 0.00511547\n",
      "Iteration 930, loss = 0.00515050\n",
      "Iteration 931, loss = 0.00516470\n",
      "Iteration 932, loss = 0.00506209\n",
      "Iteration 933, loss = 0.00512540\n",
      "Iteration 934, loss = 0.00511953\n",
      "Iteration 935, loss = 0.00534742\n",
      "Iteration 936, loss = 0.00501020\n",
      "Iteration 937, loss = 0.00506866\n",
      "Iteration 938, loss = 0.00506539\n",
      "Iteration 939, loss = 0.00498610\n",
      "Iteration 940, loss = 0.00504370\n",
      "Iteration 941, loss = 0.00501578\n",
      "Iteration 942, loss = 0.00511278\n",
      "Iteration 943, loss = 0.00499949\n",
      "Iteration 944, loss = 0.00510383\n",
      "Iteration 945, loss = 0.00497700\n",
      "Iteration 946, loss = 0.00507814\n",
      "Iteration 947, loss = 0.00503185\n",
      "Iteration 948, loss = 0.00499109\n",
      "Iteration 949, loss = 0.00502346\n",
      "Iteration 950, loss = 0.00494715\n",
      "Iteration 951, loss = 0.00497244\n",
      "Iteration 952, loss = 0.00495914\n",
      "Iteration 953, loss = 0.00497906\n",
      "Iteration 954, loss = 0.00496783\n",
      "Iteration 955, loss = 0.00493223\n",
      "Iteration 956, loss = 0.00495228\n",
      "Iteration 957, loss = 0.00498769\n",
      "Iteration 958, loss = 0.00490197\n",
      "Iteration 959, loss = 0.00488997\n",
      "Iteration 960, loss = 0.00494424\n",
      "Iteration 961, loss = 0.00487676\n",
      "Iteration 962, loss = 0.00493025\n",
      "Iteration 963, loss = 0.00489403\n",
      "Iteration 964, loss = 0.00491283\n",
      "Iteration 965, loss = 0.00484959\n",
      "Iteration 966, loss = 0.00485804\n",
      "Iteration 967, loss = 0.00485575\n",
      "Iteration 968, loss = 0.00489553\n",
      "Iteration 969, loss = 0.00480135\n",
      "Iteration 970, loss = 0.00489855\n",
      "Iteration 971, loss = 0.00482416\n",
      "Iteration 972, loss = 0.00483128\n",
      "Iteration 973, loss = 0.00480093\n",
      "Iteration 974, loss = 0.00480273\n",
      "Iteration 975, loss = 0.00486521\n",
      "Iteration 976, loss = 0.00476928\n",
      "Iteration 977, loss = 0.00476350\n",
      "Iteration 978, loss = 0.00477733\n",
      "Iteration 979, loss = 0.00477276\n",
      "Iteration 980, loss = 0.00478902\n",
      "Iteration 981, loss = 0.00478377\n",
      "Iteration 982, loss = 0.00475831\n",
      "Iteration 983, loss = 0.00479518\n",
      "Iteration 984, loss = 0.00478189\n",
      "Iteration 985, loss = 0.00475631\n",
      "Iteration 986, loss = 0.00471088\n",
      "Iteration 987, loss = 0.00474937\n",
      "Iteration 988, loss = 0.00469329\n",
      "Iteration 989, loss = 0.00471237\n",
      "Iteration 990, loss = 0.00472158\n",
      "Iteration 991, loss = 0.00469366\n",
      "Iteration 992, loss = 0.00469004\n",
      "Iteration 993, loss = 0.00481340\n",
      "Iteration 994, loss = 0.00471866\n",
      "Iteration 995, loss = 0.00477496\n",
      "Iteration 996, loss = 0.00469119\n",
      "Iteration 997, loss = 0.00475087\n",
      "Iteration 998, loss = 0.00470708\n",
      "Iteration 999, loss = 0.00461362\n",
      "Iteration 1000, loss = 0.00469061\n",
      "Iteration 1, loss = 0.91434031\n",
      "Iteration 2, loss = 0.81012554\n",
      "Iteration 3, loss = 0.71865765\n",
      "Iteration 4, loss = 0.63980125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lindino/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.57154036\n",
      "Iteration 6, loss = 0.51259112\n",
      "Iteration 7, loss = 0.46273582\n",
      "Iteration 8, loss = 0.41988085\n",
      "Iteration 9, loss = 0.38344698\n",
      "Iteration 10, loss = 0.35197103\n",
      "Iteration 11, loss = 0.32534934\n",
      "Iteration 12, loss = 0.30220570\n",
      "Iteration 13, loss = 0.28223977\n",
      "Iteration 14, loss = 0.26491814\n",
      "Iteration 15, loss = 0.24958578\n",
      "Iteration 16, loss = 0.23605150\n",
      "Iteration 17, loss = 0.22395846\n",
      "Iteration 18, loss = 0.21314316\n",
      "Iteration 19, loss = 0.20336459\n",
      "Iteration 20, loss = 0.19454288\n",
      "Iteration 21, loss = 0.18658175\n",
      "Iteration 22, loss = 0.17922683\n",
      "Iteration 23, loss = 0.17249131\n",
      "Iteration 24, loss = 0.16632661\n",
      "Iteration 25, loss = 0.16062147\n",
      "Iteration 26, loss = 0.15530531\n",
      "Iteration 27, loss = 0.15032382\n",
      "Iteration 28, loss = 0.14570509\n",
      "Iteration 29, loss = 0.14141081\n",
      "Iteration 30, loss = 0.13748858\n",
      "Iteration 31, loss = 0.13379064\n",
      "Iteration 32, loss = 0.13033235\n",
      "Iteration 33, loss = 0.12706192\n",
      "Iteration 34, loss = 0.12403153\n",
      "Iteration 35, loss = 0.12117815\n",
      "Iteration 36, loss = 0.11839683\n",
      "Iteration 37, loss = 0.11579852\n",
      "Iteration 38, loss = 0.11331220\n",
      "Iteration 39, loss = 0.11104101\n",
      "Iteration 40, loss = 0.10881986\n",
      "Iteration 41, loss = 0.10678187\n",
      "Iteration 42, loss = 0.10476522\n",
      "Iteration 43, loss = 0.10284662\n",
      "Iteration 44, loss = 0.10099004\n",
      "Iteration 45, loss = 0.09929785\n",
      "Iteration 46, loss = 0.09768091\n",
      "Iteration 47, loss = 0.09611507\n",
      "Iteration 48, loss = 0.09455084\n",
      "Iteration 49, loss = 0.09304488\n",
      "Iteration 50, loss = 0.09174545\n",
      "Iteration 51, loss = 0.09030227\n",
      "Iteration 52, loss = 0.08903431\n",
      "Iteration 53, loss = 0.08774036\n",
      "Iteration 54, loss = 0.08654134\n",
      "Iteration 55, loss = 0.08537876\n",
      "Iteration 56, loss = 0.08425478\n",
      "Iteration 57, loss = 0.08311688\n",
      "Iteration 58, loss = 0.08211911\n",
      "Iteration 59, loss = 0.08101362\n",
      "Iteration 60, loss = 0.08000571\n",
      "Iteration 61, loss = 0.07906688\n",
      "Iteration 62, loss = 0.07811381\n",
      "Iteration 63, loss = 0.07713221\n",
      "Iteration 64, loss = 0.07623920\n",
      "Iteration 65, loss = 0.07538350\n",
      "Iteration 66, loss = 0.07454504\n",
      "Iteration 67, loss = 0.07367689\n",
      "Iteration 68, loss = 0.07293998\n",
      "Iteration 69, loss = 0.07214873\n",
      "Iteration 70, loss = 0.07135023\n",
      "Iteration 71, loss = 0.07057497\n",
      "Iteration 72, loss = 0.06988258\n",
      "Iteration 73, loss = 0.06915444\n",
      "Iteration 74, loss = 0.06846114\n",
      "Iteration 75, loss = 0.06779079\n",
      "Iteration 76, loss = 0.06699421\n",
      "Iteration 77, loss = 0.06636444\n",
      "Iteration 78, loss = 0.06571732\n",
      "Iteration 79, loss = 0.06505637\n",
      "Iteration 80, loss = 0.06442816\n",
      "Iteration 81, loss = 0.06385400\n",
      "Iteration 82, loss = 0.06322168\n",
      "Iteration 83, loss = 0.06257115\n",
      "Iteration 84, loss = 0.06198300\n",
      "Iteration 85, loss = 0.06146545\n",
      "Iteration 86, loss = 0.06089324\n",
      "Iteration 87, loss = 0.06032939\n",
      "Iteration 88, loss = 0.05988234\n",
      "Iteration 89, loss = 0.05923915\n",
      "Iteration 90, loss = 0.05869365\n",
      "Iteration 91, loss = 0.05841245\n",
      "Iteration 92, loss = 0.05770082\n",
      "Iteration 93, loss = 0.05720579\n",
      "Iteration 94, loss = 0.05671238\n",
      "Iteration 95, loss = 0.05624026\n",
      "Iteration 96, loss = 0.05575095\n",
      "Iteration 97, loss = 0.05533086\n",
      "Iteration 98, loss = 0.05483846\n",
      "Iteration 99, loss = 0.05437175\n",
      "Iteration 100, loss = 0.05392753\n",
      "Iteration 101, loss = 0.05345793\n",
      "Iteration 102, loss = 0.05303651\n",
      "Iteration 103, loss = 0.05265592\n",
      "Iteration 104, loss = 0.05220619\n",
      "Iteration 105, loss = 0.05181266\n",
      "Iteration 106, loss = 0.05142335\n",
      "Iteration 107, loss = 0.05105003\n",
      "Iteration 108, loss = 0.05062008\n",
      "Iteration 109, loss = 0.05022827\n",
      "Iteration 110, loss = 0.04989283\n",
      "Iteration 111, loss = 0.04949969\n",
      "Iteration 112, loss = 0.04918719\n",
      "Iteration 113, loss = 0.04876157\n",
      "Iteration 114, loss = 0.04845130\n",
      "Iteration 115, loss = 0.04806610\n",
      "Iteration 116, loss = 0.04772251\n",
      "Iteration 117, loss = 0.04739015\n",
      "Iteration 118, loss = 0.04699981\n",
      "Iteration 119, loss = 0.04672016\n",
      "Iteration 120, loss = 0.04631691\n",
      "Iteration 121, loss = 0.04600844\n",
      "Iteration 122, loss = 0.04572440\n",
      "Iteration 123, loss = 0.04543691\n",
      "Iteration 124, loss = 0.04509615\n",
      "Iteration 125, loss = 0.04471603\n",
      "Iteration 126, loss = 0.04441197\n",
      "Iteration 127, loss = 0.04413205\n",
      "Iteration 128, loss = 0.04379617\n",
      "Iteration 129, loss = 0.04352098\n",
      "Iteration 130, loss = 0.04326126\n",
      "Iteration 131, loss = 0.04302440\n",
      "Iteration 132, loss = 0.04269099\n",
      "Iteration 133, loss = 0.04247061\n",
      "Iteration 134, loss = 0.04211069\n",
      "Iteration 135, loss = 0.04188443\n",
      "Iteration 136, loss = 0.04166538\n",
      "Iteration 137, loss = 0.04133043\n",
      "Iteration 138, loss = 0.04106541\n",
      "Iteration 139, loss = 0.04081122\n",
      "Iteration 140, loss = 0.04062878\n",
      "Iteration 141, loss = 0.04041573\n",
      "Iteration 142, loss = 0.04008030\n",
      "Iteration 143, loss = 0.03981752\n",
      "Iteration 144, loss = 0.03959621\n",
      "Iteration 145, loss = 0.03940266\n",
      "Iteration 146, loss = 0.03911973\n",
      "Iteration 147, loss = 0.03892796\n",
      "Iteration 148, loss = 0.03864928\n",
      "Iteration 149, loss = 0.03844040\n",
      "Iteration 150, loss = 0.03822625\n",
      "Iteration 151, loss = 0.03802841\n",
      "Iteration 152, loss = 0.03777577\n",
      "Iteration 153, loss = 0.03767198\n",
      "Iteration 154, loss = 0.03733391\n",
      "Iteration 155, loss = 0.03716934\n",
      "Iteration 156, loss = 0.03695444\n",
      "Iteration 157, loss = 0.03672597\n",
      "Iteration 158, loss = 0.03649118\n",
      "Iteration 159, loss = 0.03634981\n",
      "Iteration 160, loss = 0.03620621\n",
      "Iteration 161, loss = 0.03595703\n",
      "Iteration 162, loss = 0.03575631\n",
      "Iteration 163, loss = 0.03555527\n",
      "Iteration 164, loss = 0.03536529\n",
      "Iteration 165, loss = 0.03518591\n",
      "Iteration 166, loss = 0.03499319\n",
      "Iteration 167, loss = 0.03486902\n",
      "Iteration 168, loss = 0.03460401\n",
      "Iteration 169, loss = 0.03447499\n",
      "Iteration 170, loss = 0.03429693\n",
      "Iteration 171, loss = 0.03413520\n",
      "Iteration 172, loss = 0.03386122\n",
      "Iteration 173, loss = 0.03367071\n",
      "Iteration 174, loss = 0.03350613\n",
      "Iteration 175, loss = 0.03333640\n",
      "Iteration 176, loss = 0.03315508\n",
      "Iteration 177, loss = 0.03305338\n",
      "Iteration 178, loss = 0.03281015\n",
      "Iteration 179, loss = 0.03262282\n",
      "Iteration 180, loss = 0.03247049\n",
      "Iteration 181, loss = 0.03229169\n",
      "Iteration 182, loss = 0.03214163\n",
      "Iteration 183, loss = 0.03203093\n",
      "Iteration 184, loss = 0.03182768\n",
      "Iteration 185, loss = 0.03171657\n",
      "Iteration 186, loss = 0.03148279\n",
      "Iteration 187, loss = 0.03143196\n",
      "Iteration 188, loss = 0.03119247\n",
      "Iteration 189, loss = 0.03104953\n",
      "Iteration 190, loss = 0.03089285\n",
      "Iteration 191, loss = 0.03073008\n",
      "Iteration 192, loss = 0.03067320\n",
      "Iteration 193, loss = 0.03055539\n",
      "Iteration 194, loss = 0.03029638\n",
      "Iteration 195, loss = 0.03017452\n",
      "Iteration 196, loss = 0.03003087\n",
      "Iteration 197, loss = 0.02992489\n",
      "Iteration 198, loss = 0.02972376\n",
      "Iteration 199, loss = 0.02962027\n",
      "Iteration 200, loss = 0.02948026\n",
      "Iteration 201, loss = 0.02934620\n",
      "Iteration 202, loss = 0.02922151\n",
      "Iteration 203, loss = 0.02906900\n",
      "Iteration 204, loss = 0.02893606\n",
      "Iteration 205, loss = 0.02886961\n",
      "Iteration 206, loss = 0.02866770\n",
      "Iteration 207, loss = 0.02855352\n",
      "Iteration 208, loss = 0.02839876\n",
      "Iteration 209, loss = 0.02824539\n",
      "Iteration 210, loss = 0.02813024\n",
      "Iteration 211, loss = 0.02801787\n",
      "Iteration 212, loss = 0.02793139\n",
      "Iteration 213, loss = 0.02775820\n",
      "Iteration 214, loss = 0.02763031\n",
      "Iteration 215, loss = 0.02750395\n",
      "Iteration 216, loss = 0.02751728\n",
      "Iteration 217, loss = 0.02744378\n",
      "Iteration 218, loss = 0.02720560\n",
      "Iteration 219, loss = 0.02712200\n",
      "Iteration 220, loss = 0.02692872\n",
      "Iteration 221, loss = 0.02682416\n",
      "Iteration 222, loss = 0.02672467\n",
      "Iteration 223, loss = 0.02663392\n",
      "Iteration 224, loss = 0.02648180\n",
      "Iteration 225, loss = 0.02640318\n",
      "Iteration 226, loss = 0.02630633\n",
      "Iteration 227, loss = 0.02618932\n",
      "Iteration 228, loss = 0.02602620\n",
      "Iteration 229, loss = 0.02595247\n",
      "Iteration 230, loss = 0.02592260\n",
      "Iteration 231, loss = 0.02578185\n",
      "Iteration 232, loss = 0.02558701\n",
      "Iteration 233, loss = 0.02570948\n",
      "Iteration 234, loss = 0.02546880\n",
      "Iteration 235, loss = 0.02532501\n",
      "Iteration 236, loss = 0.02522988\n",
      "Iteration 237, loss = 0.02510973\n",
      "Iteration 238, loss = 0.02504108\n",
      "Iteration 239, loss = 0.02492383\n",
      "Iteration 240, loss = 0.02484346\n",
      "Iteration 241, loss = 0.02480656\n",
      "Iteration 242, loss = 0.02463285\n",
      "Iteration 243, loss = 0.02459693\n",
      "Iteration 244, loss = 0.02449132\n",
      "Iteration 245, loss = 0.02435904\n",
      "Iteration 246, loss = 0.02428095\n",
      "Iteration 247, loss = 0.02422667\n",
      "Iteration 248, loss = 0.02409789\n",
      "Iteration 249, loss = 0.02402102\n",
      "Iteration 250, loss = 0.02391740\n",
      "Iteration 251, loss = 0.02378201\n",
      "Iteration 252, loss = 0.02369422\n",
      "Iteration 253, loss = 0.02364868\n",
      "Iteration 254, loss = 0.02352844\n",
      "Iteration 255, loss = 0.02342711\n",
      "Iteration 256, loss = 0.02334385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 257, loss = 0.02335738\n",
      "Iteration 258, loss = 0.02319567\n",
      "Iteration 259, loss = 0.02311473\n",
      "Iteration 260, loss = 0.02302130\n",
      "Iteration 261, loss = 0.02297715\n",
      "Iteration 262, loss = 0.02282423\n",
      "Iteration 263, loss = 0.02277743\n",
      "Iteration 264, loss = 0.02266343\n",
      "Iteration 265, loss = 0.02260689\n",
      "Iteration 266, loss = 0.02265587\n",
      "Iteration 267, loss = 0.02244284\n",
      "Iteration 268, loss = 0.02240229\n",
      "Iteration 269, loss = 0.02234699\n",
      "Iteration 270, loss = 0.02221413\n",
      "Iteration 271, loss = 0.02212647\n",
      "Iteration 272, loss = 0.02205781\n",
      "Iteration 273, loss = 0.02207900\n",
      "Iteration 274, loss = 0.02187877\n",
      "Iteration 275, loss = 0.02183735\n",
      "Iteration 276, loss = 0.02176282\n",
      "Iteration 277, loss = 0.02164764\n",
      "Iteration 278, loss = 0.02161888\n",
      "Iteration 279, loss = 0.02149659\n",
      "Iteration 280, loss = 0.02155487\n",
      "Iteration 281, loss = 0.02139757\n",
      "Iteration 282, loss = 0.02128037\n",
      "Iteration 283, loss = 0.02125546\n",
      "Iteration 284, loss = 0.02118332\n",
      "Iteration 285, loss = 0.02110380\n",
      "Iteration 286, loss = 0.02100942\n",
      "Iteration 287, loss = 0.02092403\n",
      "Iteration 288, loss = 0.02090777\n",
      "Iteration 289, loss = 0.02079301\n",
      "Iteration 290, loss = 0.02070035\n",
      "Iteration 291, loss = 0.02078083\n",
      "Iteration 292, loss = 0.02064592\n",
      "Iteration 293, loss = 0.02069427\n",
      "Iteration 294, loss = 0.02064808\n",
      "Iteration 295, loss = 0.02050020\n",
      "Iteration 296, loss = 0.02041326\n",
      "Iteration 297, loss = 0.02031918\n",
      "Iteration 298, loss = 0.02018651\n",
      "Iteration 299, loss = 0.02010945\n",
      "Iteration 300, loss = 0.02013535\n",
      "Iteration 301, loss = 0.01997708\n",
      "Iteration 302, loss = 0.01992711\n",
      "Iteration 303, loss = 0.01986893\n",
      "Iteration 304, loss = 0.01977138\n",
      "Iteration 305, loss = 0.01986416\n",
      "Iteration 306, loss = 0.01979231\n",
      "Iteration 307, loss = 0.01960707\n",
      "Iteration 308, loss = 0.01954088\n",
      "Iteration 309, loss = 0.01952234\n",
      "Iteration 310, loss = 0.01948960\n",
      "Iteration 311, loss = 0.01933783\n",
      "Iteration 312, loss = 0.01927403\n",
      "Iteration 313, loss = 0.01921817\n",
      "Iteration 314, loss = 0.01916922\n",
      "Iteration 315, loss = 0.01909643\n",
      "Iteration 316, loss = 0.01910939\n",
      "Iteration 317, loss = 0.01905030\n",
      "Iteration 318, loss = 0.01903930\n",
      "Iteration 319, loss = 0.01894822\n",
      "Iteration 320, loss = 0.01883404\n",
      "Iteration 321, loss = 0.01886223\n",
      "Iteration 322, loss = 0.01872268\n",
      "Iteration 323, loss = 0.01874793\n",
      "Iteration 324, loss = 0.01859376\n",
      "Iteration 325, loss = 0.01854053\n",
      "Iteration 326, loss = 0.01850598\n",
      "Iteration 327, loss = 0.01840322\n",
      "Iteration 328, loss = 0.01836301\n",
      "Iteration 329, loss = 0.01833098\n",
      "Iteration 330, loss = 0.01829342\n",
      "Iteration 331, loss = 0.01821859\n",
      "Iteration 332, loss = 0.01830938\n",
      "Iteration 333, loss = 0.01813378\n",
      "Iteration 334, loss = 0.01812509\n",
      "Iteration 335, loss = 0.01803509\n",
      "Iteration 336, loss = 0.01793399\n",
      "Iteration 337, loss = 0.01798854\n",
      "Iteration 338, loss = 0.01784978\n",
      "Iteration 339, loss = 0.01779205\n",
      "Iteration 340, loss = 0.01770102\n",
      "Iteration 341, loss = 0.01767866\n",
      "Iteration 342, loss = 0.01765349\n",
      "Iteration 343, loss = 0.01756587\n",
      "Iteration 344, loss = 0.01754039\n",
      "Iteration 345, loss = 0.01745580\n",
      "Iteration 346, loss = 0.01748444\n",
      "Iteration 347, loss = 0.01739791\n",
      "Iteration 348, loss = 0.01739301\n",
      "Iteration 349, loss = 0.01736716\n",
      "Iteration 350, loss = 0.01728510\n",
      "Iteration 351, loss = 0.01719950\n",
      "Iteration 352, loss = 0.01712037\n",
      "Iteration 353, loss = 0.01707985\n",
      "Iteration 354, loss = 0.01704546\n",
      "Iteration 355, loss = 0.01702079\n",
      "Iteration 356, loss = 0.01693629\n",
      "Iteration 357, loss = 0.01701609\n",
      "Iteration 358, loss = 0.01687368\n",
      "Iteration 359, loss = 0.01686337\n",
      "Iteration 360, loss = 0.01686238\n",
      "Iteration 361, loss = 0.01671500\n",
      "Iteration 362, loss = 0.01678436\n",
      "Iteration 363, loss = 0.01665884\n",
      "Iteration 364, loss = 0.01656688\n",
      "Iteration 365, loss = 0.01655176\n",
      "Iteration 366, loss = 0.01649669\n",
      "Iteration 367, loss = 0.01640049\n",
      "Iteration 368, loss = 0.01640713\n",
      "Iteration 369, loss = 0.01632938\n",
      "Iteration 370, loss = 0.01629800\n",
      "Iteration 371, loss = 0.01631124\n",
      "Iteration 372, loss = 0.01622703\n",
      "Iteration 373, loss = 0.01614176\n",
      "Iteration 374, loss = 0.01608213\n",
      "Iteration 375, loss = 0.01612632\n",
      "Iteration 376, loss = 0.01601142\n",
      "Iteration 377, loss = 0.01594305\n",
      "Iteration 378, loss = 0.01596764\n",
      "Iteration 379, loss = 0.01592904\n",
      "Iteration 380, loss = 0.01588758\n",
      "Iteration 381, loss = 0.01579831\n",
      "Iteration 382, loss = 0.01573728\n",
      "Iteration 383, loss = 0.01570596\n",
      "Iteration 384, loss = 0.01579381\n",
      "Iteration 385, loss = 0.01561080\n",
      "Iteration 386, loss = 0.01563371\n",
      "Iteration 387, loss = 0.01554849\n",
      "Iteration 388, loss = 0.01554652\n",
      "Iteration 389, loss = 0.01546169\n",
      "Iteration 390, loss = 0.01541544\n",
      "Iteration 391, loss = 0.01565046\n",
      "Iteration 392, loss = 0.01532281\n",
      "Iteration 393, loss = 0.01533378\n",
      "Iteration 394, loss = 0.01529889\n",
      "Iteration 395, loss = 0.01527508\n",
      "Iteration 396, loss = 0.01516641\n",
      "Iteration 397, loss = 0.01512487\n",
      "Iteration 398, loss = 0.01515312\n",
      "Iteration 399, loss = 0.01510557\n",
      "Iteration 400, loss = 0.01501771\n",
      "Iteration 401, loss = 0.01501653\n",
      "Iteration 402, loss = 0.01498506\n",
      "Iteration 403, loss = 0.01490362\n",
      "Iteration 404, loss = 0.01491445\n",
      "Iteration 405, loss = 0.01490443\n",
      "Iteration 406, loss = 0.01488944\n",
      "Iteration 407, loss = 0.01483090\n",
      "Iteration 408, loss = 0.01473856\n",
      "Iteration 409, loss = 0.01471789\n",
      "Iteration 410, loss = 0.01469924\n",
      "Iteration 411, loss = 0.01461714\n",
      "Iteration 412, loss = 0.01466454\n",
      "Iteration 413, loss = 0.01450879\n",
      "Iteration 414, loss = 0.01451670\n",
      "Iteration 415, loss = 0.01447675\n",
      "Iteration 416, loss = 0.01451154\n",
      "Iteration 417, loss = 0.01443749\n",
      "Iteration 418, loss = 0.01440019\n",
      "Iteration 419, loss = 0.01437877\n",
      "Iteration 420, loss = 0.01435681\n",
      "Iteration 421, loss = 0.01424689\n",
      "Iteration 422, loss = 0.01423370\n",
      "Iteration 423, loss = 0.01423615\n",
      "Iteration 424, loss = 0.01427764\n",
      "Iteration 425, loss = 0.01415327\n",
      "Iteration 426, loss = 0.01407832\n",
      "Iteration 427, loss = 0.01407693\n",
      "Iteration 428, loss = 0.01405684\n",
      "Iteration 429, loss = 0.01400834\n",
      "Iteration 430, loss = 0.01391593\n",
      "Iteration 431, loss = 0.01392375\n",
      "Iteration 432, loss = 0.01389128\n",
      "Iteration 433, loss = 0.01386755\n",
      "Iteration 434, loss = 0.01383648\n",
      "Iteration 435, loss = 0.01374213\n",
      "Iteration 436, loss = 0.01372480\n",
      "Iteration 437, loss = 0.01378249\n",
      "Iteration 438, loss = 0.01367492\n",
      "Iteration 439, loss = 0.01368244\n",
      "Iteration 440, loss = 0.01364369\n",
      "Iteration 441, loss = 0.01356561\n",
      "Iteration 442, loss = 0.01357662\n",
      "Iteration 443, loss = 0.01358382\n",
      "Iteration 444, loss = 0.01348708\n",
      "Iteration 445, loss = 0.01346800\n",
      "Iteration 446, loss = 0.01346278\n",
      "Iteration 447, loss = 0.01340214\n",
      "Iteration 448, loss = 0.01350184\n",
      "Iteration 449, loss = 0.01329332\n",
      "Iteration 450, loss = 0.01323912\n",
      "Iteration 451, loss = 0.01323850\n",
      "Iteration 452, loss = 0.01331198\n",
      "Iteration 453, loss = 0.01320162\n",
      "Iteration 454, loss = 0.01331844\n",
      "Iteration 455, loss = 0.01312560\n",
      "Iteration 456, loss = 0.01315768\n",
      "Iteration 457, loss = 0.01310880\n",
      "Iteration 458, loss = 0.01326488\n",
      "Iteration 459, loss = 0.01307949\n",
      "Iteration 460, loss = 0.01304792\n",
      "Iteration 461, loss = 0.01294100\n",
      "Iteration 462, loss = 0.01294606\n",
      "Iteration 463, loss = 0.01287066\n",
      "Iteration 464, loss = 0.01293725\n",
      "Iteration 465, loss = 0.01293160\n",
      "Iteration 466, loss = 0.01292749\n",
      "Iteration 467, loss = 0.01287936\n",
      "Iteration 468, loss = 0.01292791\n",
      "Iteration 469, loss = 0.01272806\n",
      "Iteration 470, loss = 0.01273826\n",
      "Iteration 471, loss = 0.01264779\n",
      "Iteration 472, loss = 0.01261346\n",
      "Iteration 473, loss = 0.01261303\n",
      "Iteration 474, loss = 0.01259264\n",
      "Iteration 475, loss = 0.01253213\n",
      "Iteration 476, loss = 0.01259504\n",
      "Iteration 477, loss = 0.01244144\n",
      "Iteration 478, loss = 0.01247405\n",
      "Iteration 479, loss = 0.01253888\n",
      "Iteration 480, loss = 0.01239427\n",
      "Iteration 481, loss = 0.01237391\n",
      "Iteration 482, loss = 0.01240007\n",
      "Iteration 483, loss = 0.01234847\n",
      "Iteration 484, loss = 0.01232804\n",
      "Iteration 485, loss = 0.01227691\n",
      "Iteration 486, loss = 0.01220851\n",
      "Iteration 487, loss = 0.01223074\n",
      "Iteration 488, loss = 0.01226247\n",
      "Iteration 489, loss = 0.01220338\n",
      "Iteration 490, loss = 0.01215046\n",
      "Iteration 491, loss = 0.01210503\n",
      "Iteration 492, loss = 0.01207295\n",
      "Iteration 493, loss = 0.01209462\n",
      "Iteration 494, loss = 0.01206868\n",
      "Iteration 495, loss = 0.01204861\n",
      "Iteration 496, loss = 0.01195964\n",
      "Iteration 497, loss = 0.01191424\n",
      "Iteration 498, loss = 0.01193542\n",
      "Iteration 499, loss = 0.01194495\n",
      "Iteration 500, loss = 0.01191778\n",
      "Iteration 501, loss = 0.01183780\n",
      "Iteration 502, loss = 0.01183248\n",
      "Iteration 503, loss = 0.01182677\n",
      "Iteration 504, loss = 0.01192778\n",
      "Iteration 505, loss = 0.01175799\n",
      "Iteration 506, loss = 0.01178082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 507, loss = 0.01171182\n",
      "Iteration 508, loss = 0.01165731\n",
      "Iteration 509, loss = 0.01166155\n",
      "Iteration 510, loss = 0.01168340\n",
      "Iteration 511, loss = 0.01169940\n",
      "Iteration 512, loss = 0.01156252\n",
      "Iteration 513, loss = 0.01152803\n",
      "Iteration 514, loss = 0.01149843\n",
      "Iteration 515, loss = 0.01150483\n",
      "Iteration 516, loss = 0.01154159\n",
      "Iteration 517, loss = 0.01148071\n",
      "Iteration 518, loss = 0.01143203\n",
      "Iteration 519, loss = 0.01141393\n",
      "Iteration 520, loss = 0.01141612\n",
      "Iteration 521, loss = 0.01144898\n",
      "Iteration 522, loss = 0.01135513\n",
      "Iteration 523, loss = 0.01140128\n",
      "Iteration 524, loss = 0.01145849\n",
      "Iteration 525, loss = 0.01120804\n",
      "Iteration 526, loss = 0.01122793\n",
      "Iteration 527, loss = 0.01119514\n",
      "Iteration 528, loss = 0.01119892\n",
      "Iteration 529, loss = 0.01117886\n",
      "Iteration 530, loss = 0.01109893\n",
      "Iteration 531, loss = 0.01113244\n",
      "Iteration 532, loss = 0.01113738\n",
      "Iteration 533, loss = 0.01113659\n",
      "Iteration 534, loss = 0.01105319\n",
      "Iteration 535, loss = 0.01111000\n",
      "Iteration 536, loss = 0.01102772\n",
      "Iteration 537, loss = 0.01098772\n",
      "Iteration 538, loss = 0.01099949\n",
      "Iteration 539, loss = 0.01095644\n",
      "Iteration 540, loss = 0.01094483\n",
      "Iteration 541, loss = 0.01093920\n",
      "Iteration 542, loss = 0.01083945\n",
      "Iteration 543, loss = 0.01085860\n",
      "Iteration 544, loss = 0.01085564\n",
      "Iteration 545, loss = 0.01077346\n",
      "Iteration 546, loss = 0.01089494\n",
      "Iteration 547, loss = 0.01078777\n",
      "Iteration 548, loss = 0.01076790\n",
      "Iteration 549, loss = 0.01069057\n",
      "Iteration 550, loss = 0.01063018\n",
      "Iteration 551, loss = 0.01068384\n",
      "Iteration 552, loss = 0.01071358\n",
      "Iteration 553, loss = 0.01073311\n",
      "Iteration 554, loss = 0.01064639\n",
      "Iteration 555, loss = 0.01066138\n",
      "Iteration 556, loss = 0.01061349\n",
      "Iteration 557, loss = 0.01058141\n",
      "Iteration 558, loss = 0.01061787\n",
      "Iteration 559, loss = 0.01047732\n",
      "Iteration 560, loss = 0.01046173\n",
      "Iteration 561, loss = 0.01047534\n",
      "Iteration 562, loss = 0.01040810\n",
      "Iteration 563, loss = 0.01048965\n",
      "Iteration 564, loss = 0.01034892\n",
      "Iteration 565, loss = 0.01042923\n",
      "Iteration 566, loss = 0.01042091\n",
      "Iteration 567, loss = 0.01036312\n",
      "Iteration 568, loss = 0.01027454\n",
      "Iteration 569, loss = 0.01031674\n",
      "Iteration 570, loss = 0.01028762\n",
      "Iteration 571, loss = 0.01024003\n",
      "Iteration 572, loss = 0.01019936\n",
      "Iteration 573, loss = 0.01025345\n",
      "Iteration 574, loss = 0.01027791\n",
      "Iteration 575, loss = 0.01021397\n",
      "Iteration 576, loss = 0.01021788\n",
      "Iteration 577, loss = 0.01010173\n",
      "Iteration 578, loss = 0.01011262\n",
      "Iteration 579, loss = 0.01016513\n",
      "Iteration 580, loss = 0.01025856\n",
      "Iteration 581, loss = 0.01008703\n",
      "Iteration 582, loss = 0.01003670\n",
      "Iteration 583, loss = 0.01005082\n",
      "Iteration 584, loss = 0.01002397\n",
      "Iteration 585, loss = 0.01002731\n",
      "Iteration 586, loss = 0.01019008\n",
      "Iteration 587, loss = 0.00992443\n",
      "Iteration 588, loss = 0.00993720\n",
      "Iteration 589, loss = 0.00991397\n",
      "Iteration 590, loss = 0.00990072\n",
      "Iteration 591, loss = 0.00988177\n",
      "Iteration 592, loss = 0.00997121\n",
      "Iteration 593, loss = 0.00986270\n",
      "Iteration 594, loss = 0.00981532\n",
      "Iteration 595, loss = 0.00985409\n",
      "Iteration 596, loss = 0.00977379\n",
      "Iteration 597, loss = 0.00978537\n",
      "Iteration 598, loss = 0.00978263\n",
      "Iteration 599, loss = 0.00973177\n",
      "Iteration 600, loss = 0.00971794\n",
      "Iteration 601, loss = 0.00970877\n",
      "Iteration 602, loss = 0.00965651\n",
      "Iteration 603, loss = 0.00967487\n",
      "Iteration 604, loss = 0.00966517\n",
      "Iteration 605, loss = 0.00969401\n",
      "Iteration 606, loss = 0.00963032\n",
      "Iteration 607, loss = 0.00953999\n",
      "Iteration 608, loss = 0.00962627\n",
      "Iteration 609, loss = 0.00954253\n",
      "Iteration 610, loss = 0.00951434\n",
      "Iteration 611, loss = 0.00950034\n",
      "Iteration 612, loss = 0.00950108\n",
      "Iteration 613, loss = 0.00946943\n",
      "Iteration 614, loss = 0.00950688\n",
      "Iteration 615, loss = 0.00942501\n",
      "Iteration 616, loss = 0.00941440\n",
      "Iteration 617, loss = 0.00935675\n",
      "Iteration 618, loss = 0.00938921\n",
      "Iteration 619, loss = 0.00955756\n",
      "Iteration 620, loss = 0.00929163\n",
      "Iteration 621, loss = 0.00938630\n",
      "Iteration 622, loss = 0.00932021\n",
      "Iteration 623, loss = 0.00930093\n",
      "Iteration 624, loss = 0.00930235\n",
      "Iteration 625, loss = 0.00934460\n",
      "Iteration 626, loss = 0.00927869\n",
      "Iteration 627, loss = 0.00924784\n",
      "Iteration 628, loss = 0.00928268\n",
      "Iteration 629, loss = 0.00926661\n",
      "Iteration 630, loss = 0.00915187\n",
      "Iteration 631, loss = 0.00926252\n",
      "Iteration 632, loss = 0.00919320\n",
      "Iteration 633, loss = 0.00918109\n",
      "Iteration 634, loss = 0.00912935\n",
      "Iteration 635, loss = 0.00911237\n",
      "Iteration 636, loss = 0.00909069\n",
      "Iteration 637, loss = 0.00913372\n",
      "Iteration 638, loss = 0.00909995\n",
      "Iteration 639, loss = 0.00904152\n",
      "Iteration 640, loss = 0.00906954\n",
      "Iteration 641, loss = 0.00901382\n",
      "Iteration 642, loss = 0.00902818\n",
      "Iteration 643, loss = 0.00895866\n",
      "Iteration 644, loss = 0.00900041\n",
      "Iteration 645, loss = 0.00897750\n",
      "Iteration 646, loss = 0.00889712\n",
      "Iteration 647, loss = 0.00894272\n",
      "Iteration 648, loss = 0.00890563\n",
      "Iteration 649, loss = 0.00887471\n",
      "Iteration 650, loss = 0.00884087\n",
      "Iteration 651, loss = 0.00881522\n",
      "Iteration 652, loss = 0.00885894\n",
      "Iteration 653, loss = 0.00884616\n",
      "Iteration 654, loss = 0.00881695\n",
      "Iteration 655, loss = 0.00890201\n",
      "Iteration 656, loss = 0.00882152\n",
      "Iteration 657, loss = 0.00872371\n",
      "Iteration 658, loss = 0.00870868\n",
      "Iteration 659, loss = 0.00869525\n",
      "Iteration 660, loss = 0.00869714\n",
      "Iteration 661, loss = 0.00879707\n",
      "Iteration 662, loss = 0.00871176\n",
      "Iteration 663, loss = 0.00873596\n",
      "Iteration 664, loss = 0.00871724\n",
      "Iteration 665, loss = 0.00856629\n",
      "Iteration 666, loss = 0.00867731\n",
      "Iteration 667, loss = 0.00865579\n",
      "Iteration 668, loss = 0.00850726\n",
      "Iteration 669, loss = 0.00863493\n",
      "Iteration 670, loss = 0.00855717\n",
      "Iteration 671, loss = 0.00854190\n",
      "Iteration 672, loss = 0.00852270\n",
      "Iteration 673, loss = 0.00856435\n",
      "Iteration 674, loss = 0.00848815\n",
      "Iteration 675, loss = 0.00849638\n",
      "Iteration 676, loss = 0.00851897\n",
      "Iteration 677, loss = 0.00852442\n",
      "Iteration 678, loss = 0.00845933\n",
      "Iteration 679, loss = 0.00840447\n",
      "Iteration 680, loss = 0.00842268\n",
      "Iteration 681, loss = 0.00840331\n",
      "Iteration 682, loss = 0.00837506\n",
      "Iteration 683, loss = 0.00837913\n",
      "Iteration 684, loss = 0.00836038\n",
      "Iteration 685, loss = 0.00834489\n",
      "Iteration 686, loss = 0.00833640\n",
      "Iteration 687, loss = 0.00830654\n",
      "Iteration 688, loss = 0.00835700\n",
      "Iteration 689, loss = 0.00830251\n",
      "Iteration 690, loss = 0.00833937\n",
      "Iteration 691, loss = 0.00827342\n",
      "Iteration 692, loss = 0.00827884\n",
      "Iteration 693, loss = 0.00823824\n",
      "Iteration 694, loss = 0.00823943\n",
      "Iteration 695, loss = 0.00821551\n",
      "Iteration 696, loss = 0.00829382\n",
      "Iteration 697, loss = 0.00825323\n",
      "Iteration 698, loss = 0.00830419\n",
      "Iteration 699, loss = 0.00826094\n",
      "Iteration 700, loss = 0.00827337\n",
      "Iteration 701, loss = 0.00822553\n",
      "Iteration 702, loss = 0.00812090\n",
      "Iteration 703, loss = 0.00810531\n",
      "Iteration 704, loss = 0.00806361\n",
      "Iteration 705, loss = 0.00814140\n",
      "Iteration 706, loss = 0.00801626\n",
      "Iteration 707, loss = 0.00807732\n",
      "Iteration 708, loss = 0.00803539\n",
      "Iteration 709, loss = 0.00797671\n",
      "Iteration 710, loss = 0.00810664\n",
      "Iteration 711, loss = 0.00795869\n",
      "Iteration 712, loss = 0.00804735\n",
      "Iteration 713, loss = 0.00792389\n",
      "Iteration 714, loss = 0.00805909\n",
      "Iteration 715, loss = 0.00801616\n",
      "Iteration 716, loss = 0.00790061\n",
      "Iteration 717, loss = 0.00795976\n",
      "Iteration 718, loss = 0.00793512\n",
      "Iteration 719, loss = 0.00784615\n",
      "Iteration 720, loss = 0.00787709\n",
      "Iteration 721, loss = 0.00786282\n",
      "Iteration 722, loss = 0.00781453\n",
      "Iteration 723, loss = 0.00784631\n",
      "Iteration 724, loss = 0.00779560\n",
      "Iteration 725, loss = 0.00783715\n",
      "Iteration 726, loss = 0.00778686\n",
      "Iteration 727, loss = 0.00779119\n",
      "Iteration 728, loss = 0.00782414\n",
      "Iteration 729, loss = 0.00781587\n",
      "Iteration 730, loss = 0.00772628\n",
      "Iteration 731, loss = 0.00775717\n",
      "Iteration 732, loss = 0.00771038\n",
      "Iteration 733, loss = 0.00769296\n",
      "Iteration 734, loss = 0.00771915\n",
      "Iteration 735, loss = 0.00766654\n",
      "Iteration 736, loss = 0.00764733\n",
      "Iteration 737, loss = 0.00767157\n",
      "Iteration 738, loss = 0.00768871\n",
      "Iteration 739, loss = 0.00762937\n",
      "Iteration 740, loss = 0.00765599\n",
      "Iteration 741, loss = 0.00757751\n",
      "Iteration 742, loss = 0.00759848\n",
      "Iteration 743, loss = 0.00757520\n",
      "Iteration 744, loss = 0.00760039\n",
      "Iteration 745, loss = 0.00771067\n",
      "Iteration 746, loss = 0.00766992\n",
      "Iteration 747, loss = 0.00755727\n",
      "Iteration 748, loss = 0.00749667\n",
      "Iteration 749, loss = 0.00754391\n",
      "Iteration 750, loss = 0.00747662\n",
      "Iteration 751, loss = 0.00758781\n",
      "Iteration 752, loss = 0.00749022\n",
      "Iteration 753, loss = 0.00744821\n",
      "Iteration 754, loss = 0.00742116\n",
      "Iteration 755, loss = 0.00743815\n",
      "Iteration 756, loss = 0.00753116\n",
      "Iteration 757, loss = 0.00736493\n",
      "Iteration 758, loss = 0.00737713\n",
      "Iteration 759, loss = 0.00741705\n",
      "Iteration 760, loss = 0.00746925\n",
      "Iteration 761, loss = 0.00737408\n",
      "Iteration 762, loss = 0.00738555\n",
      "Iteration 763, loss = 0.00734393\n",
      "Iteration 764, loss = 0.00737460\n",
      "Iteration 765, loss = 0.00728561\n",
      "Iteration 766, loss = 0.00740992\n",
      "Iteration 767, loss = 0.00734737\n",
      "Iteration 768, loss = 0.00729205\n",
      "Iteration 769, loss = 0.00724070\n",
      "Iteration 770, loss = 0.00732381\n",
      "Iteration 771, loss = 0.00724326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 772, loss = 0.00729022\n",
      "Iteration 773, loss = 0.00721254\n",
      "Iteration 774, loss = 0.00724371\n",
      "Iteration 775, loss = 0.00722572\n",
      "Iteration 776, loss = 0.00728193\n",
      "Iteration 777, loss = 0.00716186\n",
      "Iteration 778, loss = 0.00716864\n",
      "Iteration 779, loss = 0.00713872\n",
      "Iteration 780, loss = 0.00717376\n",
      "Iteration 781, loss = 0.00710307\n",
      "Iteration 782, loss = 0.00717370\n",
      "Iteration 783, loss = 0.00717655\n",
      "Iteration 784, loss = 0.00708278\n",
      "Iteration 785, loss = 0.00707795\n",
      "Iteration 786, loss = 0.00711705\n",
      "Iteration 787, loss = 0.00709264\n",
      "Iteration 788, loss = 0.00714381\n",
      "Iteration 789, loss = 0.00711537\n",
      "Iteration 790, loss = 0.00700939\n",
      "Iteration 791, loss = 0.00702741\n",
      "Iteration 792, loss = 0.00705357\n",
      "Iteration 793, loss = 0.00710868\n",
      "Iteration 794, loss = 0.00700042\n",
      "Iteration 795, loss = 0.00700778\n",
      "Iteration 796, loss = 0.00701291\n",
      "Iteration 797, loss = 0.00693098\n",
      "Iteration 798, loss = 0.00694446\n",
      "Iteration 799, loss = 0.00697952\n",
      "Iteration 800, loss = 0.00695702\n",
      "Iteration 801, loss = 0.00698129\n",
      "Iteration 802, loss = 0.00693550\n",
      "Iteration 803, loss = 0.00686207\n",
      "Iteration 804, loss = 0.00694390\n",
      "Iteration 805, loss = 0.00696338\n",
      "Iteration 806, loss = 0.00687516\n",
      "Iteration 807, loss = 0.00684860\n",
      "Iteration 808, loss = 0.00685396\n",
      "Iteration 809, loss = 0.00682128\n",
      "Iteration 810, loss = 0.00683831\n",
      "Iteration 811, loss = 0.00686855\n",
      "Iteration 812, loss = 0.00687150\n",
      "Iteration 813, loss = 0.00681252\n",
      "Iteration 814, loss = 0.00681361\n",
      "Iteration 815, loss = 0.00680000\n",
      "Iteration 816, loss = 0.00688599\n",
      "Iteration 817, loss = 0.00675323\n",
      "Iteration 818, loss = 0.00670958\n",
      "Iteration 819, loss = 0.00674723\n",
      "Iteration 820, loss = 0.00668852\n",
      "Iteration 821, loss = 0.00669456\n",
      "Iteration 822, loss = 0.00675404\n",
      "Iteration 823, loss = 0.00667886\n",
      "Iteration 824, loss = 0.00672231\n",
      "Iteration 825, loss = 0.00668699\n",
      "Iteration 826, loss = 0.00673040\n",
      "Iteration 827, loss = 0.00667930\n",
      "Iteration 828, loss = 0.00663109\n",
      "Iteration 829, loss = 0.00663365\n",
      "Iteration 830, loss = 0.00660587\n",
      "Iteration 831, loss = 0.00660068\n",
      "Iteration 832, loss = 0.00663088\n",
      "Iteration 833, loss = 0.00657865\n",
      "Iteration 834, loss = 0.00666366\n",
      "Iteration 835, loss = 0.00661647\n",
      "Iteration 836, loss = 0.00664814\n",
      "Iteration 837, loss = 0.00660068\n",
      "Iteration 838, loss = 0.00663751\n",
      "Iteration 839, loss = 0.00649797\n",
      "Iteration 840, loss = 0.00652495\n",
      "Iteration 841, loss = 0.00668342\n",
      "Iteration 842, loss = 0.00650297\n",
      "Iteration 843, loss = 0.00653640\n",
      "Iteration 844, loss = 0.00651754\n",
      "Iteration 845, loss = 0.00652152\n",
      "Iteration 846, loss = 0.00655738\n",
      "Iteration 847, loss = 0.00648270\n",
      "Iteration 848, loss = 0.00647381\n",
      "Iteration 849, loss = 0.00661038\n",
      "Iteration 850, loss = 0.00640546\n",
      "Iteration 851, loss = 0.00645520\n",
      "Iteration 852, loss = 0.00640493\n",
      "Iteration 853, loss = 0.00641231\n",
      "Iteration 854, loss = 0.00638682\n",
      "Iteration 855, loss = 0.00640212\n",
      "Iteration 856, loss = 0.00638367\n",
      "Iteration 857, loss = 0.00634823\n",
      "Iteration 858, loss = 0.00639481\n",
      "Iteration 859, loss = 0.00647894\n",
      "Iteration 860, loss = 0.00634562\n",
      "Iteration 861, loss = 0.00634882\n",
      "Iteration 862, loss = 0.00636029\n",
      "Iteration 863, loss = 0.00636290\n",
      "Iteration 864, loss = 0.00629764\n",
      "Iteration 865, loss = 0.00628099\n",
      "Iteration 866, loss = 0.00634819\n",
      "Iteration 867, loss = 0.00624703\n",
      "Iteration 868, loss = 0.00626997\n",
      "Iteration 869, loss = 0.00625353\n",
      "Iteration 870, loss = 0.00629255\n",
      "Iteration 871, loss = 0.00627897\n",
      "Iteration 872, loss = 0.00625293\n",
      "Iteration 873, loss = 0.00623343\n",
      "Iteration 874, loss = 0.00624214\n",
      "Iteration 875, loss = 0.00619869\n",
      "Iteration 876, loss = 0.00646868\n",
      "Iteration 877, loss = 0.00616191\n",
      "Iteration 878, loss = 0.00622809\n",
      "Iteration 879, loss = 0.00635122\n",
      "Iteration 880, loss = 0.00622569\n",
      "Iteration 881, loss = 0.00622969\n",
      "Iteration 882, loss = 0.00616855\n",
      "Iteration 883, loss = 0.00620570\n",
      "Iteration 884, loss = 0.00614665\n",
      "Iteration 885, loss = 0.00611304\n",
      "Iteration 886, loss = 0.00612662\n",
      "Iteration 887, loss = 0.00620704\n",
      "Iteration 888, loss = 0.00611939\n",
      "Iteration 889, loss = 0.00616072\n",
      "Iteration 890, loss = 0.00607198\n",
      "Iteration 891, loss = 0.00610422\n",
      "Iteration 892, loss = 0.00614379\n",
      "Iteration 893, loss = 0.00608136\n",
      "Iteration 894, loss = 0.00612947\n",
      "Iteration 895, loss = 0.00602922\n",
      "Iteration 896, loss = 0.00605303\n",
      "Iteration 897, loss = 0.00619416\n",
      "Iteration 898, loss = 0.00599253\n",
      "Iteration 899, loss = 0.00603283\n",
      "Iteration 900, loss = 0.00607230\n",
      "Iteration 901, loss = 0.00598582\n",
      "Iteration 902, loss = 0.00605076\n",
      "Iteration 903, loss = 0.00596229\n",
      "Iteration 904, loss = 0.00594769\n",
      "Iteration 905, loss = 0.00603686\n",
      "Iteration 906, loss = 0.00597785\n",
      "Iteration 907, loss = 0.00601932\n",
      "Iteration 908, loss = 0.00590052\n",
      "Iteration 909, loss = 0.00595620\n",
      "Iteration 910, loss = 0.00590964\n",
      "Iteration 911, loss = 0.00593223\n",
      "Iteration 912, loss = 0.00588898\n",
      "Iteration 913, loss = 0.00586833\n",
      "Iteration 914, loss = 0.00587166\n",
      "Iteration 915, loss = 0.00589339\n",
      "Iteration 916, loss = 0.00587275\n",
      "Iteration 917, loss = 0.00584412\n",
      "Iteration 918, loss = 0.00593253\n",
      "Iteration 919, loss = 0.00590996\n",
      "Iteration 920, loss = 0.00587740\n",
      "Iteration 921, loss = 0.00586790\n",
      "Iteration 922, loss = 0.00584473\n",
      "Iteration 923, loss = 0.00587870\n",
      "Iteration 924, loss = 0.00584181\n",
      "Iteration 925, loss = 0.00577436\n",
      "Iteration 926, loss = 0.00582919\n",
      "Iteration 927, loss = 0.00576517\n",
      "Iteration 928, loss = 0.00577233\n",
      "Iteration 929, loss = 0.00580161\n",
      "Iteration 930, loss = 0.00575698\n",
      "Iteration 931, loss = 0.00581533\n",
      "Iteration 932, loss = 0.00576378\n",
      "Iteration 933, loss = 0.00573452\n",
      "Iteration 934, loss = 0.00572152\n",
      "Iteration 935, loss = 0.00571801\n",
      "Iteration 936, loss = 0.00569901\n",
      "Iteration 937, loss = 0.00569606\n",
      "Iteration 938, loss = 0.00570004\n",
      "Iteration 939, loss = 0.00568745\n",
      "Iteration 940, loss = 0.00572844\n",
      "Iteration 941, loss = 0.00571583\n",
      "Iteration 942, loss = 0.00568246\n",
      "Iteration 943, loss = 0.00571612\n",
      "Iteration 944, loss = 0.00568836\n",
      "Iteration 945, loss = 0.00565688\n",
      "Iteration 946, loss = 0.00563091\n",
      "Iteration 947, loss = 0.00560599\n",
      "Iteration 948, loss = 0.00567721\n",
      "Iteration 949, loss = 0.00566963\n",
      "Iteration 950, loss = 0.00573014\n",
      "Iteration 951, loss = 0.00559844\n",
      "Iteration 952, loss = 0.00560683\n",
      "Iteration 953, loss = 0.00566214\n",
      "Iteration 954, loss = 0.00560787\n",
      "Iteration 955, loss = 0.00557887\n",
      "Iteration 956, loss = 0.00556871\n",
      "Iteration 957, loss = 0.00555897\n",
      "Iteration 958, loss = 0.00563333\n",
      "Iteration 959, loss = 0.00560390\n",
      "Iteration 960, loss = 0.00552581\n",
      "Iteration 961, loss = 0.00560193\n",
      "Iteration 962, loss = 0.00550736\n",
      "Iteration 963, loss = 0.00557351\n",
      "Iteration 964, loss = 0.00551861\n",
      "Iteration 965, loss = 0.00551471\n",
      "Iteration 966, loss = 0.00551788\n",
      "Iteration 967, loss = 0.00549362\n",
      "Iteration 968, loss = 0.00547329\n",
      "Iteration 969, loss = 0.00549167\n",
      "Iteration 970, loss = 0.00549146\n",
      "Iteration 971, loss = 0.00554350\n",
      "Iteration 972, loss = 0.00543255\n",
      "Iteration 973, loss = 0.00540285\n",
      "Iteration 974, loss = 0.00548361\n",
      "Iteration 975, loss = 0.00543705\n",
      "Iteration 976, loss = 0.00555025\n",
      "Iteration 977, loss = 0.00543308\n",
      "Iteration 978, loss = 0.00550177\n",
      "Iteration 979, loss = 0.00548568\n",
      "Iteration 980, loss = 0.00540076\n",
      "Iteration 981, loss = 0.00541689\n",
      "Iteration 982, loss = 0.00538236\n",
      "Iteration 983, loss = 0.00539441\n",
      "Iteration 984, loss = 0.00535880\n",
      "Iteration 985, loss = 0.00541243\n",
      "Iteration 986, loss = 0.00543260\n",
      "Iteration 987, loss = 0.00535028\n",
      "Iteration 988, loss = 0.00535811\n",
      "Iteration 989, loss = 0.00546980\n",
      "Iteration 990, loss = 0.00534640\n",
      "Iteration 991, loss = 0.00529550\n",
      "Iteration 992, loss = 0.00535244\n",
      "Iteration 993, loss = 0.00536274\n",
      "Iteration 994, loss = 0.00531720\n",
      "Iteration 995, loss = 0.00533595\n",
      "Iteration 996, loss = 0.00525575\n",
      "Iteration 997, loss = 0.00538955\n",
      "Iteration 998, loss = 0.00529633\n",
      "Iteration 999, loss = 0.00524015\n",
      "Iteration 1000, loss = 0.00529215\n",
      "Iteration 1, loss = 0.67258988\n",
      "Iteration 2, loss = 0.60105203\n",
      "Iteration 3, loss = 0.54064337\n",
      "Iteration 4, loss = 0.48904115\n",
      "Iteration 5, loss = 0.44456141\n",
      "Iteration 6, loss = 0.40685718\n",
      "Iteration 7, loss = 0.37435526\n",
      "Iteration 8, loss = 0.34629623\n",
      "Iteration 9, loss = 0.32152292\n",
      "Iteration 10, loss = 0.30000677\n",
      "Iteration 11, loss = 0.28067986\n",
      "Iteration 12, loss = 0.26365134\n",
      "Iteration 13, loss = 0.24840211\n",
      "Iteration 14, loss = 0.23469373\n",
      "Iteration 15, loss = 0.22250395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lindino/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.21139330\n",
      "Iteration 17, loss = 0.20154710\n",
      "Iteration 18, loss = 0.19245605\n",
      "Iteration 19, loss = 0.18416763\n",
      "Iteration 20, loss = 0.17662113\n",
      "Iteration 21, loss = 0.16966566\n",
      "Iteration 22, loss = 0.16342853\n",
      "Iteration 23, loss = 0.15771907\n",
      "Iteration 24, loss = 0.15237692\n",
      "Iteration 25, loss = 0.14757900\n",
      "Iteration 26, loss = 0.14310145\n",
      "Iteration 27, loss = 0.13886582\n",
      "Iteration 28, loss = 0.13498686\n",
      "Iteration 29, loss = 0.13134581\n",
      "Iteration 30, loss = 0.12794364\n",
      "Iteration 31, loss = 0.12473471\n",
      "Iteration 32, loss = 0.12165279\n",
      "Iteration 33, loss = 0.11886163\n",
      "Iteration 34, loss = 0.11626276\n",
      "Iteration 35, loss = 0.11381729\n",
      "Iteration 36, loss = 0.11151877\n",
      "Iteration 37, loss = 0.10935297\n",
      "Iteration 38, loss = 0.10723339\n",
      "Iteration 39, loss = 0.10523608\n",
      "Iteration 40, loss = 0.10340061\n",
      "Iteration 41, loss = 0.10160441\n",
      "Iteration 42, loss = 0.09986865\n",
      "Iteration 43, loss = 0.09817933\n",
      "Iteration 44, loss = 0.09662714\n",
      "Iteration 45, loss = 0.09511492\n",
      "Iteration 46, loss = 0.09361006\n",
      "Iteration 47, loss = 0.09223380\n",
      "Iteration 48, loss = 0.09087101\n",
      "Iteration 49, loss = 0.08948492\n",
      "Iteration 50, loss = 0.08819898\n",
      "Iteration 51, loss = 0.08705115\n",
      "Iteration 52, loss = 0.08580393\n",
      "Iteration 53, loss = 0.08466743\n",
      "Iteration 54, loss = 0.08357338\n",
      "Iteration 55, loss = 0.08252961\n",
      "Iteration 56, loss = 0.08156199\n",
      "Iteration 57, loss = 0.08047358\n",
      "Iteration 58, loss = 0.07950669\n",
      "Iteration 59, loss = 0.07854837\n",
      "Iteration 60, loss = 0.07758839\n",
      "Iteration 61, loss = 0.07671441\n",
      "Iteration 62, loss = 0.07577715\n",
      "Iteration 63, loss = 0.07503591\n",
      "Iteration 64, loss = 0.07413526\n",
      "Iteration 65, loss = 0.07331415\n",
      "Iteration 66, loss = 0.07246528\n",
      "Iteration 67, loss = 0.07168157\n",
      "Iteration 68, loss = 0.07087387\n",
      "Iteration 69, loss = 0.07019597\n",
      "Iteration 70, loss = 0.06943100\n",
      "Iteration 71, loss = 0.06864910\n",
      "Iteration 72, loss = 0.06791550\n",
      "Iteration 73, loss = 0.06724378\n",
      "Iteration 74, loss = 0.06652159\n",
      "Iteration 75, loss = 0.06583419\n",
      "Iteration 76, loss = 0.06517812\n",
      "Iteration 77, loss = 0.06448588\n",
      "Iteration 78, loss = 0.06392755\n",
      "Iteration 79, loss = 0.06327057\n",
      "Iteration 80, loss = 0.06268272\n",
      "Iteration 81, loss = 0.06190492\n",
      "Iteration 82, loss = 0.06131059\n",
      "Iteration 83, loss = 0.06073160\n",
      "Iteration 84, loss = 0.06019974\n",
      "Iteration 85, loss = 0.05953536\n",
      "Iteration 86, loss = 0.05895201\n",
      "Iteration 87, loss = 0.05838905\n",
      "Iteration 88, loss = 0.05785017\n",
      "Iteration 89, loss = 0.05732804\n",
      "Iteration 90, loss = 0.05694975\n",
      "Iteration 91, loss = 0.05646033\n",
      "Iteration 92, loss = 0.05584515\n",
      "Iteration 93, loss = 0.05523679\n",
      "Iteration 94, loss = 0.05470526\n",
      "Iteration 95, loss = 0.05428940\n",
      "Iteration 96, loss = 0.05372111\n",
      "Iteration 97, loss = 0.05328265\n",
      "Iteration 98, loss = 0.05286343\n",
      "Iteration 99, loss = 0.05235548\n",
      "Iteration 100, loss = 0.05188309\n",
      "Iteration 101, loss = 0.05149020\n",
      "Iteration 102, loss = 0.05106958\n",
      "Iteration 103, loss = 0.05059102\n",
      "Iteration 104, loss = 0.05018251\n",
      "Iteration 105, loss = 0.04976182\n",
      "Iteration 106, loss = 0.04932390\n",
      "Iteration 107, loss = 0.04886868\n",
      "Iteration 108, loss = 0.04861484\n",
      "Iteration 109, loss = 0.04819602\n",
      "Iteration 110, loss = 0.04770822\n",
      "Iteration 111, loss = 0.04730154\n",
      "Iteration 112, loss = 0.04697369\n",
      "Iteration 113, loss = 0.04656541\n",
      "Iteration 114, loss = 0.04621665\n",
      "Iteration 115, loss = 0.04583767\n",
      "Iteration 116, loss = 0.04553922\n",
      "Iteration 117, loss = 0.04517335\n",
      "Iteration 118, loss = 0.04480018\n",
      "Iteration 119, loss = 0.04446703\n",
      "Iteration 120, loss = 0.04418082\n",
      "Iteration 121, loss = 0.04383170\n",
      "Iteration 122, loss = 0.04363201\n",
      "Iteration 123, loss = 0.04318197\n",
      "Iteration 124, loss = 0.04285012\n",
      "Iteration 125, loss = 0.04251176\n",
      "Iteration 126, loss = 0.04222057\n",
      "Iteration 127, loss = 0.04206216\n",
      "Iteration 128, loss = 0.04165248\n",
      "Iteration 129, loss = 0.04130941\n",
      "Iteration 130, loss = 0.04107916\n",
      "Iteration 131, loss = 0.04073759\n",
      "Iteration 132, loss = 0.04041700\n",
      "Iteration 133, loss = 0.04013772\n",
      "Iteration 134, loss = 0.03984193\n",
      "Iteration 135, loss = 0.03953418\n",
      "Iteration 136, loss = 0.03930965\n",
      "Iteration 137, loss = 0.03906482\n",
      "Iteration 138, loss = 0.03875174\n",
      "Iteration 139, loss = 0.03847576\n",
      "Iteration 140, loss = 0.03822760\n",
      "Iteration 141, loss = 0.03801977\n",
      "Iteration 142, loss = 0.03767387\n",
      "Iteration 143, loss = 0.03741637\n",
      "Iteration 144, loss = 0.03718183\n",
      "Iteration 145, loss = 0.03699081\n",
      "Iteration 146, loss = 0.03676601\n",
      "Iteration 147, loss = 0.03648886\n",
      "Iteration 148, loss = 0.03625995\n",
      "Iteration 149, loss = 0.03605568\n",
      "Iteration 150, loss = 0.03583606\n",
      "Iteration 151, loss = 0.03568647\n",
      "Iteration 152, loss = 0.03535334\n",
      "Iteration 153, loss = 0.03517078\n",
      "Iteration 154, loss = 0.03502718\n",
      "Iteration 155, loss = 0.03476339\n",
      "Iteration 156, loss = 0.03453339\n",
      "Iteration 157, loss = 0.03430769\n",
      "Iteration 158, loss = 0.03412953\n",
      "Iteration 159, loss = 0.03394717\n",
      "Iteration 160, loss = 0.03375504\n",
      "Iteration 161, loss = 0.03354528\n",
      "Iteration 162, loss = 0.03335948\n",
      "Iteration 163, loss = 0.03313156\n",
      "Iteration 164, loss = 0.03293103\n",
      "Iteration 165, loss = 0.03270805\n",
      "Iteration 166, loss = 0.03254083\n",
      "Iteration 167, loss = 0.03236244\n",
      "Iteration 168, loss = 0.03214850\n",
      "Iteration 169, loss = 0.03201953\n",
      "Iteration 170, loss = 0.03180045\n",
      "Iteration 171, loss = 0.03161565\n",
      "Iteration 172, loss = 0.03150521\n",
      "Iteration 173, loss = 0.03137857\n",
      "Iteration 174, loss = 0.03111830\n",
      "Iteration 175, loss = 0.03100986\n",
      "Iteration 176, loss = 0.03081940\n",
      "Iteration 177, loss = 0.03063802\n",
      "Iteration 178, loss = 0.03044600\n",
      "Iteration 179, loss = 0.03029530\n",
      "Iteration 180, loss = 0.03012911\n",
      "Iteration 181, loss = 0.03006039\n",
      "Iteration 182, loss = 0.02982838\n",
      "Iteration 183, loss = 0.02964244\n",
      "Iteration 184, loss = 0.02955625\n",
      "Iteration 185, loss = 0.02938950\n",
      "Iteration 186, loss = 0.02922217\n",
      "Iteration 187, loss = 0.02911554\n",
      "Iteration 188, loss = 0.02893560\n",
      "Iteration 189, loss = 0.02880048\n",
      "Iteration 190, loss = 0.02867283\n",
      "Iteration 191, loss = 0.02854501\n",
      "Iteration 192, loss = 0.02848895\n",
      "Iteration 193, loss = 0.02823125\n",
      "Iteration 194, loss = 0.02812994\n",
      "Iteration 195, loss = 0.02802554\n",
      "Iteration 196, loss = 0.02786278\n",
      "Iteration 197, loss = 0.02775202\n",
      "Iteration 198, loss = 0.02763352\n",
      "Iteration 199, loss = 0.02761791\n",
      "Iteration 200, loss = 0.02741058\n",
      "Iteration 201, loss = 0.02727454\n",
      "Iteration 202, loss = 0.02708350\n",
      "Iteration 203, loss = 0.02705994\n",
      "Iteration 204, loss = 0.02687457\n",
      "Iteration 205, loss = 0.02680356\n",
      "Iteration 206, loss = 0.02658762\n",
      "Iteration 207, loss = 0.02648943\n",
      "Iteration 208, loss = 0.02643161\n",
      "Iteration 209, loss = 0.02628128\n",
      "Iteration 210, loss = 0.02616659\n",
      "Iteration 211, loss = 0.02603557\n",
      "Iteration 212, loss = 0.02587316\n",
      "Iteration 213, loss = 0.02581917\n",
      "Iteration 214, loss = 0.02571570\n",
      "Iteration 215, loss = 0.02559031\n",
      "Iteration 216, loss = 0.02570489\n",
      "Iteration 217, loss = 0.02547201\n",
      "Iteration 218, loss = 0.02534944\n",
      "Iteration 219, loss = 0.02518590\n",
      "Iteration 220, loss = 0.02504233\n",
      "Iteration 221, loss = 0.02491770\n",
      "Iteration 222, loss = 0.02490682\n",
      "Iteration 223, loss = 0.02483096\n",
      "Iteration 224, loss = 0.02473413\n",
      "Iteration 225, loss = 0.02461368\n",
      "Iteration 226, loss = 0.02442894\n",
      "Iteration 227, loss = 0.02441791\n",
      "Iteration 228, loss = 0.02422916\n",
      "Iteration 229, loss = 0.02407556\n",
      "Iteration 230, loss = 0.02409019\n",
      "Iteration 231, loss = 0.02395158\n",
      "Iteration 232, loss = 0.02384478\n",
      "Iteration 233, loss = 0.02375323\n",
      "Iteration 234, loss = 0.02364537\n",
      "Iteration 235, loss = 0.02353957\n",
      "Iteration 236, loss = 0.02347459\n",
      "Iteration 237, loss = 0.02337578\n",
      "Iteration 238, loss = 0.02321942\n",
      "Iteration 239, loss = 0.02325092\n",
      "Iteration 240, loss = 0.02311194\n",
      "Iteration 241, loss = 0.02300834\n",
      "Iteration 242, loss = 0.02295829\n",
      "Iteration 243, loss = 0.02281755\n",
      "Iteration 244, loss = 0.02269915\n",
      "Iteration 245, loss = 0.02263964\n",
      "Iteration 246, loss = 0.02256143\n",
      "Iteration 247, loss = 0.02244910\n",
      "Iteration 248, loss = 0.02246759\n",
      "Iteration 249, loss = 0.02231576\n",
      "Iteration 250, loss = 0.02226300\n",
      "Iteration 251, loss = 0.02214093\n",
      "Iteration 252, loss = 0.02201972\n",
      "Iteration 253, loss = 0.02205415\n",
      "Iteration 254, loss = 0.02185174\n",
      "Iteration 255, loss = 0.02179254\n",
      "Iteration 256, loss = 0.02169098\n",
      "Iteration 257, loss = 0.02166726\n",
      "Iteration 258, loss = 0.02159265\n",
      "Iteration 259, loss = 0.02146317\n",
      "Iteration 260, loss = 0.02138038\n",
      "Iteration 261, loss = 0.02131996\n",
      "Iteration 262, loss = 0.02133365\n",
      "Iteration 263, loss = 0.02115649\n",
      "Iteration 264, loss = 0.02107886\n",
      "Iteration 265, loss = 0.02101701\n",
      "Iteration 266, loss = 0.02094124\n",
      "Iteration 267, loss = 0.02084795\n",
      "Iteration 268, loss = 0.02083430\n",
      "Iteration 269, loss = 0.02071712\n",
      "Iteration 270, loss = 0.02063244\n",
      "Iteration 271, loss = 0.02055842\n",
      "Iteration 272, loss = 0.02048269\n",
      "Iteration 273, loss = 0.02046430\n",
      "Iteration 274, loss = 0.02029845\n",
      "Iteration 275, loss = 0.02026783\n",
      "Iteration 276, loss = 0.02016966\n",
      "Iteration 277, loss = 0.02010095\n",
      "Iteration 278, loss = 0.02003516\n",
      "Iteration 279, loss = 0.01995833\n",
      "Iteration 280, loss = 0.01988797\n",
      "Iteration 281, loss = 0.01998412\n",
      "Iteration 282, loss = 0.01978246\n",
      "Iteration 283, loss = 0.01971650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 284, loss = 0.01967408\n",
      "Iteration 285, loss = 0.01958497\n",
      "Iteration 286, loss = 0.01952143\n",
      "Iteration 287, loss = 0.01945457\n",
      "Iteration 288, loss = 0.01941707\n",
      "Iteration 289, loss = 0.01930108\n",
      "Iteration 290, loss = 0.01925842\n",
      "Iteration 291, loss = 0.01919008\n",
      "Iteration 292, loss = 0.01908323\n",
      "Iteration 293, loss = 0.01903908\n",
      "Iteration 294, loss = 0.01905175\n",
      "Iteration 295, loss = 0.01892245\n",
      "Iteration 296, loss = 0.01899205\n",
      "Iteration 297, loss = 0.01877849\n",
      "Iteration 298, loss = 0.01871038\n",
      "Iteration 299, loss = 0.01866989\n",
      "Iteration 300, loss = 0.01862610\n",
      "Iteration 301, loss = 0.01855739\n",
      "Iteration 302, loss = 0.01854246\n",
      "Iteration 303, loss = 0.01843610\n",
      "Iteration 304, loss = 0.01835509\n",
      "Iteration 305, loss = 0.01828621\n",
      "Iteration 306, loss = 0.01829643\n",
      "Iteration 307, loss = 0.01825528\n",
      "Iteration 308, loss = 0.01818114\n",
      "Iteration 309, loss = 0.01811771\n",
      "Iteration 310, loss = 0.01808221\n",
      "Iteration 311, loss = 0.01797858\n",
      "Iteration 312, loss = 0.01791828\n",
      "Iteration 313, loss = 0.01783029\n",
      "Iteration 314, loss = 0.01777718\n",
      "Iteration 315, loss = 0.01781694\n",
      "Iteration 316, loss = 0.01770963\n",
      "Iteration 317, loss = 0.01763279\n",
      "Iteration 318, loss = 0.01767373\n",
      "Iteration 319, loss = 0.01753418\n",
      "Iteration 320, loss = 0.01746630\n",
      "Iteration 321, loss = 0.01739562\n",
      "Iteration 322, loss = 0.01741366\n",
      "Iteration 323, loss = 0.01746751\n",
      "Iteration 324, loss = 0.01726674\n",
      "Iteration 325, loss = 0.01724696\n",
      "Iteration 326, loss = 0.01711036\n",
      "Iteration 327, loss = 0.01706525\n",
      "Iteration 328, loss = 0.01705394\n",
      "Iteration 329, loss = 0.01697927\n",
      "Iteration 330, loss = 0.01692811\n",
      "Iteration 331, loss = 0.01687144\n",
      "Iteration 332, loss = 0.01684476\n",
      "Iteration 333, loss = 0.01682259\n",
      "Iteration 334, loss = 0.01677163\n",
      "Iteration 335, loss = 0.01664194\n",
      "Iteration 336, loss = 0.01667115\n",
      "Iteration 337, loss = 0.01663044\n",
      "Iteration 338, loss = 0.01651950\n",
      "Iteration 339, loss = 0.01650092\n",
      "Iteration 340, loss = 0.01640628\n",
      "Iteration 341, loss = 0.01634813\n",
      "Iteration 342, loss = 0.01639504\n",
      "Iteration 343, loss = 0.01629804\n",
      "Iteration 344, loss = 0.01625289\n",
      "Iteration 345, loss = 0.01619099\n",
      "Iteration 346, loss = 0.01614364\n",
      "Iteration 347, loss = 0.01611295\n",
      "Iteration 348, loss = 0.01603796\n",
      "Iteration 349, loss = 0.01596607\n",
      "Iteration 350, loss = 0.01608695\n",
      "Iteration 351, loss = 0.01595496\n",
      "Iteration 352, loss = 0.01589883\n",
      "Iteration 353, loss = 0.01578764\n",
      "Iteration 354, loss = 0.01574509\n",
      "Iteration 355, loss = 0.01568337\n",
      "Iteration 356, loss = 0.01569505\n",
      "Iteration 357, loss = 0.01562025\n",
      "Iteration 358, loss = 0.01558914\n",
      "Iteration 359, loss = 0.01550721\n",
      "Iteration 360, loss = 0.01547468\n",
      "Iteration 361, loss = 0.01546289\n",
      "Iteration 362, loss = 0.01544836\n",
      "Iteration 363, loss = 0.01536378\n",
      "Iteration 364, loss = 0.01531568\n",
      "Iteration 365, loss = 0.01530812\n",
      "Iteration 366, loss = 0.01539617\n",
      "Iteration 367, loss = 0.01518854\n",
      "Iteration 368, loss = 0.01515328\n",
      "Iteration 369, loss = 0.01512593\n",
      "Iteration 370, loss = 0.01503292\n",
      "Iteration 371, loss = 0.01501527\n",
      "Iteration 372, loss = 0.01495957\n",
      "Iteration 373, loss = 0.01499837\n",
      "Iteration 374, loss = 0.01490396\n",
      "Iteration 375, loss = 0.01482730\n",
      "Iteration 376, loss = 0.01483251\n",
      "Iteration 377, loss = 0.01484436\n",
      "Iteration 378, loss = 0.01477156\n",
      "Iteration 379, loss = 0.01471022\n",
      "Iteration 380, loss = 0.01471437\n",
      "Iteration 381, loss = 0.01468451\n",
      "Iteration 382, loss = 0.01464982\n",
      "Iteration 383, loss = 0.01460256\n",
      "Iteration 384, loss = 0.01446674\n",
      "Iteration 385, loss = 0.01449123\n",
      "Iteration 386, loss = 0.01438878\n",
      "Iteration 387, loss = 0.01438000\n",
      "Iteration 388, loss = 0.01431590\n",
      "Iteration 389, loss = 0.01431877\n",
      "Iteration 390, loss = 0.01422618\n",
      "Iteration 391, loss = 0.01425870\n",
      "Iteration 392, loss = 0.01421602\n",
      "Iteration 393, loss = 0.01410229\n",
      "Iteration 394, loss = 0.01410633\n",
      "Iteration 395, loss = 0.01403874\n",
      "Iteration 396, loss = 0.01417705\n",
      "Iteration 397, loss = 0.01399803\n",
      "Iteration 398, loss = 0.01396768\n",
      "Iteration 399, loss = 0.01398498\n",
      "Iteration 400, loss = 0.01385727\n",
      "Iteration 401, loss = 0.01381644\n",
      "Iteration 402, loss = 0.01379903\n",
      "Iteration 403, loss = 0.01373149\n",
      "Iteration 404, loss = 0.01379956\n",
      "Iteration 405, loss = 0.01362475\n",
      "Iteration 406, loss = 0.01366303\n",
      "Iteration 407, loss = 0.01369425\n",
      "Iteration 408, loss = 0.01357030\n",
      "Iteration 409, loss = 0.01352989\n",
      "Iteration 410, loss = 0.01349793\n",
      "Iteration 411, loss = 0.01345211\n",
      "Iteration 412, loss = 0.01343056\n",
      "Iteration 413, loss = 0.01343842\n",
      "Iteration 414, loss = 0.01333363\n",
      "Iteration 415, loss = 0.01333048\n",
      "Iteration 416, loss = 0.01336390\n",
      "Iteration 417, loss = 0.01326720\n",
      "Iteration 418, loss = 0.01325962\n",
      "Iteration 419, loss = 0.01316387\n",
      "Iteration 420, loss = 0.01318535\n",
      "Iteration 421, loss = 0.01313854\n",
      "Iteration 422, loss = 0.01306553\n",
      "Iteration 423, loss = 0.01309766\n",
      "Iteration 424, loss = 0.01300200\n",
      "Iteration 425, loss = 0.01297832\n",
      "Iteration 426, loss = 0.01294842\n",
      "Iteration 427, loss = 0.01289955\n",
      "Iteration 428, loss = 0.01290993\n",
      "Iteration 429, loss = 0.01282896\n",
      "Iteration 430, loss = 0.01282040\n",
      "Iteration 431, loss = 0.01280650\n",
      "Iteration 432, loss = 0.01273664\n",
      "Iteration 433, loss = 0.01274451\n",
      "Iteration 434, loss = 0.01270552\n",
      "Iteration 435, loss = 0.01267271\n",
      "Iteration 436, loss = 0.01265206\n",
      "Iteration 437, loss = 0.01258889\n",
      "Iteration 438, loss = 0.01258405\n",
      "Iteration 439, loss = 0.01258819\n",
      "Iteration 440, loss = 0.01250428\n",
      "Iteration 441, loss = 0.01253192\n",
      "Iteration 442, loss = 0.01245658\n",
      "Iteration 443, loss = 0.01244310\n",
      "Iteration 444, loss = 0.01243422\n",
      "Iteration 445, loss = 0.01239730\n",
      "Iteration 446, loss = 0.01237547\n",
      "Iteration 447, loss = 0.01252297\n",
      "Iteration 448, loss = 0.01233169\n",
      "Iteration 449, loss = 0.01225691\n",
      "Iteration 450, loss = 0.01224275\n",
      "Iteration 451, loss = 0.01219144\n",
      "Iteration 452, loss = 0.01214833\n",
      "Iteration 453, loss = 0.01210838\n",
      "Iteration 454, loss = 0.01210796\n",
      "Iteration 455, loss = 0.01208452\n",
      "Iteration 456, loss = 0.01208068\n",
      "Iteration 457, loss = 0.01209289\n",
      "Iteration 458, loss = 0.01202576\n",
      "Iteration 459, loss = 0.01197040\n",
      "Iteration 460, loss = 0.01193177\n",
      "Iteration 461, loss = 0.01191886\n",
      "Iteration 462, loss = 0.01193222\n",
      "Iteration 463, loss = 0.01206987\n",
      "Iteration 464, loss = 0.01189097\n",
      "Iteration 465, loss = 0.01176599\n",
      "Iteration 466, loss = 0.01171911\n",
      "Iteration 467, loss = 0.01174790\n",
      "Iteration 468, loss = 0.01169054\n",
      "Iteration 469, loss = 0.01162070\n",
      "Iteration 470, loss = 0.01162572\n",
      "Iteration 471, loss = 0.01162933\n",
      "Iteration 472, loss = 0.01179898\n",
      "Iteration 473, loss = 0.01155894\n",
      "Iteration 474, loss = 0.01151516\n",
      "Iteration 475, loss = 0.01149594\n",
      "Iteration 476, loss = 0.01151823\n",
      "Iteration 477, loss = 0.01146180\n",
      "Iteration 478, loss = 0.01139552\n",
      "Iteration 479, loss = 0.01144210\n",
      "Iteration 480, loss = 0.01136480\n",
      "Iteration 481, loss = 0.01149472\n",
      "Iteration 482, loss = 0.01129241\n",
      "Iteration 483, loss = 0.01126464\n",
      "Iteration 484, loss = 0.01127261\n",
      "Iteration 485, loss = 0.01136199\n",
      "Iteration 486, loss = 0.01123679\n",
      "Iteration 487, loss = 0.01120980\n",
      "Iteration 488, loss = 0.01121333\n",
      "Iteration 489, loss = 0.01114766\n",
      "Iteration 490, loss = 0.01106748\n",
      "Iteration 491, loss = 0.01107986\n",
      "Iteration 492, loss = 0.01106847\n",
      "Iteration 493, loss = 0.01122261\n",
      "Iteration 494, loss = 0.01098778\n",
      "Iteration 495, loss = 0.01098053\n",
      "Iteration 496, loss = 0.01101021\n",
      "Iteration 497, loss = 0.01104913\n",
      "Iteration 498, loss = 0.01091483\n",
      "Iteration 499, loss = 0.01088991\n",
      "Iteration 500, loss = 0.01093028\n",
      "Iteration 501, loss = 0.01098086\n",
      "Iteration 502, loss = 0.01079869\n",
      "Iteration 503, loss = 0.01081428\n",
      "Iteration 504, loss = 0.01087398\n",
      "Iteration 505, loss = 0.01071735\n",
      "Iteration 506, loss = 0.01075227\n",
      "Iteration 507, loss = 0.01067308\n",
      "Iteration 508, loss = 0.01070994\n",
      "Iteration 509, loss = 0.01064544\n",
      "Iteration 510, loss = 0.01063202\n",
      "Iteration 511, loss = 0.01057296\n",
      "Iteration 512, loss = 0.01064764\n",
      "Iteration 513, loss = 0.01052910\n",
      "Iteration 514, loss = 0.01058203\n",
      "Iteration 515, loss = 0.01056861\n",
      "Iteration 516, loss = 0.01049728\n",
      "Iteration 517, loss = 0.01051020\n",
      "Iteration 518, loss = 0.01047589\n",
      "Iteration 519, loss = 0.01040694\n",
      "Iteration 520, loss = 0.01042997\n",
      "Iteration 521, loss = 0.01038299\n",
      "Iteration 522, loss = 0.01038222\n",
      "Iteration 523, loss = 0.01032345\n",
      "Iteration 524, loss = 0.01032878\n",
      "Iteration 525, loss = 0.01028145\n",
      "Iteration 526, loss = 0.01026756\n",
      "Iteration 527, loss = 0.01018702\n",
      "Iteration 528, loss = 0.01018476\n",
      "Iteration 529, loss = 0.01020980\n",
      "Iteration 530, loss = 0.01016839\n",
      "Iteration 531, loss = 0.01014927\n",
      "Iteration 532, loss = 0.01016058\n",
      "Iteration 533, loss = 0.01008747\n",
      "Iteration 534, loss = 0.01005847\n",
      "Iteration 535, loss = 0.01004858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 536, loss = 0.01004110\n",
      "Iteration 537, loss = 0.01003698\n",
      "Iteration 538, loss = 0.00997967\n",
      "Iteration 539, loss = 0.00997727\n",
      "Iteration 540, loss = 0.00994430\n",
      "Iteration 541, loss = 0.00995012\n",
      "Iteration 542, loss = 0.00990388\n",
      "Iteration 543, loss = 0.00998229\n",
      "Iteration 544, loss = 0.00987187\n",
      "Iteration 545, loss = 0.00988521\n",
      "Iteration 546, loss = 0.00983678\n",
      "Iteration 547, loss = 0.00978534\n",
      "Iteration 548, loss = 0.00981519\n",
      "Iteration 549, loss = 0.00975615\n",
      "Iteration 550, loss = 0.00972470\n",
      "Iteration 551, loss = 0.00974914\n",
      "Iteration 552, loss = 0.00972725\n",
      "Iteration 553, loss = 0.00978424\n",
      "Iteration 554, loss = 0.00972158\n",
      "Iteration 555, loss = 0.00966381\n",
      "Iteration 556, loss = 0.00960289\n",
      "Iteration 557, loss = 0.00959363\n",
      "Iteration 558, loss = 0.00963678\n",
      "Iteration 559, loss = 0.00955333\n",
      "Iteration 560, loss = 0.00951118\n",
      "Iteration 561, loss = 0.00952130\n",
      "Iteration 562, loss = 0.00950979\n",
      "Iteration 563, loss = 0.00950099\n",
      "Iteration 564, loss = 0.00947177\n",
      "Iteration 565, loss = 0.00943273\n",
      "Iteration 566, loss = 0.00941279\n",
      "Iteration 567, loss = 0.00940632\n",
      "Iteration 568, loss = 0.00941560\n",
      "Iteration 569, loss = 0.00937085\n",
      "Iteration 570, loss = 0.00936609\n",
      "Iteration 571, loss = 0.00929702\n",
      "Iteration 572, loss = 0.00936074\n",
      "Iteration 573, loss = 0.00932221\n",
      "Iteration 574, loss = 0.00925329\n",
      "Iteration 575, loss = 0.00939525\n",
      "Iteration 576, loss = 0.00933896\n",
      "Iteration 577, loss = 0.00926441\n",
      "Iteration 578, loss = 0.00933327\n",
      "Iteration 579, loss = 0.00916121\n",
      "Iteration 580, loss = 0.00918168\n",
      "Iteration 581, loss = 0.00917804\n",
      "Iteration 582, loss = 0.00909718\n",
      "Iteration 583, loss = 0.00910019\n",
      "Iteration 584, loss = 0.00910477\n",
      "Iteration 585, loss = 0.00903188\n",
      "Iteration 586, loss = 0.00907162\n",
      "Iteration 587, loss = 0.00902858\n",
      "Iteration 588, loss = 0.00899983\n",
      "Iteration 589, loss = 0.00898254\n",
      "Iteration 590, loss = 0.00906280\n",
      "Iteration 591, loss = 0.00893370\n",
      "Iteration 592, loss = 0.00896747\n",
      "Iteration 593, loss = 0.00897435\n",
      "Iteration 594, loss = 0.00893369\n",
      "Iteration 595, loss = 0.00891795\n",
      "Iteration 596, loss = 0.00887085\n",
      "Iteration 597, loss = 0.00883822\n",
      "Iteration 598, loss = 0.00885625\n",
      "Iteration 599, loss = 0.00885324\n",
      "Iteration 600, loss = 0.00889869\n",
      "Iteration 601, loss = 0.00889610\n",
      "Iteration 602, loss = 0.00878439\n",
      "Iteration 603, loss = 0.00880932\n",
      "Iteration 604, loss = 0.00890671\n",
      "Iteration 605, loss = 0.00872782\n",
      "Iteration 606, loss = 0.00866943\n",
      "Iteration 607, loss = 0.00868964\n",
      "Iteration 608, loss = 0.00863812\n",
      "Iteration 609, loss = 0.00863411\n",
      "Iteration 610, loss = 0.00868382\n",
      "Iteration 611, loss = 0.00861014\n",
      "Iteration 612, loss = 0.00861609\n",
      "Iteration 613, loss = 0.00857731\n",
      "Iteration 614, loss = 0.00857786\n",
      "Iteration 615, loss = 0.00854355\n",
      "Iteration 616, loss = 0.00861556\n",
      "Iteration 617, loss = 0.00849067\n",
      "Iteration 618, loss = 0.00865864\n",
      "Iteration 619, loss = 0.00849929\n",
      "Iteration 620, loss = 0.00851508\n",
      "Iteration 621, loss = 0.00857788\n",
      "Iteration 622, loss = 0.00844704\n",
      "Iteration 623, loss = 0.00843807\n",
      "Iteration 624, loss = 0.00858118\n",
      "Iteration 625, loss = 0.00834417\n",
      "Iteration 626, loss = 0.00840309\n",
      "Iteration 627, loss = 0.00835964\n",
      "Iteration 628, loss = 0.00832397\n",
      "Iteration 629, loss = 0.00832286\n",
      "Iteration 630, loss = 0.00835781\n",
      "Iteration 631, loss = 0.00832795\n",
      "Iteration 632, loss = 0.00826282\n",
      "Iteration 633, loss = 0.00832840\n",
      "Iteration 634, loss = 0.00823293\n",
      "Iteration 635, loss = 0.00833943\n",
      "Iteration 636, loss = 0.00822603\n",
      "Iteration 637, loss = 0.00821709\n",
      "Iteration 638, loss = 0.00822256\n",
      "Iteration 639, loss = 0.00816827\n",
      "Iteration 640, loss = 0.00815467\n",
      "Iteration 641, loss = 0.00824000\n",
      "Iteration 642, loss = 0.00817288\n",
      "Iteration 643, loss = 0.00814439\n",
      "Iteration 644, loss = 0.00808399\n",
      "Iteration 645, loss = 0.00805910\n",
      "Iteration 646, loss = 0.00809439\n",
      "Iteration 647, loss = 0.00818537\n",
      "Iteration 648, loss = 0.00805252\n",
      "Iteration 649, loss = 0.00802543\n",
      "Iteration 650, loss = 0.00810424\n",
      "Iteration 651, loss = 0.00809572\n",
      "Iteration 652, loss = 0.00797782\n",
      "Iteration 653, loss = 0.00798699\n",
      "Iteration 654, loss = 0.00791638\n",
      "Iteration 655, loss = 0.00799429\n",
      "Iteration 656, loss = 0.00797377\n",
      "Iteration 657, loss = 0.00791459\n",
      "Iteration 658, loss = 0.00791917\n",
      "Iteration 659, loss = 0.00787685\n",
      "Iteration 660, loss = 0.00790114\n",
      "Iteration 661, loss = 0.00783779\n",
      "Iteration 662, loss = 0.00787903\n",
      "Iteration 663, loss = 0.00786508\n",
      "Iteration 664, loss = 0.00777869\n",
      "Iteration 665, loss = 0.00776880\n",
      "Iteration 666, loss = 0.00778651\n",
      "Iteration 667, loss = 0.00775828\n",
      "Iteration 668, loss = 0.00771603\n",
      "Iteration 669, loss = 0.00773140\n",
      "Iteration 670, loss = 0.00776152\n",
      "Iteration 671, loss = 0.00774740\n",
      "Iteration 672, loss = 0.00777867\n",
      "Iteration 673, loss = 0.00766552\n",
      "Iteration 674, loss = 0.00769264\n",
      "Iteration 675, loss = 0.00765008\n",
      "Iteration 676, loss = 0.00768691\n",
      "Iteration 677, loss = 0.00758869\n",
      "Iteration 678, loss = 0.00769083\n",
      "Iteration 679, loss = 0.00761750\n",
      "Iteration 680, loss = 0.00759002\n",
      "Iteration 681, loss = 0.00758584\n",
      "Iteration 682, loss = 0.00754958\n",
      "Iteration 683, loss = 0.00765445\n",
      "Iteration 684, loss = 0.00752180\n",
      "Iteration 685, loss = 0.00754011\n",
      "Iteration 686, loss = 0.00750154\n",
      "Iteration 687, loss = 0.00748423\n",
      "Iteration 688, loss = 0.00763054\n",
      "Iteration 689, loss = 0.00748629\n",
      "Iteration 690, loss = 0.00749883\n",
      "Iteration 691, loss = 0.00755421\n",
      "Iteration 692, loss = 0.00749509\n",
      "Iteration 693, loss = 0.00742085\n",
      "Iteration 694, loss = 0.00744535\n",
      "Iteration 695, loss = 0.00747191\n",
      "Iteration 696, loss = 0.00732823\n",
      "Iteration 697, loss = 0.00743826\n",
      "Iteration 698, loss = 0.00732976\n",
      "Iteration 699, loss = 0.00738774\n",
      "Iteration 700, loss = 0.00738248\n",
      "Iteration 701, loss = 0.00730378\n",
      "Iteration 702, loss = 0.00731837\n",
      "Iteration 703, loss = 0.00740674\n",
      "Iteration 704, loss = 0.00731520\n",
      "Iteration 705, loss = 0.00723374\n",
      "Iteration 706, loss = 0.00729683\n",
      "Iteration 707, loss = 0.00723439\n",
      "Iteration 708, loss = 0.00731497\n",
      "Iteration 709, loss = 0.00723807\n",
      "Iteration 710, loss = 0.00724146\n",
      "Iteration 711, loss = 0.00720677\n",
      "Iteration 712, loss = 0.00715771\n",
      "Iteration 713, loss = 0.00720341\n",
      "Iteration 714, loss = 0.00714284\n",
      "Iteration 715, loss = 0.00712966\n",
      "Iteration 716, loss = 0.00715779\n",
      "Iteration 717, loss = 0.00713488\n",
      "Iteration 718, loss = 0.00734643\n",
      "Iteration 719, loss = 0.00718319\n",
      "Iteration 720, loss = 0.00712373\n",
      "Iteration 721, loss = 0.00711447\n",
      "Iteration 722, loss = 0.00704186\n",
      "Iteration 723, loss = 0.00704230\n",
      "Iteration 724, loss = 0.00702593\n",
      "Iteration 725, loss = 0.00700369\n",
      "Iteration 726, loss = 0.00702239\n",
      "Iteration 727, loss = 0.00701565\n",
      "Iteration 728, loss = 0.00699187\n",
      "Iteration 729, loss = 0.00704020\n",
      "Iteration 730, loss = 0.00702273\n",
      "Iteration 731, loss = 0.00693187\n",
      "Iteration 732, loss = 0.00696025\n",
      "Iteration 733, loss = 0.00706029\n",
      "Iteration 734, loss = 0.00696641\n",
      "Iteration 735, loss = 0.00697329\n",
      "Iteration 736, loss = 0.00691546\n",
      "Iteration 737, loss = 0.00694684\n",
      "Iteration 738, loss = 0.00689087\n",
      "Iteration 739, loss = 0.00682046\n",
      "Iteration 740, loss = 0.00683119\n",
      "Iteration 741, loss = 0.00685553\n",
      "Iteration 742, loss = 0.00684316\n",
      "Iteration 743, loss = 0.00680590\n",
      "Iteration 744, loss = 0.00685003\n",
      "Iteration 745, loss = 0.00681797\n",
      "Iteration 746, loss = 0.00682445\n",
      "Iteration 747, loss = 0.00678189\n",
      "Iteration 748, loss = 0.00684865\n",
      "Iteration 749, loss = 0.00680441\n",
      "Iteration 750, loss = 0.00678857\n",
      "Iteration 751, loss = 0.00671473\n",
      "Iteration 752, loss = 0.00672866\n",
      "Iteration 753, loss = 0.00665899\n",
      "Iteration 754, loss = 0.00672981\n",
      "Iteration 755, loss = 0.00667484\n",
      "Iteration 756, loss = 0.00665243\n",
      "Iteration 757, loss = 0.00664794\n",
      "Iteration 758, loss = 0.00666853\n",
      "Iteration 759, loss = 0.00673714\n",
      "Iteration 760, loss = 0.00661190\n",
      "Iteration 761, loss = 0.00662553\n",
      "Iteration 762, loss = 0.00667893\n",
      "Iteration 763, loss = 0.00656555\n",
      "Iteration 764, loss = 0.00654202\n",
      "Iteration 765, loss = 0.00660009\n",
      "Iteration 766, loss = 0.00660305\n",
      "Iteration 767, loss = 0.00663137\n",
      "Iteration 768, loss = 0.00656445\n",
      "Iteration 769, loss = 0.00663600\n",
      "Iteration 770, loss = 0.00648737\n",
      "Iteration 771, loss = 0.00665871\n",
      "Iteration 772, loss = 0.00646970\n",
      "Iteration 773, loss = 0.00646718\n",
      "Iteration 774, loss = 0.00652877\n",
      "Iteration 775, loss = 0.00655163\n",
      "Iteration 776, loss = 0.00648980\n",
      "Iteration 777, loss = 0.00650705\n",
      "Iteration 778, loss = 0.00643879\n",
      "Iteration 779, loss = 0.00640938\n",
      "Iteration 780, loss = 0.00646047\n",
      "Iteration 781, loss = 0.00655514\n",
      "Iteration 782, loss = 0.00644484\n",
      "Iteration 783, loss = 0.00639109\n",
      "Iteration 784, loss = 0.00656507\n",
      "Iteration 785, loss = 0.00634552\n",
      "Iteration 786, loss = 0.00635477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 787, loss = 0.00636773\n",
      "Iteration 788, loss = 0.00635630\n",
      "Iteration 789, loss = 0.00638774\n",
      "Iteration 790, loss = 0.00628754\n",
      "Iteration 791, loss = 0.00631941\n",
      "Iteration 792, loss = 0.00635817\n",
      "Iteration 793, loss = 0.00628540\n",
      "Iteration 794, loss = 0.00627329\n",
      "Iteration 795, loss = 0.00644314\n",
      "Iteration 796, loss = 0.00639662\n",
      "Iteration 797, loss = 0.00627547\n",
      "Iteration 798, loss = 0.00624664\n",
      "Iteration 799, loss = 0.00623271\n",
      "Iteration 800, loss = 0.00635382\n",
      "Iteration 801, loss = 0.00618899\n",
      "Iteration 802, loss = 0.00621916\n",
      "Iteration 803, loss = 0.00620958\n",
      "Iteration 804, loss = 0.00617836\n",
      "Iteration 805, loss = 0.00620261\n",
      "Iteration 806, loss = 0.00612445\n",
      "Iteration 807, loss = 0.00617443\n",
      "Iteration 808, loss = 0.00616187\n",
      "Iteration 809, loss = 0.00611997\n",
      "Iteration 810, loss = 0.00610187\n",
      "Iteration 811, loss = 0.00619678\n",
      "Iteration 812, loss = 0.00611465\n",
      "Iteration 813, loss = 0.00607488\n",
      "Iteration 814, loss = 0.00612372\n",
      "Iteration 815, loss = 0.00613263\n",
      "Iteration 816, loss = 0.00604747\n",
      "Iteration 817, loss = 0.00605489\n",
      "Iteration 818, loss = 0.00602484\n",
      "Iteration 819, loss = 0.00604601\n",
      "Iteration 820, loss = 0.00606738\n",
      "Iteration 821, loss = 0.00611369\n",
      "Iteration 822, loss = 0.00604064\n",
      "Iteration 823, loss = 0.00602273\n",
      "Iteration 824, loss = 0.00601433\n",
      "Iteration 825, loss = 0.00597263\n",
      "Iteration 826, loss = 0.00598348\n",
      "Iteration 827, loss = 0.00596395\n",
      "Iteration 828, loss = 0.00598303\n",
      "Iteration 829, loss = 0.00594552\n",
      "Iteration 830, loss = 0.00593065\n",
      "Iteration 831, loss = 0.00590091\n",
      "Iteration 832, loss = 0.00591698\n",
      "Iteration 833, loss = 0.00595812\n",
      "Iteration 834, loss = 0.00586370\n",
      "Iteration 835, loss = 0.00588022\n",
      "Iteration 836, loss = 0.00587181\n",
      "Iteration 837, loss = 0.00589295\n",
      "Iteration 838, loss = 0.00586127\n",
      "Iteration 839, loss = 0.00589868\n",
      "Iteration 840, loss = 0.00586532\n",
      "Iteration 841, loss = 0.00585132\n",
      "Iteration 842, loss = 0.00583196\n",
      "Iteration 843, loss = 0.00579524\n",
      "Iteration 844, loss = 0.00585754\n",
      "Iteration 845, loss = 0.00585822\n",
      "Iteration 846, loss = 0.00586611\n",
      "Iteration 847, loss = 0.00582126\n",
      "Iteration 848, loss = 0.00588056\n",
      "Iteration 849, loss = 0.00594925\n",
      "Iteration 850, loss = 0.00585111\n",
      "Iteration 851, loss = 0.00575815\n",
      "Iteration 852, loss = 0.00575420\n",
      "Iteration 853, loss = 0.00578980\n",
      "Iteration 854, loss = 0.00570390\n",
      "Iteration 855, loss = 0.00569968\n",
      "Iteration 856, loss = 0.00588167\n",
      "Iteration 857, loss = 0.00576094\n",
      "Iteration 858, loss = 0.00571829\n",
      "Iteration 859, loss = 0.00567637\n",
      "Iteration 860, loss = 0.00567727\n",
      "Iteration 861, loss = 0.00567256\n",
      "Iteration 862, loss = 0.00567592\n",
      "Iteration 863, loss = 0.00565374\n",
      "Iteration 864, loss = 0.00568710\n",
      "Iteration 865, loss = 0.00567441\n",
      "Iteration 866, loss = 0.00561552\n",
      "Iteration 867, loss = 0.00557949\n",
      "Iteration 868, loss = 0.00562687\n",
      "Iteration 869, loss = 0.00558910\n",
      "Iteration 870, loss = 0.00560715\n",
      "Iteration 871, loss = 0.00567624\n",
      "Iteration 872, loss = 0.00563140\n",
      "Iteration 873, loss = 0.00556894\n",
      "Iteration 874, loss = 0.00567705\n",
      "Iteration 875, loss = 0.00557743\n",
      "Iteration 876, loss = 0.00557409\n",
      "Iteration 877, loss = 0.00552049\n",
      "Iteration 878, loss = 0.00553074\n",
      "Iteration 879, loss = 0.00559723\n",
      "Iteration 880, loss = 0.00553964\n",
      "Iteration 881, loss = 0.00555625\n",
      "Iteration 882, loss = 0.00549517\n",
      "Iteration 883, loss = 0.00552308\n",
      "Iteration 884, loss = 0.00549353\n",
      "Iteration 885, loss = 0.00545772\n",
      "Iteration 886, loss = 0.00547897\n",
      "Iteration 887, loss = 0.00549390\n",
      "Iteration 888, loss = 0.00548490\n",
      "Iteration 889, loss = 0.00545808\n",
      "Iteration 890, loss = 0.00552929\n",
      "Iteration 891, loss = 0.00543145\n",
      "Iteration 892, loss = 0.00569097\n",
      "Iteration 893, loss = 0.00537301\n",
      "Iteration 894, loss = 0.00539074\n",
      "Iteration 895, loss = 0.00545362\n",
      "Iteration 896, loss = 0.00540792\n",
      "Iteration 897, loss = 0.00537792\n",
      "Iteration 898, loss = 0.00539532\n",
      "Iteration 899, loss = 0.00533313\n",
      "Iteration 900, loss = 0.00543338\n",
      "Iteration 901, loss = 0.00534516\n",
      "Iteration 902, loss = 0.00541132\n",
      "Iteration 903, loss = 0.00533644\n",
      "Iteration 904, loss = 0.00536462\n",
      "Iteration 905, loss = 0.00529442\n",
      "Iteration 906, loss = 0.00529107\n",
      "Iteration 907, loss = 0.00532129\n",
      "Iteration 908, loss = 0.00535303\n",
      "Iteration 909, loss = 0.00537310\n",
      "Iteration 910, loss = 0.00525128\n",
      "Iteration 911, loss = 0.00544512\n",
      "Iteration 912, loss = 0.00521511\n",
      "Iteration 913, loss = 0.00533312\n",
      "Iteration 914, loss = 0.00525029\n",
      "Iteration 915, loss = 0.00524857\n",
      "Iteration 916, loss = 0.00529761\n",
      "Iteration 917, loss = 0.00531331\n",
      "Iteration 918, loss = 0.00534319\n",
      "Iteration 919, loss = 0.00513965\n",
      "Iteration 920, loss = 0.00538777\n",
      "Iteration 921, loss = 0.00519415\n",
      "Iteration 922, loss = 0.00528013\n",
      "Iteration 923, loss = 0.00531928\n",
      "Iteration 924, loss = 0.00522181\n",
      "Iteration 925, loss = 0.00529097\n",
      "Iteration 926, loss = 0.00522662\n",
      "Iteration 927, loss = 0.00519606\n",
      "Iteration 928, loss = 0.00518429\n",
      "Iteration 929, loss = 0.00520414\n",
      "Iteration 930, loss = 0.00521885\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72831154\n",
      "Iteration 2, loss = 0.64444344\n",
      "Iteration 3, loss = 0.57345514\n",
      "Iteration 4, loss = 0.51421682\n",
      "Iteration 5, loss = 0.46378027\n",
      "Iteration 6, loss = 0.42120489\n",
      "Iteration 7, loss = 0.38506304\n",
      "Iteration 8, loss = 0.35415721\n",
      "Iteration 9, loss = 0.32711445\n",
      "Iteration 10, loss = 0.30347440\n",
      "Iteration 11, loss = 0.28268029\n",
      "Iteration 12, loss = 0.26437406\n",
      "Iteration 13, loss = 0.24811183\n",
      "Iteration 14, loss = 0.23359108\n",
      "Iteration 15, loss = 0.22089931\n",
      "Iteration 16, loss = 0.20929214\n",
      "Iteration 17, loss = 0.19882280\n",
      "Iteration 18, loss = 0.18947081\n",
      "Iteration 19, loss = 0.18095758\n",
      "Iteration 20, loss = 0.17318762\n",
      "Iteration 21, loss = 0.16616557\n",
      "Iteration 22, loss = 0.15979695\n",
      "Iteration 23, loss = 0.15386927\n",
      "Iteration 24, loss = 0.14850690\n",
      "Iteration 25, loss = 0.14366548\n",
      "Iteration 26, loss = 0.13903444\n",
      "Iteration 27, loss = 0.13487820\n",
      "Iteration 28, loss = 0.13104069\n",
      "Iteration 29, loss = 0.12736754\n",
      "Iteration 30, loss = 0.12398886\n",
      "Iteration 31, loss = 0.12089084\n",
      "Iteration 32, loss = 0.11790821\n",
      "Iteration 33, loss = 0.11522941\n",
      "Iteration 34, loss = 0.11252907\n",
      "Iteration 35, loss = 0.11012307\n",
      "Iteration 36, loss = 0.10778082\n",
      "Iteration 37, loss = 0.10555985\n",
      "Iteration 38, loss = 0.10349849\n",
      "Iteration 39, loss = 0.10146980\n",
      "Iteration 40, loss = 0.09964829\n",
      "Iteration 41, loss = 0.09782760\n",
      "Iteration 42, loss = 0.09605101\n",
      "Iteration 43, loss = 0.09441272\n",
      "Iteration 44, loss = 0.09280297\n",
      "Iteration 45, loss = 0.09129582\n",
      "Iteration 46, loss = 0.08994117\n",
      "Iteration 47, loss = 0.08841911\n",
      "Iteration 48, loss = 0.08716582\n",
      "Iteration 49, loss = 0.08576675\n",
      "Iteration 50, loss = 0.08450667\n",
      "Iteration 51, loss = 0.08332136\n",
      "Iteration 52, loss = 0.08213731\n",
      "Iteration 53, loss = 0.08098979\n",
      "Iteration 54, loss = 0.07990588\n",
      "Iteration 55, loss = 0.07883365\n",
      "Iteration 56, loss = 0.07781610\n",
      "Iteration 57, loss = 0.07680669\n",
      "Iteration 58, loss = 0.07585849\n",
      "Iteration 59, loss = 0.07487386\n",
      "Iteration 60, loss = 0.07391069\n",
      "Iteration 61, loss = 0.07308317\n",
      "Iteration 62, loss = 0.07222210\n",
      "Iteration 63, loss = 0.07126621\n",
      "Iteration 64, loss = 0.07045889\n",
      "Iteration 65, loss = 0.06963335\n",
      "Iteration 66, loss = 0.06878014\n",
      "Iteration 67, loss = 0.06803042\n",
      "Iteration 68, loss = 0.06720790\n",
      "Iteration 69, loss = 0.06652758\n",
      "Iteration 70, loss = 0.06570066\n",
      "Iteration 71, loss = 0.06502051\n",
      "Iteration 72, loss = 0.06426546\n",
      "Iteration 73, loss = 0.06354038\n",
      "Iteration 74, loss = 0.06283470\n",
      "Iteration 75, loss = 0.06214939\n",
      "Iteration 76, loss = 0.06149479\n",
      "Iteration 77, loss = 0.06086549\n",
      "Iteration 78, loss = 0.06019790\n",
      "Iteration 79, loss = 0.05960324\n",
      "Iteration 80, loss = 0.05895950\n",
      "Iteration 81, loss = 0.05831542\n",
      "Iteration 82, loss = 0.05777387\n",
      "Iteration 83, loss = 0.05724760\n",
      "Iteration 84, loss = 0.05670310\n",
      "Iteration 85, loss = 0.05605241\n",
      "Iteration 86, loss = 0.05551904\n",
      "Iteration 87, loss = 0.05502019\n",
      "Iteration 88, loss = 0.05454321\n",
      "Iteration 89, loss = 0.05396074\n",
      "Iteration 90, loss = 0.05354407\n",
      "Iteration 91, loss = 0.05294294\n",
      "Iteration 92, loss = 0.05248708\n",
      "Iteration 93, loss = 0.05206534\n",
      "Iteration 94, loss = 0.05161804\n",
      "Iteration 95, loss = 0.05124698\n",
      "Iteration 96, loss = 0.05073238\n",
      "Iteration 97, loss = 0.05026155\n",
      "Iteration 98, loss = 0.04984396\n",
      "Iteration 99, loss = 0.04941316\n",
      "Iteration 100, loss = 0.04899443\n",
      "Iteration 101, loss = 0.04867272\n",
      "Iteration 102, loss = 0.04816131\n",
      "Iteration 103, loss = 0.04778519\n",
      "Iteration 104, loss = 0.04745037\n",
      "Iteration 105, loss = 0.04695929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 106, loss = 0.04660143\n",
      "Iteration 107, loss = 0.04625843\n",
      "Iteration 108, loss = 0.04586352\n",
      "Iteration 109, loss = 0.04547772\n",
      "Iteration 110, loss = 0.04517659\n",
      "Iteration 111, loss = 0.04486689\n",
      "Iteration 112, loss = 0.04441750\n",
      "Iteration 113, loss = 0.04419264\n",
      "Iteration 114, loss = 0.04373665\n",
      "Iteration 115, loss = 0.04338709\n",
      "Iteration 116, loss = 0.04309542\n",
      "Iteration 117, loss = 0.04275884\n",
      "Iteration 118, loss = 0.04252101\n",
      "Iteration 119, loss = 0.04214024\n",
      "Iteration 120, loss = 0.04183394\n",
      "Iteration 121, loss = 0.04155322\n",
      "Iteration 122, loss = 0.04128879\n",
      "Iteration 123, loss = 0.04104576\n",
      "Iteration 124, loss = 0.04076015\n",
      "Iteration 125, loss = 0.04043408\n",
      "Iteration 126, loss = 0.04015125\n",
      "Iteration 127, loss = 0.03980048\n",
      "Iteration 128, loss = 0.03964448\n",
      "Iteration 129, loss = 0.03924624\n",
      "Iteration 130, loss = 0.03905851\n",
      "Iteration 131, loss = 0.03886407\n",
      "Iteration 132, loss = 0.03850478\n",
      "Iteration 133, loss = 0.03824606\n",
      "Iteration 134, loss = 0.03800354\n",
      "Iteration 135, loss = 0.03791349\n",
      "Iteration 136, loss = 0.03755118\n",
      "Iteration 137, loss = 0.03725048\n",
      "Iteration 138, loss = 0.03710149\n",
      "Iteration 139, loss = 0.03685570\n",
      "Iteration 140, loss = 0.03656293\n",
      "Iteration 141, loss = 0.03637583\n",
      "Iteration 142, loss = 0.03610244\n",
      "Iteration 143, loss = 0.03586862\n",
      "Iteration 144, loss = 0.03563247\n",
      "Iteration 145, loss = 0.03544808\n",
      "Iteration 146, loss = 0.03518850\n",
      "Iteration 147, loss = 0.03505105\n",
      "Iteration 148, loss = 0.03476663\n",
      "Iteration 149, loss = 0.03463679\n",
      "Iteration 150, loss = 0.03438231\n",
      "Iteration 151, loss = 0.03421407\n",
      "Iteration 152, loss = 0.03402399\n",
      "Iteration 153, loss = 0.03374336\n",
      "Iteration 154, loss = 0.03353812\n",
      "Iteration 155, loss = 0.03338233\n",
      "Iteration 156, loss = 0.03317908\n",
      "Iteration 157, loss = 0.03301765\n",
      "Iteration 158, loss = 0.03279018\n",
      "Iteration 159, loss = 0.03261002\n",
      "Iteration 160, loss = 0.03250864\n",
      "Iteration 161, loss = 0.03221941\n",
      "Iteration 162, loss = 0.03210672\n",
      "Iteration 163, loss = 0.03188201\n",
      "Iteration 164, loss = 0.03180220\n",
      "Iteration 165, loss = 0.03149233\n",
      "Iteration 166, loss = 0.03135180\n",
      "Iteration 167, loss = 0.03119123\n",
      "Iteration 168, loss = 0.03104582\n",
      "Iteration 169, loss = 0.03085082\n",
      "Iteration 170, loss = 0.03072800\n",
      "Iteration 171, loss = 0.03047457\n",
      "Iteration 172, loss = 0.03055021\n",
      "Iteration 173, loss = 0.03031304\n",
      "Iteration 174, loss = 0.03006809\n",
      "Iteration 175, loss = 0.02984676\n",
      "Iteration 176, loss = 0.02977355\n",
      "Iteration 177, loss = 0.02957142\n",
      "Iteration 178, loss = 0.02943037\n",
      "Iteration 179, loss = 0.02928336\n",
      "Iteration 180, loss = 0.02911154\n",
      "Iteration 181, loss = 0.02898025\n",
      "Iteration 182, loss = 0.02887137\n",
      "Iteration 183, loss = 0.02870075\n",
      "Iteration 184, loss = 0.02850274\n",
      "Iteration 185, loss = 0.02849467\n",
      "Iteration 186, loss = 0.02826199\n",
      "Iteration 187, loss = 0.02812242\n",
      "Iteration 188, loss = 0.02795154\n",
      "Iteration 189, loss = 0.02784645\n",
      "Iteration 190, loss = 0.02775486\n",
      "Iteration 191, loss = 0.02754013\n",
      "Iteration 192, loss = 0.02738993\n",
      "Iteration 193, loss = 0.02738279\n",
      "Iteration 194, loss = 0.02718919\n",
      "Iteration 195, loss = 0.02703105\n",
      "Iteration 196, loss = 0.02693036\n",
      "Iteration 197, loss = 0.02678616\n",
      "Iteration 198, loss = 0.02669297\n",
      "Iteration 199, loss = 0.02651047\n",
      "Iteration 200, loss = 0.02643342\n",
      "Iteration 201, loss = 0.02626094\n",
      "Iteration 202, loss = 0.02619007\n",
      "Iteration 203, loss = 0.02606742\n",
      "Iteration 204, loss = 0.02592186\n",
      "Iteration 205, loss = 0.02586946\n",
      "Iteration 206, loss = 0.02567540\n",
      "Iteration 207, loss = 0.02554222\n",
      "Iteration 208, loss = 0.02543264\n",
      "Iteration 209, loss = 0.02535479\n",
      "Iteration 210, loss = 0.02527134\n",
      "Iteration 211, loss = 0.02510812\n",
      "Iteration 212, loss = 0.02497941\n",
      "Iteration 213, loss = 0.02493354\n",
      "Iteration 214, loss = 0.02477944\n",
      "Iteration 215, loss = 0.02469656\n",
      "Iteration 216, loss = 0.02459419\n",
      "Iteration 217, loss = 0.02445761\n",
      "Iteration 218, loss = 0.02442431\n",
      "Iteration 219, loss = 0.02428720\n",
      "Iteration 220, loss = 0.02420025\n",
      "Iteration 221, loss = 0.02409690\n",
      "Iteration 222, loss = 0.02398596\n",
      "Iteration 223, loss = 0.02392633\n",
      "Iteration 224, loss = 0.02376125\n",
      "Iteration 225, loss = 0.02367430\n",
      "Iteration 226, loss = 0.02358353\n",
      "Iteration 227, loss = 0.02344639\n",
      "Iteration 228, loss = 0.02349810\n",
      "Iteration 229, loss = 0.02335001\n",
      "Iteration 230, loss = 0.02324341\n",
      "Iteration 231, loss = 0.02319594\n",
      "Iteration 232, loss = 0.02298421\n",
      "Iteration 233, loss = 0.02299438\n",
      "Iteration 234, loss = 0.02289172\n",
      "Iteration 235, loss = 0.02271863\n",
      "Iteration 236, loss = 0.02272552\n",
      "Iteration 237, loss = 0.02260023\n",
      "Iteration 238, loss = 0.02250575\n",
      "Iteration 239, loss = 0.02247597\n",
      "Iteration 240, loss = 0.02238426\n",
      "Iteration 241, loss = 0.02219137\n",
      "Iteration 242, loss = 0.02215563\n",
      "Iteration 243, loss = 0.02200427\n",
      "Iteration 244, loss = 0.02202996\n",
      "Iteration 245, loss = 0.02186451\n",
      "Iteration 246, loss = 0.02182533\n",
      "Iteration 247, loss = 0.02177667\n",
      "Iteration 248, loss = 0.02164036\n",
      "Iteration 249, loss = 0.02157179\n",
      "Iteration 250, loss = 0.02142330\n",
      "Iteration 251, loss = 0.02141419\n",
      "Iteration 252, loss = 0.02126038\n",
      "Iteration 253, loss = 0.02122974\n",
      "Iteration 254, loss = 0.02119080\n",
      "Iteration 255, loss = 0.02102052\n",
      "Iteration 256, loss = 0.02098701\n",
      "Iteration 257, loss = 0.02086580\n",
      "Iteration 258, loss = 0.02097191\n",
      "Iteration 259, loss = 0.02073318\n",
      "Iteration 260, loss = 0.02065428\n",
      "Iteration 261, loss = 0.02064568\n",
      "Iteration 262, loss = 0.02051735\n",
      "Iteration 263, loss = 0.02052167\n",
      "Iteration 264, loss = 0.02039591\n",
      "Iteration 265, loss = 0.02028242\n",
      "Iteration 266, loss = 0.02027245\n",
      "Iteration 267, loss = 0.02021507\n",
      "Iteration 268, loss = 0.02016942\n",
      "Iteration 269, loss = 0.01998903\n",
      "Iteration 270, loss = 0.01998554\n",
      "Iteration 271, loss = 0.01998651\n",
      "Iteration 272, loss = 0.01980589\n",
      "Iteration 273, loss = 0.01978229\n",
      "Iteration 274, loss = 0.01967335\n",
      "Iteration 275, loss = 0.01960880\n",
      "Iteration 276, loss = 0.01955213\n",
      "Iteration 277, loss = 0.01945975\n",
      "Iteration 278, loss = 0.01939479\n",
      "Iteration 279, loss = 0.01944322\n",
      "Iteration 280, loss = 0.01926162\n",
      "Iteration 281, loss = 0.01929024\n",
      "Iteration 282, loss = 0.01911282\n",
      "Iteration 283, loss = 0.01902703\n",
      "Iteration 284, loss = 0.01896816\n",
      "Iteration 285, loss = 0.01894162\n",
      "Iteration 286, loss = 0.01888591\n",
      "Iteration 287, loss = 0.01885676\n",
      "Iteration 288, loss = 0.01881275\n",
      "Iteration 289, loss = 0.01868657\n",
      "Iteration 290, loss = 0.01864981\n",
      "Iteration 291, loss = 0.01860223\n",
      "Iteration 292, loss = 0.01854809\n",
      "Iteration 293, loss = 0.01841115\n",
      "Iteration 294, loss = 0.01837439\n",
      "Iteration 295, loss = 0.01832171\n",
      "Iteration 296, loss = 0.01823549\n",
      "Iteration 297, loss = 0.01814416\n",
      "Iteration 298, loss = 0.01818579\n",
      "Iteration 299, loss = 0.01805157\n",
      "Iteration 300, loss = 0.01803354\n",
      "Iteration 301, loss = 0.01796490\n",
      "Iteration 302, loss = 0.01800843\n",
      "Iteration 303, loss = 0.01780909\n",
      "Iteration 304, loss = 0.01789412\n",
      "Iteration 305, loss = 0.01770291\n",
      "Iteration 306, loss = 0.01764773\n",
      "Iteration 307, loss = 0.01767229\n",
      "Iteration 308, loss = 0.01780850\n",
      "Iteration 309, loss = 0.01750843\n",
      "Iteration 310, loss = 0.01745384\n",
      "Iteration 311, loss = 0.01749207\n",
      "Iteration 312, loss = 0.01736880\n",
      "Iteration 313, loss = 0.01735892\n",
      "Iteration 314, loss = 0.01720459\n",
      "Iteration 315, loss = 0.01715660\n",
      "Iteration 316, loss = 0.01714542\n",
      "Iteration 317, loss = 0.01707737\n",
      "Iteration 318, loss = 0.01704250\n",
      "Iteration 319, loss = 0.01699025\n",
      "Iteration 320, loss = 0.01698539\n",
      "Iteration 321, loss = 0.01681555\n",
      "Iteration 322, loss = 0.01678528\n",
      "Iteration 323, loss = 0.01678664\n",
      "Iteration 324, loss = 0.01671939\n",
      "Iteration 325, loss = 0.01665162\n",
      "Iteration 326, loss = 0.01661619\n",
      "Iteration 327, loss = 0.01656476\n",
      "Iteration 328, loss = 0.01653440\n",
      "Iteration 329, loss = 0.01647788\n",
      "Iteration 330, loss = 0.01646916\n",
      "Iteration 331, loss = 0.01645583\n",
      "Iteration 332, loss = 0.01632099\n",
      "Iteration 333, loss = 0.01622187\n",
      "Iteration 334, loss = 0.01627444\n",
      "Iteration 335, loss = 0.01615264\n",
      "Iteration 336, loss = 0.01609300\n",
      "Iteration 337, loss = 0.01605370\n",
      "Iteration 338, loss = 0.01605247\n",
      "Iteration 339, loss = 0.01598764\n",
      "Iteration 340, loss = 0.01594729\n",
      "Iteration 341, loss = 0.01589775\n",
      "Iteration 342, loss = 0.01590237\n",
      "Iteration 343, loss = 0.01578987\n",
      "Iteration 344, loss = 0.01576915\n",
      "Iteration 345, loss = 0.01591729\n",
      "Iteration 346, loss = 0.01562976\n",
      "Iteration 347, loss = 0.01557578\n",
      "Iteration 348, loss = 0.01560246\n",
      "Iteration 349, loss = 0.01556935\n",
      "Iteration 350, loss = 0.01571803\n",
      "Iteration 351, loss = 0.01549294\n",
      "Iteration 352, loss = 0.01537263\n",
      "Iteration 353, loss = 0.01531709\n",
      "Iteration 354, loss = 0.01529765\n",
      "Iteration 355, loss = 0.01523532\n",
      "Iteration 356, loss = 0.01522537\n",
      "Iteration 357, loss = 0.01519097\n",
      "Iteration 358, loss = 0.01517570\n",
      "Iteration 359, loss = 0.01516555\n",
      "Iteration 360, loss = 0.01508222\n",
      "Iteration 361, loss = 0.01498477\n",
      "Iteration 362, loss = 0.01496808\n",
      "Iteration 363, loss = 0.01490151\n",
      "Iteration 364, loss = 0.01492182\n",
      "Iteration 365, loss = 0.01488355\n",
      "Iteration 366, loss = 0.01482352\n",
      "Iteration 367, loss = 0.01490061\n",
      "Iteration 368, loss = 0.01467266\n",
      "Iteration 369, loss = 0.01479557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 370, loss = 0.01464288\n",
      "Iteration 371, loss = 0.01458594\n",
      "Iteration 372, loss = 0.01456672\n",
      "Iteration 373, loss = 0.01449976\n",
      "Iteration 374, loss = 0.01450148\n",
      "Iteration 375, loss = 0.01440551\n",
      "Iteration 376, loss = 0.01443276\n",
      "Iteration 377, loss = 0.01436259\n",
      "Iteration 378, loss = 0.01432895\n",
      "Iteration 379, loss = 0.01424566\n",
      "Iteration 380, loss = 0.01420451\n",
      "Iteration 381, loss = 0.01425375\n",
      "Iteration 382, loss = 0.01413369\n",
      "Iteration 383, loss = 0.01413885\n",
      "Iteration 384, loss = 0.01413358\n",
      "Iteration 385, loss = 0.01406291\n",
      "Iteration 386, loss = 0.01402966\n",
      "Iteration 387, loss = 0.01399284\n",
      "Iteration 388, loss = 0.01396746\n",
      "Iteration 389, loss = 0.01393777\n",
      "Iteration 390, loss = 0.01390694\n",
      "Iteration 391, loss = 0.01384797\n",
      "Iteration 392, loss = 0.01377278\n",
      "Iteration 393, loss = 0.01380031\n",
      "Iteration 394, loss = 0.01384651\n",
      "Iteration 395, loss = 0.01367893\n",
      "Iteration 396, loss = 0.01363617\n",
      "Iteration 397, loss = 0.01358498\n",
      "Iteration 398, loss = 0.01364920\n",
      "Iteration 399, loss = 0.01358273\n",
      "Iteration 400, loss = 0.01357660\n",
      "Iteration 401, loss = 0.01370542\n",
      "Iteration 402, loss = 0.01344324\n",
      "Iteration 403, loss = 0.01346987\n",
      "Iteration 404, loss = 0.01337054\n",
      "Iteration 405, loss = 0.01338234\n",
      "Iteration 406, loss = 0.01330766\n",
      "Iteration 407, loss = 0.01323107\n",
      "Iteration 408, loss = 0.01321624\n",
      "Iteration 409, loss = 0.01320678\n",
      "Iteration 410, loss = 0.01323830\n",
      "Iteration 411, loss = 0.01313314\n",
      "Iteration 412, loss = 0.01307302\n",
      "Iteration 413, loss = 0.01307752\n",
      "Iteration 414, loss = 0.01307818\n",
      "Iteration 415, loss = 0.01300885\n",
      "Iteration 416, loss = 0.01298755\n",
      "Iteration 417, loss = 0.01292657\n",
      "Iteration 418, loss = 0.01287134\n",
      "Iteration 419, loss = 0.01291500\n",
      "Iteration 420, loss = 0.01290635\n",
      "Iteration 421, loss = 0.01280183\n",
      "Iteration 422, loss = 0.01281280\n",
      "Iteration 423, loss = 0.01275737\n",
      "Iteration 424, loss = 0.01270365\n",
      "Iteration 425, loss = 0.01268436\n",
      "Iteration 426, loss = 0.01264325\n",
      "Iteration 427, loss = 0.01260174\n",
      "Iteration 428, loss = 0.01269935\n",
      "Iteration 429, loss = 0.01255212\n",
      "Iteration 430, loss = 0.01255708\n",
      "Iteration 431, loss = 0.01245382\n",
      "Iteration 432, loss = 0.01249150\n",
      "Iteration 433, loss = 0.01240895\n",
      "Iteration 434, loss = 0.01256282\n",
      "Iteration 435, loss = 0.01235547\n",
      "Iteration 436, loss = 0.01238688\n",
      "Iteration 437, loss = 0.01231597\n",
      "Iteration 438, loss = 0.01234357\n",
      "Iteration 439, loss = 0.01232958\n",
      "Iteration 440, loss = 0.01228796\n",
      "Iteration 441, loss = 0.01221764\n",
      "Iteration 442, loss = 0.01219234\n",
      "Iteration 443, loss = 0.01215416\n",
      "Iteration 444, loss = 0.01217436\n",
      "Iteration 445, loss = 0.01207308\n",
      "Iteration 446, loss = 0.01205652\n",
      "Iteration 447, loss = 0.01200285\n",
      "Iteration 448, loss = 0.01199542\n",
      "Iteration 449, loss = 0.01199290\n",
      "Iteration 450, loss = 0.01191507\n",
      "Iteration 451, loss = 0.01195559\n",
      "Iteration 452, loss = 0.01186603\n",
      "Iteration 453, loss = 0.01184130\n",
      "Iteration 454, loss = 0.01181387\n",
      "Iteration 455, loss = 0.01186683\n",
      "Iteration 456, loss = 0.01178380\n",
      "Iteration 457, loss = 0.01173495\n",
      "Iteration 458, loss = 0.01176214\n",
      "Iteration 459, loss = 0.01163716\n",
      "Iteration 460, loss = 0.01168519\n",
      "Iteration 461, loss = 0.01163139\n",
      "Iteration 462, loss = 0.01168376\n",
      "Iteration 463, loss = 0.01159254\n",
      "Iteration 464, loss = 0.01171172\n",
      "Iteration 465, loss = 0.01157119\n",
      "Iteration 466, loss = 0.01149618\n",
      "Iteration 467, loss = 0.01146819\n",
      "Iteration 468, loss = 0.01143141\n",
      "Iteration 469, loss = 0.01141714\n",
      "Iteration 470, loss = 0.01145507\n",
      "Iteration 471, loss = 0.01138071\n",
      "Iteration 472, loss = 0.01138899\n",
      "Iteration 473, loss = 0.01134935\n",
      "Iteration 474, loss = 0.01129828\n",
      "Iteration 475, loss = 0.01137367\n",
      "Iteration 476, loss = 0.01123663\n",
      "Iteration 477, loss = 0.01125596\n",
      "Iteration 478, loss = 0.01120763\n",
      "Iteration 479, loss = 0.01114584\n",
      "Iteration 480, loss = 0.01110320\n",
      "Iteration 481, loss = 0.01112672\n",
      "Iteration 482, loss = 0.01117630\n",
      "Iteration 483, loss = 0.01107555\n",
      "Iteration 484, loss = 0.01106444\n",
      "Iteration 485, loss = 0.01099734\n",
      "Iteration 486, loss = 0.01100651\n",
      "Iteration 487, loss = 0.01096892\n",
      "Iteration 488, loss = 0.01101416\n",
      "Iteration 489, loss = 0.01096607\n",
      "Iteration 490, loss = 0.01092164\n",
      "Iteration 491, loss = 0.01085709\n",
      "Iteration 492, loss = 0.01087135\n",
      "Iteration 493, loss = 0.01081506\n",
      "Iteration 494, loss = 0.01080789\n",
      "Iteration 495, loss = 0.01078657\n",
      "Iteration 496, loss = 0.01074398\n",
      "Iteration 497, loss = 0.01074227\n",
      "Iteration 498, loss = 0.01071724\n",
      "Iteration 499, loss = 0.01066359\n",
      "Iteration 500, loss = 0.01072348\n",
      "Iteration 501, loss = 0.01064752\n",
      "Iteration 502, loss = 0.01073521\n",
      "Iteration 503, loss = 0.01071206\n",
      "Iteration 504, loss = 0.01064401\n",
      "Iteration 505, loss = 0.01052607\n",
      "Iteration 506, loss = 0.01053674\n",
      "Iteration 507, loss = 0.01055899\n",
      "Iteration 508, loss = 0.01046965\n",
      "Iteration 509, loss = 0.01045343\n",
      "Iteration 510, loss = 0.01039918\n",
      "Iteration 511, loss = 0.01042394\n",
      "Iteration 512, loss = 0.01035826\n",
      "Iteration 513, loss = 0.01039539\n",
      "Iteration 514, loss = 0.01033564\n",
      "Iteration 515, loss = 0.01032330\n",
      "Iteration 516, loss = 0.01035422\n",
      "Iteration 517, loss = 0.01031232\n",
      "Iteration 518, loss = 0.01027254\n",
      "Iteration 519, loss = 0.01024469\n",
      "Iteration 520, loss = 0.01016420\n",
      "Iteration 521, loss = 0.01020529\n",
      "Iteration 522, loss = 0.01015238\n",
      "Iteration 523, loss = 0.01015195\n",
      "Iteration 524, loss = 0.01013479\n",
      "Iteration 525, loss = 0.01010929\n",
      "Iteration 526, loss = 0.01008539\n",
      "Iteration 527, loss = 0.01007298\n",
      "Iteration 528, loss = 0.01006171\n",
      "Iteration 529, loss = 0.00999050\n",
      "Iteration 530, loss = 0.00996128\n",
      "Iteration 531, loss = 0.01009568\n",
      "Iteration 532, loss = 0.00998085\n",
      "Iteration 533, loss = 0.00996446\n",
      "Iteration 534, loss = 0.00987655\n",
      "Iteration 535, loss = 0.00986906\n",
      "Iteration 536, loss = 0.00984337\n",
      "Iteration 537, loss = 0.00987479\n",
      "Iteration 538, loss = 0.00979124\n",
      "Iteration 539, loss = 0.00986251\n",
      "Iteration 540, loss = 0.00975989\n",
      "Iteration 541, loss = 0.00981288\n",
      "Iteration 542, loss = 0.00977352\n",
      "Iteration 543, loss = 0.00971371\n",
      "Iteration 544, loss = 0.00972387\n",
      "Iteration 545, loss = 0.00966949\n",
      "Iteration 546, loss = 0.00969221\n",
      "Iteration 547, loss = 0.00969327\n",
      "Iteration 548, loss = 0.00970680\n",
      "Iteration 549, loss = 0.00967954\n",
      "Iteration 550, loss = 0.00966168\n",
      "Iteration 551, loss = 0.00963381\n",
      "Iteration 552, loss = 0.00958559\n",
      "Iteration 553, loss = 0.00957337\n",
      "Iteration 554, loss = 0.00953660\n",
      "Iteration 555, loss = 0.00945704\n",
      "Iteration 556, loss = 0.00950236\n",
      "Iteration 557, loss = 0.00944761\n",
      "Iteration 558, loss = 0.00944066\n",
      "Iteration 559, loss = 0.00946689\n",
      "Iteration 560, loss = 0.00944559\n",
      "Iteration 561, loss = 0.00950997\n",
      "Iteration 562, loss = 0.00947045\n",
      "Iteration 563, loss = 0.00935169\n",
      "Iteration 564, loss = 0.00939228\n",
      "Iteration 565, loss = 0.00931331\n",
      "Iteration 566, loss = 0.00927758\n",
      "Iteration 567, loss = 0.00932641\n",
      "Iteration 568, loss = 0.00922124\n",
      "Iteration 569, loss = 0.00937391\n",
      "Iteration 570, loss = 0.00923302\n",
      "Iteration 571, loss = 0.00925385\n",
      "Iteration 572, loss = 0.00920030\n",
      "Iteration 573, loss = 0.00919163\n",
      "Iteration 574, loss = 0.00910407\n",
      "Iteration 575, loss = 0.00922637\n",
      "Iteration 576, loss = 0.00910727\n",
      "Iteration 577, loss = 0.00911118\n",
      "Iteration 578, loss = 0.00911571\n",
      "Iteration 579, loss = 0.00907926\n",
      "Iteration 580, loss = 0.00904868\n",
      "Iteration 581, loss = 0.00903867\n",
      "Iteration 582, loss = 0.00906109\n",
      "Iteration 583, loss = 0.00902419\n",
      "Iteration 584, loss = 0.00893895\n",
      "Iteration 585, loss = 0.00892519\n",
      "Iteration 586, loss = 0.00893907\n",
      "Iteration 587, loss = 0.00894624\n",
      "Iteration 588, loss = 0.00889456\n",
      "Iteration 589, loss = 0.00893517\n",
      "Iteration 590, loss = 0.00887557\n",
      "Iteration 591, loss = 0.00890529\n",
      "Iteration 592, loss = 0.00885949\n",
      "Iteration 593, loss = 0.00889353\n",
      "Iteration 594, loss = 0.00879400\n",
      "Iteration 595, loss = 0.00879411\n",
      "Iteration 596, loss = 0.00877105\n",
      "Iteration 597, loss = 0.00874929\n",
      "Iteration 598, loss = 0.00872405\n",
      "Iteration 599, loss = 0.00869136\n",
      "Iteration 600, loss = 0.00869092\n",
      "Iteration 601, loss = 0.00870819\n",
      "Iteration 602, loss = 0.00865965\n",
      "Iteration 603, loss = 0.00868666\n",
      "Iteration 604, loss = 0.00863205\n",
      "Iteration 605, loss = 0.00871918\n",
      "Iteration 606, loss = 0.00860841\n",
      "Iteration 607, loss = 0.00863006\n",
      "Iteration 608, loss = 0.00862679\n",
      "Iteration 609, loss = 0.00860415\n",
      "Iteration 610, loss = 0.00854265\n",
      "Iteration 611, loss = 0.00848998\n",
      "Iteration 612, loss = 0.00850954\n",
      "Iteration 613, loss = 0.00845395\n",
      "Iteration 614, loss = 0.00849504\n",
      "Iteration 615, loss = 0.00845967\n",
      "Iteration 616, loss = 0.00844174\n",
      "Iteration 617, loss = 0.00856024\n",
      "Iteration 618, loss = 0.00839132\n",
      "Iteration 619, loss = 0.00844504\n",
      "Iteration 620, loss = 0.00849617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 621, loss = 0.00851558\n",
      "Iteration 622, loss = 0.00840093\n",
      "Iteration 623, loss = 0.00830688\n",
      "Iteration 624, loss = 0.00834066\n",
      "Iteration 625, loss = 0.00831410\n",
      "Iteration 626, loss = 0.00836663\n",
      "Iteration 627, loss = 0.00847361\n",
      "Iteration 628, loss = 0.00826589\n",
      "Iteration 629, loss = 0.00840595\n",
      "Iteration 630, loss = 0.00838282\n",
      "Iteration 631, loss = 0.00828443\n",
      "Iteration 632, loss = 0.00821942\n",
      "Iteration 633, loss = 0.00849575\n",
      "Iteration 634, loss = 0.00813110\n",
      "Iteration 635, loss = 0.00836315\n",
      "Iteration 636, loss = 0.00824020\n",
      "Iteration 637, loss = 0.00814691\n",
      "Iteration 638, loss = 0.00809619\n",
      "Iteration 639, loss = 0.00813618\n",
      "Iteration 640, loss = 0.00809900\n",
      "Iteration 641, loss = 0.00812063\n",
      "Iteration 642, loss = 0.00804498\n",
      "Iteration 643, loss = 0.00802148\n",
      "Iteration 644, loss = 0.00799819\n",
      "Iteration 645, loss = 0.00798718\n",
      "Iteration 646, loss = 0.00799898\n",
      "Iteration 647, loss = 0.00806985\n",
      "Iteration 648, loss = 0.00799223\n",
      "Iteration 649, loss = 0.00796717\n",
      "Iteration 650, loss = 0.00796594\n",
      "Iteration 651, loss = 0.00812562\n",
      "Iteration 652, loss = 0.00788854\n",
      "Iteration 653, loss = 0.00788071\n",
      "Iteration 654, loss = 0.00795350\n",
      "Iteration 655, loss = 0.00794558\n",
      "Iteration 656, loss = 0.00789650\n",
      "Iteration 657, loss = 0.00781017\n",
      "Iteration 658, loss = 0.00785672\n",
      "Iteration 659, loss = 0.00784988\n",
      "Iteration 660, loss = 0.00779954\n",
      "Iteration 661, loss = 0.00779732\n",
      "Iteration 662, loss = 0.00781389\n",
      "Iteration 663, loss = 0.00773212\n",
      "Iteration 664, loss = 0.00776675\n",
      "Iteration 665, loss = 0.00777499\n",
      "Iteration 666, loss = 0.00778673\n",
      "Iteration 667, loss = 0.00775985\n",
      "Iteration 668, loss = 0.00776976\n",
      "Iteration 669, loss = 0.00768341\n",
      "Iteration 670, loss = 0.00774589\n",
      "Iteration 671, loss = 0.00763899\n",
      "Iteration 672, loss = 0.00772694\n",
      "Iteration 673, loss = 0.00767738\n",
      "Iteration 674, loss = 0.00765629\n",
      "Iteration 675, loss = 0.00766479\n",
      "Iteration 676, loss = 0.00765655\n",
      "Iteration 677, loss = 0.00761167\n",
      "Iteration 678, loss = 0.00770722\n",
      "Iteration 679, loss = 0.00762892\n",
      "Iteration 680, loss = 0.00755007\n",
      "Iteration 681, loss = 0.00748285\n",
      "Iteration 682, loss = 0.00747445\n",
      "Iteration 683, loss = 0.00748378\n",
      "Iteration 684, loss = 0.00756023\n",
      "Iteration 685, loss = 0.00750278\n",
      "Iteration 686, loss = 0.00745466\n",
      "Iteration 687, loss = 0.00743669\n",
      "Iteration 688, loss = 0.00748965\n",
      "Iteration 689, loss = 0.00739496\n",
      "Iteration 690, loss = 0.00736257\n",
      "Iteration 691, loss = 0.00740847\n",
      "Iteration 692, loss = 0.00738954\n",
      "Iteration 693, loss = 0.00741380\n",
      "Iteration 694, loss = 0.00735034\n",
      "Iteration 695, loss = 0.00729677\n",
      "Iteration 696, loss = 0.00737567\n",
      "Iteration 697, loss = 0.00729279\n",
      "Iteration 698, loss = 0.00730482\n",
      "Iteration 699, loss = 0.00734141\n",
      "Iteration 700, loss = 0.00730191\n",
      "Iteration 701, loss = 0.00727568\n",
      "Iteration 702, loss = 0.00730222\n",
      "Iteration 703, loss = 0.00722274\n",
      "Iteration 704, loss = 0.00722183\n",
      "Iteration 705, loss = 0.00726759\n",
      "Iteration 706, loss = 0.00719524\n",
      "Iteration 707, loss = 0.00723948\n",
      "Iteration 708, loss = 0.00716300\n",
      "Iteration 709, loss = 0.00716859\n",
      "Iteration 710, loss = 0.00714841\n",
      "Iteration 711, loss = 0.00722203\n",
      "Iteration 712, loss = 0.00716231\n",
      "Iteration 713, loss = 0.00710796\n",
      "Iteration 714, loss = 0.00711199\n",
      "Iteration 715, loss = 0.00708685\n",
      "Iteration 716, loss = 0.00709089\n",
      "Iteration 717, loss = 0.00719002\n",
      "Iteration 718, loss = 0.00702333\n",
      "Iteration 719, loss = 0.00700082\n",
      "Iteration 720, loss = 0.00705137\n",
      "Iteration 721, loss = 0.00702858\n",
      "Iteration 722, loss = 0.00698150\n",
      "Iteration 723, loss = 0.00699385\n",
      "Iteration 724, loss = 0.00704203\n",
      "Iteration 725, loss = 0.00694785\n",
      "Iteration 726, loss = 0.00706225\n",
      "Iteration 727, loss = 0.00696444\n",
      "Iteration 728, loss = 0.00692639\n",
      "Iteration 729, loss = 0.00695684\n",
      "Iteration 730, loss = 0.00697172\n",
      "Iteration 731, loss = 0.00691601\n",
      "Iteration 732, loss = 0.00694801\n",
      "Iteration 733, loss = 0.00689139\n",
      "Iteration 734, loss = 0.00687769\n",
      "Iteration 735, loss = 0.00700643\n",
      "Iteration 736, loss = 0.00683918\n",
      "Iteration 737, loss = 0.00686894\n",
      "Iteration 738, loss = 0.00682841\n",
      "Iteration 739, loss = 0.00680284\n",
      "Iteration 740, loss = 0.00679925\n",
      "Iteration 741, loss = 0.00679218\n",
      "Iteration 742, loss = 0.00675566\n",
      "Iteration 743, loss = 0.00682628\n",
      "Iteration 744, loss = 0.00677512\n",
      "Iteration 745, loss = 0.00680250\n",
      "Iteration 746, loss = 0.00673382\n",
      "Iteration 747, loss = 0.00674880\n",
      "Iteration 748, loss = 0.00673496\n",
      "Iteration 749, loss = 0.00671782\n",
      "Iteration 750, loss = 0.00673436\n",
      "Iteration 751, loss = 0.00665851\n",
      "Iteration 752, loss = 0.00676386\n",
      "Iteration 753, loss = 0.00676183\n",
      "Iteration 754, loss = 0.00669011\n",
      "Iteration 755, loss = 0.00667536\n",
      "Iteration 756, loss = 0.00668798\n",
      "Iteration 757, loss = 0.00674195\n",
      "Iteration 758, loss = 0.00655363\n",
      "Iteration 759, loss = 0.00666612\n",
      "Iteration 760, loss = 0.00662986\n",
      "Iteration 761, loss = 0.00656619\n",
      "Iteration 762, loss = 0.00656547\n",
      "Iteration 763, loss = 0.00658753\n",
      "Iteration 764, loss = 0.00660855\n",
      "Iteration 765, loss = 0.00657232\n",
      "Iteration 766, loss = 0.00651282\n",
      "Iteration 767, loss = 0.00658592\n",
      "Iteration 768, loss = 0.00650729\n",
      "Iteration 769, loss = 0.00652927\n",
      "Iteration 770, loss = 0.00659586\n",
      "Iteration 771, loss = 0.00645729\n",
      "Iteration 772, loss = 0.00644849\n",
      "Iteration 773, loss = 0.00645013\n",
      "Iteration 774, loss = 0.00642916\n",
      "Iteration 775, loss = 0.00642036\n",
      "Iteration 776, loss = 0.00643395\n",
      "Iteration 777, loss = 0.00636546\n",
      "Iteration 778, loss = 0.00641154\n",
      "Iteration 779, loss = 0.00642325\n",
      "Iteration 780, loss = 0.00637864\n",
      "Iteration 781, loss = 0.00659720\n",
      "Iteration 782, loss = 0.00630304\n",
      "Iteration 783, loss = 0.00638584\n",
      "Iteration 784, loss = 0.00644531\n",
      "Iteration 785, loss = 0.00636911\n",
      "Iteration 786, loss = 0.00631159\n",
      "Iteration 787, loss = 0.00628786\n",
      "Iteration 788, loss = 0.00642967\n",
      "Iteration 789, loss = 0.00639354\n",
      "Iteration 790, loss = 0.00647652\n",
      "Iteration 791, loss = 0.00627181\n",
      "Iteration 792, loss = 0.00623977\n",
      "Iteration 793, loss = 0.00624302\n",
      "Iteration 794, loss = 0.00625121\n",
      "Iteration 795, loss = 0.00620203\n",
      "Iteration 796, loss = 0.00626234\n",
      "Iteration 797, loss = 0.00625537\n",
      "Iteration 798, loss = 0.00621832\n",
      "Iteration 799, loss = 0.00616823\n",
      "Iteration 800, loss = 0.00618227\n",
      "Iteration 801, loss = 0.00619141\n",
      "Iteration 802, loss = 0.00617419\n",
      "Iteration 803, loss = 0.00615203\n",
      "Iteration 804, loss = 0.00619201\n",
      "Iteration 805, loss = 0.00617417\n",
      "Iteration 806, loss = 0.00614044\n",
      "Iteration 807, loss = 0.00617986\n",
      "Iteration 808, loss = 0.00616415\n",
      "Iteration 809, loss = 0.00607416\n",
      "Iteration 810, loss = 0.00610133\n",
      "Iteration 811, loss = 0.00612123\n",
      "Iteration 812, loss = 0.00607339\n",
      "Iteration 813, loss = 0.00605934\n",
      "Iteration 814, loss = 0.00606054\n",
      "Iteration 815, loss = 0.00601918\n",
      "Iteration 816, loss = 0.00603184\n",
      "Iteration 817, loss = 0.00610745\n",
      "Iteration 818, loss = 0.00601422\n",
      "Iteration 819, loss = 0.00597714\n",
      "Iteration 820, loss = 0.00595885\n",
      "Iteration 821, loss = 0.00598027\n",
      "Iteration 822, loss = 0.00596635\n",
      "Iteration 823, loss = 0.00597676\n",
      "Iteration 824, loss = 0.00600372\n",
      "Iteration 825, loss = 0.00595782\n",
      "Iteration 826, loss = 0.00594916\n",
      "Iteration 827, loss = 0.00598350\n",
      "Iteration 828, loss = 0.00592401\n",
      "Iteration 829, loss = 0.00591062\n",
      "Iteration 830, loss = 0.00595861\n",
      "Iteration 831, loss = 0.00588147\n",
      "Iteration 832, loss = 0.00589192\n",
      "Iteration 833, loss = 0.00602556\n",
      "Iteration 834, loss = 0.00586903\n",
      "Iteration 835, loss = 0.00595232\n",
      "Iteration 836, loss = 0.00583284\n",
      "Iteration 837, loss = 0.00587048\n",
      "Iteration 838, loss = 0.00579731\n",
      "Iteration 839, loss = 0.00589013\n",
      "Iteration 840, loss = 0.00577339\n",
      "Iteration 841, loss = 0.00577654\n",
      "Iteration 842, loss = 0.00584547\n",
      "Iteration 843, loss = 0.00576898\n",
      "Iteration 844, loss = 0.00575810\n",
      "Iteration 845, loss = 0.00579823\n",
      "Iteration 846, loss = 0.00580715\n",
      "Iteration 847, loss = 0.00577548\n",
      "Iteration 848, loss = 0.00571403\n",
      "Iteration 849, loss = 0.00574933\n",
      "Iteration 850, loss = 0.00574507\n",
      "Iteration 851, loss = 0.00567013\n",
      "Iteration 852, loss = 0.00575571\n",
      "Iteration 853, loss = 0.00574054\n",
      "Iteration 854, loss = 0.00573344\n",
      "Iteration 855, loss = 0.00576014\n",
      "Iteration 856, loss = 0.00574729\n",
      "Iteration 857, loss = 0.00567634\n",
      "Iteration 858, loss = 0.00571744\n",
      "Iteration 859, loss = 0.00565773\n",
      "Iteration 860, loss = 0.00568764\n",
      "Iteration 861, loss = 0.00567123\n",
      "Iteration 862, loss = 0.00558190\n",
      "Iteration 863, loss = 0.00558231\n",
      "Iteration 864, loss = 0.00561609\n",
      "Iteration 865, loss = 0.00565998\n",
      "Iteration 866, loss = 0.00569983\n",
      "Iteration 867, loss = 0.00566626\n",
      "Iteration 868, loss = 0.00570548\n",
      "Iteration 869, loss = 0.00561207\n",
      "Iteration 870, loss = 0.00564649\n",
      "Iteration 871, loss = 0.00559902\n",
      "Iteration 872, loss = 0.00555430\n",
      "Iteration 873, loss = 0.00550255\n",
      "Iteration 874, loss = 0.00561002\n",
      "Iteration 875, loss = 0.00549610\n",
      "Iteration 876, loss = 0.00554762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 877, loss = 0.00549070\n",
      "Iteration 878, loss = 0.00560697\n",
      "Iteration 879, loss = 0.00548814\n",
      "Iteration 880, loss = 0.00556499\n",
      "Iteration 881, loss = 0.00548019\n",
      "Iteration 882, loss = 0.00550586\n",
      "Iteration 883, loss = 0.00549474\n",
      "Iteration 884, loss = 0.00541667\n",
      "Iteration 885, loss = 0.00547497\n",
      "Iteration 886, loss = 0.00547124\n",
      "Iteration 887, loss = 0.00549180\n",
      "Iteration 888, loss = 0.00548875\n",
      "Iteration 889, loss = 0.00544252\n",
      "Iteration 890, loss = 0.00543343\n",
      "Iteration 891, loss = 0.00539311\n",
      "Iteration 892, loss = 0.00536126\n",
      "Iteration 893, loss = 0.00533171\n",
      "Iteration 894, loss = 0.00545057\n",
      "Iteration 895, loss = 0.00537515\n",
      "Iteration 896, loss = 0.00541140\n",
      "Iteration 897, loss = 0.00533912\n",
      "Iteration 898, loss = 0.00536454\n",
      "Iteration 899, loss = 0.00535623\n",
      "Iteration 900, loss = 0.00533738\n",
      "Iteration 901, loss = 0.00531634\n",
      "Iteration 902, loss = 0.00535320\n",
      "Iteration 903, loss = 0.00527168\n",
      "Iteration 904, loss = 0.00533319\n",
      "Iteration 905, loss = 0.00538283\n",
      "Iteration 906, loss = 0.00538548\n",
      "Iteration 907, loss = 0.00522830\n",
      "Iteration 908, loss = 0.00528662\n",
      "Iteration 909, loss = 0.00523363\n",
      "Iteration 910, loss = 0.00532450\n",
      "Iteration 911, loss = 0.00524032\n",
      "Iteration 912, loss = 0.00532733\n",
      "Iteration 913, loss = 0.00525237\n",
      "Iteration 914, loss = 0.00530728\n",
      "Iteration 915, loss = 0.00531131\n",
      "Iteration 916, loss = 0.00523304\n",
      "Iteration 917, loss = 0.00517012\n",
      "Iteration 918, loss = 0.00516388\n",
      "Iteration 919, loss = 0.00520362\n",
      "Iteration 920, loss = 0.00517382\n",
      "Iteration 921, loss = 0.00519043\n",
      "Iteration 922, loss = 0.00527502\n",
      "Iteration 923, loss = 0.00516854\n",
      "Iteration 924, loss = 0.00515358\n",
      "Iteration 925, loss = 0.00510176\n",
      "Iteration 926, loss = 0.00508916\n",
      "Iteration 927, loss = 0.00520007\n",
      "Iteration 928, loss = 0.00507470\n",
      "Iteration 929, loss = 0.00514340\n",
      "Iteration 930, loss = 0.00514856\n",
      "Iteration 931, loss = 0.00512116\n",
      "Iteration 932, loss = 0.00519068\n",
      "Iteration 933, loss = 0.00520559\n",
      "Iteration 934, loss = 0.00516584\n",
      "Iteration 935, loss = 0.00511698\n",
      "Iteration 936, loss = 0.00502368\n",
      "Iteration 937, loss = 0.00500374\n",
      "Iteration 938, loss = 0.00503507\n",
      "Iteration 939, loss = 0.00504436\n",
      "Iteration 940, loss = 0.00506586\n",
      "Iteration 941, loss = 0.00508539\n",
      "Iteration 942, loss = 0.00496465\n",
      "Iteration 943, loss = 0.00497769\n",
      "Iteration 944, loss = 0.00500406\n",
      "Iteration 945, loss = 0.00497355\n",
      "Iteration 946, loss = 0.00502414\n",
      "Iteration 947, loss = 0.00496395\n",
      "Iteration 948, loss = 0.00499919\n",
      "Iteration 949, loss = 0.00499266\n",
      "Iteration 950, loss = 0.00496056\n",
      "Iteration 951, loss = 0.00516012\n",
      "Iteration 952, loss = 0.00509241\n",
      "Iteration 953, loss = 0.00496514\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66176839\n",
      "Iteration 2, loss = 0.59314541\n",
      "Iteration 3, loss = 0.53489568\n",
      "Iteration 4, loss = 0.48559129\n",
      "Iteration 5, loss = 0.44272858\n",
      "Iteration 6, loss = 0.40626359\n",
      "Iteration 7, loss = 0.37415633\n",
      "Iteration 8, loss = 0.34610708\n",
      "Iteration 9, loss = 0.32123806\n",
      "Iteration 10, loss = 0.29912976\n",
      "Iteration 11, loss = 0.27955158\n",
      "Iteration 12, loss = 0.26203542\n",
      "Iteration 13, loss = 0.24675403\n",
      "Iteration 14, loss = 0.23302118\n",
      "Iteration 15, loss = 0.22096123\n",
      "Iteration 16, loss = 0.21002597\n",
      "Iteration 17, loss = 0.20032170\n",
      "Iteration 18, loss = 0.19144891\n",
      "Iteration 19, loss = 0.18351469\n",
      "Iteration 20, loss = 0.17625073\n",
      "Iteration 21, loss = 0.16946742\n",
      "Iteration 22, loss = 0.16342004\n",
      "Iteration 23, loss = 0.15773397\n",
      "Iteration 24, loss = 0.15259202\n",
      "Iteration 25, loss = 0.14769542\n",
      "Iteration 26, loss = 0.14325782\n",
      "Iteration 27, loss = 0.13901661\n",
      "Iteration 28, loss = 0.13512077\n",
      "Iteration 29, loss = 0.13158771\n",
      "Iteration 30, loss = 0.12815089\n",
      "Iteration 31, loss = 0.12508667\n",
      "Iteration 32, loss = 0.12213201\n",
      "Iteration 33, loss = 0.11930081\n",
      "Iteration 34, loss = 0.11677395\n",
      "Iteration 35, loss = 0.11430796\n",
      "Iteration 36, loss = 0.11206549\n",
      "Iteration 37, loss = 0.10982553\n",
      "Iteration 38, loss = 0.10779769\n",
      "Iteration 39, loss = 0.10586661\n",
      "Iteration 40, loss = 0.10393761\n",
      "Iteration 41, loss = 0.10209728\n",
      "Iteration 42, loss = 0.10037087\n",
      "Iteration 43, loss = 0.09864908\n",
      "Iteration 44, loss = 0.09706970\n",
      "Iteration 45, loss = 0.09546903\n",
      "Iteration 46, loss = 0.09410201\n",
      "Iteration 47, loss = 0.09265341\n",
      "Iteration 48, loss = 0.09123079\n",
      "Iteration 49, loss = 0.08983621\n",
      "Iteration 50, loss = 0.08853208\n",
      "Iteration 51, loss = 0.08728570\n",
      "Iteration 52, loss = 0.08609268\n",
      "Iteration 53, loss = 0.08492621\n",
      "Iteration 54, loss = 0.08382687\n",
      "Iteration 55, loss = 0.08278134\n",
      "Iteration 56, loss = 0.08165615\n",
      "Iteration 57, loss = 0.08062729\n",
      "Iteration 58, loss = 0.07959519\n",
      "Iteration 59, loss = 0.07870023\n",
      "Iteration 60, loss = 0.07764293\n",
      "Iteration 61, loss = 0.07681796\n",
      "Iteration 62, loss = 0.07586533\n",
      "Iteration 63, loss = 0.07504604\n",
      "Iteration 64, loss = 0.07400555\n",
      "Iteration 65, loss = 0.07331886\n",
      "Iteration 66, loss = 0.07234595\n",
      "Iteration 67, loss = 0.07150541\n",
      "Iteration 68, loss = 0.07076838\n",
      "Iteration 69, loss = 0.07007762\n",
      "Iteration 70, loss = 0.06927931\n",
      "Iteration 71, loss = 0.06857416\n",
      "Iteration 72, loss = 0.06783659\n",
      "Iteration 73, loss = 0.06714843\n",
      "Iteration 74, loss = 0.06641472\n",
      "Iteration 75, loss = 0.06579678\n",
      "Iteration 76, loss = 0.06511544\n",
      "Iteration 77, loss = 0.06448818\n",
      "Iteration 78, loss = 0.06386355\n",
      "Iteration 79, loss = 0.06331280\n",
      "Iteration 80, loss = 0.06268923\n",
      "Iteration 81, loss = 0.06212844\n",
      "Iteration 82, loss = 0.06158578\n",
      "Iteration 83, loss = 0.06106897\n",
      "Iteration 84, loss = 0.06043990\n",
      "Iteration 85, loss = 0.05997209\n",
      "Iteration 86, loss = 0.05940467\n",
      "Iteration 87, loss = 0.05889114\n",
      "Iteration 88, loss = 0.05843230\n",
      "Iteration 89, loss = 0.05805339\n",
      "Iteration 90, loss = 0.05753278\n",
      "Iteration 91, loss = 0.05699920\n",
      "Iteration 92, loss = 0.05652921\n",
      "Iteration 93, loss = 0.05607362\n",
      "Iteration 94, loss = 0.05556086\n",
      "Iteration 95, loss = 0.05505718\n",
      "Iteration 96, loss = 0.05464567\n",
      "Iteration 97, loss = 0.05428676\n",
      "Iteration 98, loss = 0.05380722\n",
      "Iteration 99, loss = 0.05336158\n",
      "Iteration 100, loss = 0.05291729\n",
      "Iteration 101, loss = 0.05260419\n",
      "Iteration 102, loss = 0.05211555\n",
      "Iteration 103, loss = 0.05171246\n",
      "Iteration 104, loss = 0.05139763\n",
      "Iteration 105, loss = 0.05093798\n",
      "Iteration 106, loss = 0.05073045\n",
      "Iteration 107, loss = 0.05018474\n",
      "Iteration 108, loss = 0.04973730\n",
      "Iteration 109, loss = 0.04939823\n",
      "Iteration 110, loss = 0.04907412\n",
      "Iteration 111, loss = 0.04865256\n",
      "Iteration 112, loss = 0.04829847\n",
      "Iteration 113, loss = 0.04797342\n",
      "Iteration 114, loss = 0.04760412\n",
      "Iteration 115, loss = 0.04723484\n",
      "Iteration 116, loss = 0.04690136\n",
      "Iteration 117, loss = 0.04656811\n",
      "Iteration 118, loss = 0.04628899\n",
      "Iteration 119, loss = 0.04594424\n",
      "Iteration 120, loss = 0.04559159\n",
      "Iteration 121, loss = 0.04545025\n",
      "Iteration 122, loss = 0.04504486\n",
      "Iteration 123, loss = 0.04464572\n",
      "Iteration 124, loss = 0.04435467\n",
      "Iteration 125, loss = 0.04410080\n",
      "Iteration 126, loss = 0.04375620\n",
      "Iteration 127, loss = 0.04354977\n",
      "Iteration 128, loss = 0.04323707\n",
      "Iteration 129, loss = 0.04288909\n",
      "Iteration 130, loss = 0.04263164\n",
      "Iteration 131, loss = 0.04234890\n",
      "Iteration 132, loss = 0.04201442\n",
      "Iteration 133, loss = 0.04184101\n",
      "Iteration 134, loss = 0.04151551\n",
      "Iteration 135, loss = 0.04124260\n",
      "Iteration 136, loss = 0.04100488\n",
      "Iteration 137, loss = 0.04071784\n",
      "Iteration 138, loss = 0.04054969\n",
      "Iteration 139, loss = 0.04022991\n",
      "Iteration 140, loss = 0.04001921\n",
      "Iteration 141, loss = 0.03988089\n",
      "Iteration 142, loss = 0.03952571\n",
      "Iteration 143, loss = 0.03933825\n",
      "Iteration 144, loss = 0.03904579\n",
      "Iteration 145, loss = 0.03881727\n",
      "Iteration 146, loss = 0.03861797\n",
      "Iteration 147, loss = 0.03842326\n",
      "Iteration 148, loss = 0.03823695\n",
      "Iteration 149, loss = 0.03793070\n",
      "Iteration 150, loss = 0.03769264\n",
      "Iteration 151, loss = 0.03750651\n",
      "Iteration 152, loss = 0.03733659\n",
      "Iteration 153, loss = 0.03719378\n",
      "Iteration 154, loss = 0.03686288\n",
      "Iteration 155, loss = 0.03677562\n",
      "Iteration 156, loss = 0.03656340\n",
      "Iteration 157, loss = 0.03627247\n",
      "Iteration 158, loss = 0.03609260\n",
      "Iteration 159, loss = 0.03591468\n",
      "Iteration 160, loss = 0.03576672\n",
      "Iteration 161, loss = 0.03548185\n",
      "Iteration 162, loss = 0.03529495\n",
      "Iteration 163, loss = 0.03522095\n",
      "Iteration 164, loss = 0.03495079\n",
      "Iteration 165, loss = 0.03478364\n",
      "Iteration 166, loss = 0.03477597\n",
      "Iteration 167, loss = 0.03440340\n",
      "Iteration 168, loss = 0.03421192\n",
      "Iteration 169, loss = 0.03408019\n",
      "Iteration 170, loss = 0.03388217\n",
      "Iteration 171, loss = 0.03376316\n",
      "Iteration 172, loss = 0.03360871\n",
      "Iteration 173, loss = 0.03347846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 174, loss = 0.03322750\n",
      "Iteration 175, loss = 0.03307351\n",
      "Iteration 176, loss = 0.03290313\n",
      "Iteration 177, loss = 0.03268037\n",
      "Iteration 178, loss = 0.03278332\n",
      "Iteration 179, loss = 0.03239954\n",
      "Iteration 180, loss = 0.03221636\n",
      "Iteration 181, loss = 0.03212065\n",
      "Iteration 182, loss = 0.03194049\n",
      "Iteration 183, loss = 0.03189905\n",
      "Iteration 184, loss = 0.03165980\n",
      "Iteration 185, loss = 0.03146377\n",
      "Iteration 186, loss = 0.03147680\n",
      "Iteration 187, loss = 0.03125106\n",
      "Iteration 188, loss = 0.03110309\n",
      "Iteration 189, loss = 0.03089223\n",
      "Iteration 190, loss = 0.03075387\n",
      "Iteration 191, loss = 0.03056516\n",
      "Iteration 192, loss = 0.03047686\n",
      "Iteration 193, loss = 0.03041111\n",
      "Iteration 194, loss = 0.03034666\n",
      "Iteration 195, loss = 0.03001863\n",
      "Iteration 196, loss = 0.02988663\n",
      "Iteration 197, loss = 0.02983760\n",
      "Iteration 198, loss = 0.02973929\n",
      "Iteration 199, loss = 0.02952348\n",
      "Iteration 200, loss = 0.02945637\n",
      "Iteration 201, loss = 0.02926547\n",
      "Iteration 202, loss = 0.02916287\n",
      "Iteration 203, loss = 0.02899949\n",
      "Iteration 204, loss = 0.02884990\n",
      "Iteration 205, loss = 0.02876628\n",
      "Iteration 206, loss = 0.02866752\n",
      "Iteration 207, loss = 0.02851590\n",
      "Iteration 208, loss = 0.02837178\n",
      "Iteration 209, loss = 0.02833428\n",
      "Iteration 210, loss = 0.02816283\n",
      "Iteration 211, loss = 0.02803454\n",
      "Iteration 212, loss = 0.02789477\n",
      "Iteration 213, loss = 0.02780782\n",
      "Iteration 214, loss = 0.02765816\n",
      "Iteration 215, loss = 0.02758053\n",
      "Iteration 216, loss = 0.02744623\n",
      "Iteration 217, loss = 0.02738228\n",
      "Iteration 218, loss = 0.02718824\n",
      "Iteration 219, loss = 0.02713089\n",
      "Iteration 220, loss = 0.02705379\n",
      "Iteration 221, loss = 0.02684756\n",
      "Iteration 222, loss = 0.02683020\n",
      "Iteration 223, loss = 0.02664575\n",
      "Iteration 224, loss = 0.02651543\n",
      "Iteration 225, loss = 0.02646039\n",
      "Iteration 226, loss = 0.02629642\n",
      "Iteration 227, loss = 0.02624367\n",
      "Iteration 228, loss = 0.02614373\n",
      "Iteration 229, loss = 0.02600261\n",
      "Iteration 230, loss = 0.02591857\n",
      "Iteration 231, loss = 0.02579499\n",
      "Iteration 232, loss = 0.02570872\n",
      "Iteration 233, loss = 0.02558466\n",
      "Iteration 234, loss = 0.02560710\n",
      "Iteration 235, loss = 0.02546430\n",
      "Iteration 236, loss = 0.02530050\n",
      "Iteration 237, loss = 0.02526352\n",
      "Iteration 238, loss = 0.02514392\n",
      "Iteration 239, loss = 0.02505865\n",
      "Iteration 240, loss = 0.02491962\n",
      "Iteration 241, loss = 0.02501682\n",
      "Iteration 242, loss = 0.02491180\n",
      "Iteration 243, loss = 0.02468992\n",
      "Iteration 244, loss = 0.02459212\n",
      "Iteration 245, loss = 0.02444760\n",
      "Iteration 246, loss = 0.02437182\n",
      "Iteration 247, loss = 0.02429875\n",
      "Iteration 248, loss = 0.02425089\n",
      "Iteration 249, loss = 0.02411034\n",
      "Iteration 250, loss = 0.02403248\n",
      "Iteration 251, loss = 0.02394423\n",
      "Iteration 252, loss = 0.02388690\n",
      "Iteration 253, loss = 0.02375550\n",
      "Iteration 254, loss = 0.02381334\n",
      "Iteration 255, loss = 0.02361367\n",
      "Iteration 256, loss = 0.02349533\n",
      "Iteration 257, loss = 0.02348506\n",
      "Iteration 258, loss = 0.02330621\n",
      "Iteration 259, loss = 0.02325253\n",
      "Iteration 260, loss = 0.02324117\n",
      "Iteration 261, loss = 0.02315488\n",
      "Iteration 262, loss = 0.02300847\n",
      "Iteration 263, loss = 0.02297542\n",
      "Iteration 264, loss = 0.02286062\n",
      "Iteration 265, loss = 0.02305360\n",
      "Iteration 266, loss = 0.02264080\n",
      "Iteration 267, loss = 0.02268941\n",
      "Iteration 268, loss = 0.02266246\n",
      "Iteration 269, loss = 0.02250063\n",
      "Iteration 270, loss = 0.02239648\n",
      "Iteration 271, loss = 0.02230691\n",
      "Iteration 272, loss = 0.02223768\n",
      "Iteration 273, loss = 0.02213125\n",
      "Iteration 274, loss = 0.02214691\n",
      "Iteration 275, loss = 0.02204889\n",
      "Iteration 276, loss = 0.02194947\n",
      "Iteration 277, loss = 0.02189187\n",
      "Iteration 278, loss = 0.02185405\n",
      "Iteration 279, loss = 0.02170400\n",
      "Iteration 280, loss = 0.02165847\n",
      "Iteration 281, loss = 0.02159067\n",
      "Iteration 282, loss = 0.02147703\n",
      "Iteration 283, loss = 0.02140475\n",
      "Iteration 284, loss = 0.02144921\n",
      "Iteration 285, loss = 0.02129423\n",
      "Iteration 286, loss = 0.02118723\n",
      "Iteration 287, loss = 0.02113196\n",
      "Iteration 288, loss = 0.02108871\n",
      "Iteration 289, loss = 0.02115599\n",
      "Iteration 290, loss = 0.02097664\n",
      "Iteration 291, loss = 0.02092688\n",
      "Iteration 292, loss = 0.02086838\n",
      "Iteration 293, loss = 0.02078364\n",
      "Iteration 294, loss = 0.02073866\n",
      "Iteration 295, loss = 0.02066185\n",
      "Iteration 296, loss = 0.02053282\n",
      "Iteration 297, loss = 0.02052756\n",
      "Iteration 298, loss = 0.02042875\n",
      "Iteration 299, loss = 0.02032079\n",
      "Iteration 300, loss = 0.02034079\n",
      "Iteration 301, loss = 0.02019059\n",
      "Iteration 302, loss = 0.02029746\n",
      "Iteration 303, loss = 0.02016648\n",
      "Iteration 304, loss = 0.02001390\n",
      "Iteration 305, loss = 0.02006774\n",
      "Iteration 306, loss = 0.01996579\n",
      "Iteration 307, loss = 0.01985297\n",
      "Iteration 308, loss = 0.01978029\n",
      "Iteration 309, loss = 0.01974935\n",
      "Iteration 310, loss = 0.01965653\n",
      "Iteration 311, loss = 0.01959351\n",
      "Iteration 312, loss = 0.01967124\n",
      "Iteration 313, loss = 0.01958151\n",
      "Iteration 314, loss = 0.01942437\n",
      "Iteration 315, loss = 0.01936980\n",
      "Iteration 316, loss = 0.01931313\n",
      "Iteration 317, loss = 0.01927992\n",
      "Iteration 318, loss = 0.01922808\n",
      "Iteration 319, loss = 0.01915715\n",
      "Iteration 320, loss = 0.01912994\n",
      "Iteration 321, loss = 0.01901164\n",
      "Iteration 322, loss = 0.01890478\n",
      "Iteration 323, loss = 0.01894805\n",
      "Iteration 324, loss = 0.01881479\n",
      "Iteration 325, loss = 0.01876352\n",
      "Iteration 326, loss = 0.01871538\n",
      "Iteration 327, loss = 0.01864534\n",
      "Iteration 328, loss = 0.01884749\n",
      "Iteration 329, loss = 0.01861296\n",
      "Iteration 330, loss = 0.01857844\n",
      "Iteration 331, loss = 0.01844668\n",
      "Iteration 332, loss = 0.01845048\n",
      "Iteration 333, loss = 0.01829799\n",
      "Iteration 334, loss = 0.01830334\n",
      "Iteration 335, loss = 0.01819918\n",
      "Iteration 336, loss = 0.01818714\n",
      "Iteration 337, loss = 0.01813671\n",
      "Iteration 338, loss = 0.01806873\n",
      "Iteration 339, loss = 0.01799218\n",
      "Iteration 340, loss = 0.01794206\n",
      "Iteration 341, loss = 0.01788885\n",
      "Iteration 342, loss = 0.01796111\n",
      "Iteration 343, loss = 0.01798135\n",
      "Iteration 344, loss = 0.01777839\n",
      "Iteration 345, loss = 0.01778901\n",
      "Iteration 346, loss = 0.01764356\n",
      "Iteration 347, loss = 0.01762609\n",
      "Iteration 348, loss = 0.01750994\n",
      "Iteration 349, loss = 0.01744486\n",
      "Iteration 350, loss = 0.01749175\n",
      "Iteration 351, loss = 0.01742057\n",
      "Iteration 352, loss = 0.01732608\n",
      "Iteration 353, loss = 0.01731330\n",
      "Iteration 354, loss = 0.01725422\n",
      "Iteration 355, loss = 0.01720450\n",
      "Iteration 356, loss = 0.01716916\n",
      "Iteration 357, loss = 0.01712327\n",
      "Iteration 358, loss = 0.01704616\n",
      "Iteration 359, loss = 0.01697487\n",
      "Iteration 360, loss = 0.01699786\n",
      "Iteration 361, loss = 0.01692542\n",
      "Iteration 362, loss = 0.01692156\n",
      "Iteration 363, loss = 0.01685386\n",
      "Iteration 364, loss = 0.01677150\n",
      "Iteration 365, loss = 0.01696568\n",
      "Iteration 366, loss = 0.01684577\n",
      "Iteration 367, loss = 0.01670893\n",
      "Iteration 368, loss = 0.01682666\n",
      "Iteration 369, loss = 0.01677339\n",
      "Iteration 370, loss = 0.01660765\n",
      "Iteration 371, loss = 0.01649464\n",
      "Iteration 372, loss = 0.01645029\n",
      "Iteration 373, loss = 0.01648354\n",
      "Iteration 374, loss = 0.01638144\n",
      "Iteration 375, loss = 0.01630918\n",
      "Iteration 376, loss = 0.01629907\n",
      "Iteration 377, loss = 0.01623516\n",
      "Iteration 378, loss = 0.01614624\n",
      "Iteration 379, loss = 0.01612613\n",
      "Iteration 380, loss = 0.01609681\n",
      "Iteration 381, loss = 0.01605798\n",
      "Iteration 382, loss = 0.01596750\n",
      "Iteration 383, loss = 0.01594112\n",
      "Iteration 384, loss = 0.01590044\n",
      "Iteration 385, loss = 0.01585988\n",
      "Iteration 386, loss = 0.01590841\n",
      "Iteration 387, loss = 0.01574657\n",
      "Iteration 388, loss = 0.01574721\n",
      "Iteration 389, loss = 0.01570588\n",
      "Iteration 390, loss = 0.01563371\n",
      "Iteration 391, loss = 0.01568937\n",
      "Iteration 392, loss = 0.01559988\n",
      "Iteration 393, loss = 0.01571020\n",
      "Iteration 394, loss = 0.01560662\n",
      "Iteration 395, loss = 0.01552348\n",
      "Iteration 396, loss = 0.01544985\n",
      "Iteration 397, loss = 0.01536159\n",
      "Iteration 398, loss = 0.01540770\n",
      "Iteration 399, loss = 0.01531152\n",
      "Iteration 400, loss = 0.01529422\n",
      "Iteration 401, loss = 0.01519249\n",
      "Iteration 402, loss = 0.01519695\n",
      "Iteration 403, loss = 0.01513474\n",
      "Iteration 404, loss = 0.01527893\n",
      "Iteration 405, loss = 0.01509320\n",
      "Iteration 406, loss = 0.01506476\n",
      "Iteration 407, loss = 0.01501159\n",
      "Iteration 408, loss = 0.01493206\n",
      "Iteration 409, loss = 0.01498896\n",
      "Iteration 410, loss = 0.01485498\n",
      "Iteration 411, loss = 0.01486930\n",
      "Iteration 412, loss = 0.01480693\n",
      "Iteration 413, loss = 0.01478866\n",
      "Iteration 414, loss = 0.01477434\n",
      "Iteration 415, loss = 0.01472347\n",
      "Iteration 416, loss = 0.01477190\n",
      "Iteration 417, loss = 0.01465836\n",
      "Iteration 418, loss = 0.01461184\n",
      "Iteration 419, loss = 0.01452230\n",
      "Iteration 420, loss = 0.01454553\n",
      "Iteration 421, loss = 0.01450165\n",
      "Iteration 422, loss = 0.01448248\n",
      "Iteration 423, loss = 0.01445458\n",
      "Iteration 424, loss = 0.01439029\n",
      "Iteration 425, loss = 0.01441496\n",
      "Iteration 426, loss = 0.01432813\n",
      "Iteration 427, loss = 0.01423633\n",
      "Iteration 428, loss = 0.01427153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 429, loss = 0.01430367\n",
      "Iteration 430, loss = 0.01418918\n",
      "Iteration 431, loss = 0.01413849\n",
      "Iteration 432, loss = 0.01411552\n",
      "Iteration 433, loss = 0.01412830\n",
      "Iteration 434, loss = 0.01400492\n",
      "Iteration 435, loss = 0.01410439\n",
      "Iteration 436, loss = 0.01412202\n",
      "Iteration 437, loss = 0.01404731\n",
      "Iteration 438, loss = 0.01395046\n",
      "Iteration 439, loss = 0.01394304\n",
      "Iteration 440, loss = 0.01394801\n",
      "Iteration 441, loss = 0.01379776\n",
      "Iteration 442, loss = 0.01381925\n",
      "Iteration 443, loss = 0.01391087\n",
      "Iteration 444, loss = 0.01374281\n",
      "Iteration 445, loss = 0.01362538\n",
      "Iteration 446, loss = 0.01367840\n",
      "Iteration 447, loss = 0.01361567\n",
      "Iteration 448, loss = 0.01357828\n",
      "Iteration 449, loss = 0.01353956\n",
      "Iteration 450, loss = 0.01356160\n",
      "Iteration 451, loss = 0.01347564\n",
      "Iteration 452, loss = 0.01346577\n",
      "Iteration 453, loss = 0.01341536\n",
      "Iteration 454, loss = 0.01341055\n",
      "Iteration 455, loss = 0.01348907\n",
      "Iteration 456, loss = 0.01332775\n",
      "Iteration 457, loss = 0.01331289\n",
      "Iteration 458, loss = 0.01327883\n",
      "Iteration 459, loss = 0.01330913\n",
      "Iteration 460, loss = 0.01327090\n",
      "Iteration 461, loss = 0.01324893\n",
      "Iteration 462, loss = 0.01330250\n",
      "Iteration 463, loss = 0.01309369\n",
      "Iteration 464, loss = 0.01306488\n",
      "Iteration 465, loss = 0.01316317\n",
      "Iteration 466, loss = 0.01305397\n",
      "Iteration 467, loss = 0.01310825\n",
      "Iteration 468, loss = 0.01300472\n",
      "Iteration 469, loss = 0.01301803\n",
      "Iteration 470, loss = 0.01288180\n",
      "Iteration 471, loss = 0.01297180\n",
      "Iteration 472, loss = 0.01288085\n",
      "Iteration 473, loss = 0.01279454\n",
      "Iteration 474, loss = 0.01283050\n",
      "Iteration 475, loss = 0.01273341\n",
      "Iteration 476, loss = 0.01276117\n",
      "Iteration 477, loss = 0.01266381\n",
      "Iteration 478, loss = 0.01271937\n",
      "Iteration 479, loss = 0.01269389\n",
      "Iteration 480, loss = 0.01267415\n",
      "Iteration 481, loss = 0.01261681\n",
      "Iteration 482, loss = 0.01256633\n",
      "Iteration 483, loss = 0.01259291\n",
      "Iteration 484, loss = 0.01247392\n",
      "Iteration 485, loss = 0.01252419\n",
      "Iteration 486, loss = 0.01252746\n",
      "Iteration 487, loss = 0.01240546\n",
      "Iteration 488, loss = 0.01242365\n",
      "Iteration 489, loss = 0.01237019\n",
      "Iteration 490, loss = 0.01234627\n",
      "Iteration 491, loss = 0.01231283\n",
      "Iteration 492, loss = 0.01230766\n",
      "Iteration 493, loss = 0.01223081\n",
      "Iteration 494, loss = 0.01223290\n",
      "Iteration 495, loss = 0.01230314\n",
      "Iteration 496, loss = 0.01231783\n",
      "Iteration 497, loss = 0.01229102\n",
      "Iteration 498, loss = 0.01225294\n",
      "Iteration 499, loss = 0.01221374\n",
      "Iteration 500, loss = 0.01208431\n",
      "Iteration 501, loss = 0.01207089\n",
      "Iteration 502, loss = 0.01214712\n",
      "Iteration 503, loss = 0.01208488\n",
      "Iteration 504, loss = 0.01199812\n",
      "Iteration 505, loss = 0.01196467\n",
      "Iteration 506, loss = 0.01197558\n",
      "Iteration 507, loss = 0.01189551\n",
      "Iteration 508, loss = 0.01200837\n",
      "Iteration 509, loss = 0.01188638\n",
      "Iteration 510, loss = 0.01184215\n",
      "Iteration 511, loss = 0.01185474\n",
      "Iteration 512, loss = 0.01181250\n",
      "Iteration 513, loss = 0.01177720\n",
      "Iteration 514, loss = 0.01175741\n",
      "Iteration 515, loss = 0.01181672\n",
      "Iteration 516, loss = 0.01165648\n",
      "Iteration 517, loss = 0.01178661\n",
      "Iteration 518, loss = 0.01159091\n",
      "Iteration 519, loss = 0.01160854\n",
      "Iteration 520, loss = 0.01168754\n",
      "Iteration 521, loss = 0.01156699\n",
      "Iteration 522, loss = 0.01156259\n",
      "Iteration 523, loss = 0.01150921\n",
      "Iteration 524, loss = 0.01151503\n",
      "Iteration 525, loss = 0.01146748\n",
      "Iteration 526, loss = 0.01162798\n",
      "Iteration 527, loss = 0.01144020\n",
      "Iteration 528, loss = 0.01142497\n",
      "Iteration 529, loss = 0.01141304\n",
      "Iteration 530, loss = 0.01138935\n",
      "Iteration 531, loss = 0.01138664\n",
      "Iteration 532, loss = 0.01139958\n",
      "Iteration 533, loss = 0.01125890\n",
      "Iteration 534, loss = 0.01136898\n",
      "Iteration 535, loss = 0.01123471\n",
      "Iteration 536, loss = 0.01124915\n",
      "Iteration 537, loss = 0.01128229\n",
      "Iteration 538, loss = 0.01118816\n",
      "Iteration 539, loss = 0.01123820\n",
      "Iteration 540, loss = 0.01121037\n",
      "Iteration 541, loss = 0.01117952\n",
      "Iteration 542, loss = 0.01106542\n",
      "Iteration 543, loss = 0.01105151\n",
      "Iteration 544, loss = 0.01121616\n",
      "Iteration 545, loss = 0.01099641\n",
      "Iteration 546, loss = 0.01098395\n",
      "Iteration 547, loss = 0.01097428\n",
      "Iteration 548, loss = 0.01097743\n",
      "Iteration 549, loss = 0.01102176\n",
      "Iteration 550, loss = 0.01097814\n",
      "Iteration 551, loss = 0.01090382\n",
      "Iteration 552, loss = 0.01088838\n",
      "Iteration 553, loss = 0.01096050\n",
      "Iteration 554, loss = 0.01089008\n",
      "Iteration 555, loss = 0.01092253\n",
      "Iteration 556, loss = 0.01083017\n",
      "Iteration 557, loss = 0.01086527\n",
      "Iteration 558, loss = 0.01071987\n",
      "Iteration 559, loss = 0.01074974\n",
      "Iteration 560, loss = 0.01079967\n",
      "Iteration 561, loss = 0.01071251\n",
      "Iteration 562, loss = 0.01070100\n",
      "Iteration 563, loss = 0.01063961\n",
      "Iteration 564, loss = 0.01068365\n",
      "Iteration 565, loss = 0.01060804\n",
      "Iteration 566, loss = 0.01055861\n",
      "Iteration 567, loss = 0.01056303\n",
      "Iteration 568, loss = 0.01056214\n",
      "Iteration 569, loss = 0.01051619\n",
      "Iteration 570, loss = 0.01052517\n",
      "Iteration 571, loss = 0.01057451\n",
      "Iteration 572, loss = 0.01050148\n",
      "Iteration 573, loss = 0.01044180\n",
      "Iteration 574, loss = 0.01043112\n",
      "Iteration 575, loss = 0.01038377\n",
      "Iteration 576, loss = 0.01040528\n",
      "Iteration 577, loss = 0.01038192\n",
      "Iteration 578, loss = 0.01044663\n",
      "Iteration 579, loss = 0.01031752\n",
      "Iteration 580, loss = 0.01027636\n",
      "Iteration 581, loss = 0.01026455\n",
      "Iteration 582, loss = 0.01028540\n",
      "Iteration 583, loss = 0.01025904\n",
      "Iteration 584, loss = 0.01026108\n",
      "Iteration 585, loss = 0.01022874\n",
      "Iteration 586, loss = 0.01021610\n",
      "Iteration 587, loss = 0.01024206\n",
      "Iteration 588, loss = 0.01030200\n",
      "Iteration 589, loss = 0.01020087\n",
      "Iteration 590, loss = 0.01020213\n",
      "Iteration 591, loss = 0.01018739\n",
      "Iteration 592, loss = 0.01010702\n",
      "Iteration 593, loss = 0.01029069\n",
      "Iteration 594, loss = 0.01004242\n",
      "Iteration 595, loss = 0.01004744\n",
      "Iteration 596, loss = 0.00999481\n",
      "Iteration 597, loss = 0.01003726\n",
      "Iteration 598, loss = 0.01003994\n",
      "Iteration 599, loss = 0.01017000\n",
      "Iteration 600, loss = 0.00990434\n",
      "Iteration 601, loss = 0.00992510\n",
      "Iteration 602, loss = 0.00989712\n",
      "Iteration 603, loss = 0.00993524\n",
      "Iteration 604, loss = 0.00988527\n",
      "Iteration 605, loss = 0.00985937\n",
      "Iteration 606, loss = 0.00985445\n",
      "Iteration 607, loss = 0.00989071\n",
      "Iteration 608, loss = 0.00984776\n",
      "Iteration 609, loss = 0.00973458\n",
      "Iteration 610, loss = 0.00976400\n",
      "Iteration 611, loss = 0.00975356\n",
      "Iteration 612, loss = 0.00969435\n",
      "Iteration 613, loss = 0.00969477\n",
      "Iteration 614, loss = 0.00968935\n",
      "Iteration 615, loss = 0.00964198\n",
      "Iteration 616, loss = 0.00962377\n",
      "Iteration 617, loss = 0.00961215\n",
      "Iteration 618, loss = 0.00965210\n",
      "Iteration 619, loss = 0.00961935\n",
      "Iteration 620, loss = 0.00971937\n",
      "Iteration 621, loss = 0.00955297\n",
      "Iteration 622, loss = 0.00960144\n",
      "Iteration 623, loss = 0.00961805\n",
      "Iteration 624, loss = 0.00948062\n",
      "Iteration 625, loss = 0.00949413\n",
      "Iteration 626, loss = 0.00948562\n",
      "Iteration 627, loss = 0.00946860\n",
      "Iteration 628, loss = 0.00942019\n",
      "Iteration 629, loss = 0.00942729\n",
      "Iteration 630, loss = 0.00938127\n",
      "Iteration 631, loss = 0.00941678\n",
      "Iteration 632, loss = 0.00939916\n",
      "Iteration 633, loss = 0.00943816\n",
      "Iteration 634, loss = 0.00955632\n",
      "Iteration 635, loss = 0.00944694\n",
      "Iteration 636, loss = 0.00936323\n",
      "Iteration 637, loss = 0.00942010\n",
      "Iteration 638, loss = 0.00928692\n",
      "Iteration 639, loss = 0.00933014\n",
      "Iteration 640, loss = 0.00927918\n",
      "Iteration 641, loss = 0.00925781\n",
      "Iteration 642, loss = 0.00918076\n",
      "Iteration 643, loss = 0.00920152\n",
      "Iteration 644, loss = 0.00923817\n",
      "Iteration 645, loss = 0.00917603\n",
      "Iteration 646, loss = 0.00917679\n",
      "Iteration 647, loss = 0.00911816\n",
      "Iteration 648, loss = 0.00916408\n",
      "Iteration 649, loss = 0.00919426\n",
      "Iteration 650, loss = 0.00915960\n",
      "Iteration 651, loss = 0.00911510\n",
      "Iteration 652, loss = 0.00904071\n",
      "Iteration 653, loss = 0.00902763\n",
      "Iteration 654, loss = 0.00900904\n",
      "Iteration 655, loss = 0.00900019\n",
      "Iteration 656, loss = 0.00901732\n",
      "Iteration 657, loss = 0.00892052\n",
      "Iteration 658, loss = 0.00891909\n",
      "Iteration 659, loss = 0.00894114\n",
      "Iteration 660, loss = 0.00893301\n",
      "Iteration 661, loss = 0.00892019\n",
      "Iteration 662, loss = 0.00887732\n",
      "Iteration 663, loss = 0.00897582\n",
      "Iteration 664, loss = 0.00887527\n",
      "Iteration 665, loss = 0.00890669\n",
      "Iteration 666, loss = 0.00882299\n",
      "Iteration 667, loss = 0.00881600\n",
      "Iteration 668, loss = 0.00883974\n",
      "Iteration 669, loss = 0.00886113\n",
      "Iteration 670, loss = 0.00883873\n",
      "Iteration 671, loss = 0.00885839\n",
      "Iteration 672, loss = 0.00878686\n",
      "Iteration 673, loss = 0.00873925\n",
      "Iteration 674, loss = 0.00873820\n",
      "Iteration 675, loss = 0.00866978\n",
      "Iteration 676, loss = 0.00871875\n",
      "Iteration 677, loss = 0.00866088\n",
      "Iteration 678, loss = 0.00870026\n",
      "Iteration 679, loss = 0.00862347\n",
      "Iteration 680, loss = 0.00867038\n",
      "Iteration 681, loss = 0.00861629\n",
      "Iteration 682, loss = 0.00857100\n",
      "Iteration 683, loss = 0.00880263\n",
      "Iteration 684, loss = 0.00856247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 685, loss = 0.00858222\n",
      "Iteration 686, loss = 0.00856051\n",
      "Iteration 687, loss = 0.00854354\n",
      "Iteration 688, loss = 0.00853884\n",
      "Iteration 689, loss = 0.00851077\n",
      "Iteration 690, loss = 0.00860156\n",
      "Iteration 691, loss = 0.00845200\n",
      "Iteration 692, loss = 0.00852460\n",
      "Iteration 693, loss = 0.00864062\n",
      "Iteration 694, loss = 0.00845354\n",
      "Iteration 695, loss = 0.00850072\n",
      "Iteration 696, loss = 0.00841848\n",
      "Iteration 697, loss = 0.00850704\n",
      "Iteration 698, loss = 0.00836046\n",
      "Iteration 699, loss = 0.00837502\n",
      "Iteration 700, loss = 0.00834020\n",
      "Iteration 701, loss = 0.00835707\n",
      "Iteration 702, loss = 0.00834242\n",
      "Iteration 703, loss = 0.00831030\n",
      "Iteration 704, loss = 0.00828222\n",
      "Iteration 705, loss = 0.00837805\n",
      "Iteration 706, loss = 0.00831208\n",
      "Iteration 707, loss = 0.00834397\n",
      "Iteration 708, loss = 0.00824057\n",
      "Iteration 709, loss = 0.00825552\n",
      "Iteration 710, loss = 0.00836586\n",
      "Iteration 711, loss = 0.00822296\n",
      "Iteration 712, loss = 0.00822620\n",
      "Iteration 713, loss = 0.00818476\n",
      "Iteration 714, loss = 0.00818038\n",
      "Iteration 715, loss = 0.00818547\n",
      "Iteration 716, loss = 0.00811376\n",
      "Iteration 717, loss = 0.00818470\n",
      "Iteration 718, loss = 0.00827699\n",
      "Iteration 719, loss = 0.00813676\n",
      "Iteration 720, loss = 0.00811395\n",
      "Iteration 721, loss = 0.00813188\n",
      "Iteration 722, loss = 0.00807549\n",
      "Iteration 723, loss = 0.00805498\n",
      "Iteration 724, loss = 0.00824369\n",
      "Iteration 725, loss = 0.00806871\n",
      "Iteration 726, loss = 0.00806046\n",
      "Iteration 727, loss = 0.00805029\n",
      "Iteration 728, loss = 0.00800161\n",
      "Iteration 729, loss = 0.00793733\n",
      "Iteration 730, loss = 0.00797850\n",
      "Iteration 731, loss = 0.00792858\n",
      "Iteration 732, loss = 0.00791886\n",
      "Iteration 733, loss = 0.00799132\n",
      "Iteration 734, loss = 0.00793489\n",
      "Iteration 735, loss = 0.00817232\n",
      "Iteration 736, loss = 0.00791621\n",
      "Iteration 737, loss = 0.00789206\n",
      "Iteration 738, loss = 0.00783740\n",
      "Iteration 739, loss = 0.00784980\n",
      "Iteration 740, loss = 0.00788569\n",
      "Iteration 741, loss = 0.00790002\n",
      "Iteration 742, loss = 0.00794805\n",
      "Iteration 743, loss = 0.00784902\n",
      "Iteration 744, loss = 0.00778410\n",
      "Iteration 745, loss = 0.00788574\n",
      "Iteration 746, loss = 0.00774799\n",
      "Iteration 747, loss = 0.00772351\n",
      "Iteration 748, loss = 0.00775636\n",
      "Iteration 749, loss = 0.00771320\n",
      "Iteration 750, loss = 0.00773836\n",
      "Iteration 751, loss = 0.00770459\n",
      "Iteration 752, loss = 0.00763506\n",
      "Iteration 753, loss = 0.00765981\n",
      "Iteration 754, loss = 0.00763307\n",
      "Iteration 755, loss = 0.00769534\n",
      "Iteration 756, loss = 0.00761371\n",
      "Iteration 757, loss = 0.00768683\n",
      "Iteration 758, loss = 0.00757581\n",
      "Iteration 759, loss = 0.00759621\n",
      "Iteration 760, loss = 0.00763199\n",
      "Iteration 761, loss = 0.00764104\n",
      "Iteration 762, loss = 0.00776730\n",
      "Iteration 763, loss = 0.00762900\n",
      "Iteration 764, loss = 0.00762513\n",
      "Iteration 765, loss = 0.00756622\n",
      "Iteration 766, loss = 0.00797584\n",
      "Iteration 767, loss = 0.00748447\n",
      "Iteration 768, loss = 0.00747188\n",
      "Iteration 769, loss = 0.00748953\n",
      "Iteration 770, loss = 0.00752380\n",
      "Iteration 771, loss = 0.00745232\n",
      "Iteration 772, loss = 0.00747688\n",
      "Iteration 773, loss = 0.00747957\n",
      "Iteration 774, loss = 0.00745927\n",
      "Iteration 775, loss = 0.00739836\n",
      "Iteration 776, loss = 0.00743801\n",
      "Iteration 777, loss = 0.00741002\n",
      "Iteration 778, loss = 0.00750181\n",
      "Iteration 779, loss = 0.00735789\n",
      "Iteration 780, loss = 0.00736151\n",
      "Iteration 781, loss = 0.00740444\n",
      "Iteration 782, loss = 0.00734908\n",
      "Iteration 783, loss = 0.00731614\n",
      "Iteration 784, loss = 0.00732197\n",
      "Iteration 785, loss = 0.00733558\n",
      "Iteration 786, loss = 0.00730592\n",
      "Iteration 787, loss = 0.00725241\n",
      "Iteration 788, loss = 0.00733894\n",
      "Iteration 789, loss = 0.00725815\n",
      "Iteration 790, loss = 0.00739931\n",
      "Iteration 791, loss = 0.00727228\n",
      "Iteration 792, loss = 0.00724336\n",
      "Iteration 793, loss = 0.00721777\n",
      "Iteration 794, loss = 0.00735955\n",
      "Iteration 795, loss = 0.00718743\n",
      "Iteration 796, loss = 0.00719271\n",
      "Iteration 797, loss = 0.00720773\n",
      "Iteration 798, loss = 0.00716653\n",
      "Iteration 799, loss = 0.00721817\n",
      "Iteration 800, loss = 0.00713220\n",
      "Iteration 801, loss = 0.00714408\n",
      "Iteration 802, loss = 0.00712621\n",
      "Iteration 803, loss = 0.00706203\n",
      "Iteration 804, loss = 0.00709278\n",
      "Iteration 805, loss = 0.00716072\n",
      "Iteration 806, loss = 0.00707772\n",
      "Iteration 807, loss = 0.00703449\n",
      "Iteration 808, loss = 0.00707491\n",
      "Iteration 809, loss = 0.00706636\n",
      "Iteration 810, loss = 0.00708808\n",
      "Iteration 811, loss = 0.00702450\n",
      "Iteration 812, loss = 0.00700205\n",
      "Iteration 813, loss = 0.00703213\n",
      "Iteration 814, loss = 0.00706139\n",
      "Iteration 815, loss = 0.00700454\n",
      "Iteration 816, loss = 0.00701208\n",
      "Iteration 817, loss = 0.00696910\n",
      "Iteration 818, loss = 0.00700558\n",
      "Iteration 819, loss = 0.00700282\n",
      "Iteration 820, loss = 0.00698130\n",
      "Iteration 821, loss = 0.00693602\n",
      "Iteration 822, loss = 0.00703065\n",
      "Iteration 823, loss = 0.00704232\n",
      "Iteration 824, loss = 0.00687061\n",
      "Iteration 825, loss = 0.00697394\n",
      "Iteration 826, loss = 0.00691082\n",
      "Iteration 827, loss = 0.00693181\n",
      "Iteration 828, loss = 0.00688176\n",
      "Iteration 829, loss = 0.00681682\n",
      "Iteration 830, loss = 0.00688255\n",
      "Iteration 831, loss = 0.00685966\n",
      "Iteration 832, loss = 0.00686455\n",
      "Iteration 833, loss = 0.00681245\n",
      "Iteration 834, loss = 0.00687511\n",
      "Iteration 835, loss = 0.00677016\n",
      "Iteration 836, loss = 0.00684856\n",
      "Iteration 837, loss = 0.00706112\n",
      "Iteration 838, loss = 0.00679932\n",
      "Iteration 839, loss = 0.00694713\n",
      "Iteration 840, loss = 0.00684467\n",
      "Iteration 841, loss = 0.00674119\n",
      "Iteration 842, loss = 0.00685702\n",
      "Iteration 843, loss = 0.00684160\n",
      "Iteration 844, loss = 0.00685378\n",
      "Iteration 845, loss = 0.00676902\n",
      "Iteration 846, loss = 0.00670999\n",
      "Iteration 847, loss = 0.00669415\n",
      "Iteration 848, loss = 0.00665060\n",
      "Iteration 849, loss = 0.00665215\n",
      "Iteration 850, loss = 0.00672354\n",
      "Iteration 851, loss = 0.00674062\n",
      "Iteration 852, loss = 0.00663519\n",
      "Iteration 853, loss = 0.00672296\n",
      "Iteration 854, loss = 0.00658653\n",
      "Iteration 855, loss = 0.00666714\n",
      "Iteration 856, loss = 0.00659809\n",
      "Iteration 857, loss = 0.00672097\n",
      "Iteration 858, loss = 0.00677363\n",
      "Iteration 859, loss = 0.00665444\n",
      "Iteration 860, loss = 0.00666907\n",
      "Iteration 861, loss = 0.00659811\n",
      "Iteration 862, loss = 0.00662537\n",
      "Iteration 863, loss = 0.00657473\n",
      "Iteration 864, loss = 0.00658779\n",
      "Iteration 865, loss = 0.00651383\n",
      "Iteration 866, loss = 0.00651801\n",
      "Iteration 867, loss = 0.00654272\n",
      "Iteration 868, loss = 0.00648747\n",
      "Iteration 869, loss = 0.00650564\n",
      "Iteration 870, loss = 0.00645604\n",
      "Iteration 871, loss = 0.00658809\n",
      "Iteration 872, loss = 0.00651039\n",
      "Iteration 873, loss = 0.00645308\n",
      "Iteration 874, loss = 0.00647473\n",
      "Iteration 875, loss = 0.00645735\n",
      "Iteration 876, loss = 0.00646516\n",
      "Iteration 877, loss = 0.00646800\n",
      "Iteration 878, loss = 0.00638375\n",
      "Iteration 879, loss = 0.00651747\n",
      "Iteration 880, loss = 0.00643838\n",
      "Iteration 881, loss = 0.00637906\n",
      "Iteration 882, loss = 0.00637998\n",
      "Iteration 883, loss = 0.00638923\n",
      "Iteration 884, loss = 0.00635562\n",
      "Iteration 885, loss = 0.00638637\n",
      "Iteration 886, loss = 0.00647465\n",
      "Iteration 887, loss = 0.00633122\n",
      "Iteration 888, loss = 0.00640160\n",
      "Iteration 889, loss = 0.00639027\n",
      "Iteration 890, loss = 0.00641495\n",
      "Iteration 891, loss = 0.00631533\n",
      "Iteration 892, loss = 0.00640353\n",
      "Iteration 893, loss = 0.00638275\n",
      "Iteration 894, loss = 0.00648766\n",
      "Iteration 895, loss = 0.00647783\n",
      "Iteration 896, loss = 0.00633475\n",
      "Iteration 897, loss = 0.00622225\n",
      "Iteration 898, loss = 0.00625461\n",
      "Iteration 899, loss = 0.00627883\n",
      "Iteration 900, loss = 0.00625095\n",
      "Iteration 901, loss = 0.00623568\n",
      "Iteration 902, loss = 0.00622370\n",
      "Iteration 903, loss = 0.00620932\n",
      "Iteration 904, loss = 0.00623088\n",
      "Iteration 905, loss = 0.00617272\n",
      "Iteration 906, loss = 0.00624564\n",
      "Iteration 907, loss = 0.00621525\n",
      "Iteration 908, loss = 0.00616120\n",
      "Iteration 909, loss = 0.00621052\n",
      "Iteration 910, loss = 0.00618390\n",
      "Iteration 911, loss = 0.00622673\n",
      "Iteration 912, loss = 0.00619914\n",
      "Iteration 913, loss = 0.00611875\n",
      "Iteration 914, loss = 0.00615979\n",
      "Iteration 915, loss = 0.00609957\n",
      "Iteration 916, loss = 0.00619402\n",
      "Iteration 917, loss = 0.00612500\n",
      "Iteration 918, loss = 0.00608029\n",
      "Iteration 919, loss = 0.00607912\n",
      "Iteration 920, loss = 0.00609346\n",
      "Iteration 921, loss = 0.00613562\n",
      "Iteration 922, loss = 0.00603900\n",
      "Iteration 923, loss = 0.00608303\n",
      "Iteration 924, loss = 0.00607853\n",
      "Iteration 925, loss = 0.00603268\n",
      "Iteration 926, loss = 0.00598872\n",
      "Iteration 927, loss = 0.00609248\n",
      "Iteration 928, loss = 0.00613229\n",
      "Iteration 929, loss = 0.00602945\n",
      "Iteration 930, loss = 0.00613578\n",
      "Iteration 931, loss = 0.00602489\n",
      "Iteration 932, loss = 0.00599435\n",
      "Iteration 933, loss = 0.00595636\n",
      "Iteration 934, loss = 0.00600643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 935, loss = 0.00598293\n",
      "Iteration 936, loss = 0.00599842\n",
      "Iteration 937, loss = 0.00610236\n",
      "Iteration 938, loss = 0.00607159\n",
      "Iteration 939, loss = 0.00594515\n",
      "Iteration 940, loss = 0.00597160\n",
      "Iteration 941, loss = 0.00592811\n",
      "Iteration 942, loss = 0.00591614\n",
      "Iteration 943, loss = 0.00592951\n",
      "Iteration 944, loss = 0.00597638\n",
      "Iteration 945, loss = 0.00591384\n",
      "Iteration 946, loss = 0.00597338\n",
      "Iteration 947, loss = 0.00585076\n",
      "Iteration 948, loss = 0.00584668\n",
      "Iteration 949, loss = 0.00588260\n",
      "Iteration 950, loss = 0.00589553\n",
      "Iteration 951, loss = 0.00584352\n",
      "Iteration 952, loss = 0.00584449\n",
      "Iteration 953, loss = 0.00581307\n",
      "Iteration 954, loss = 0.00597531\n",
      "Iteration 955, loss = 0.00582210\n",
      "Iteration 956, loss = 0.00594134\n",
      "Iteration 957, loss = 0.00577891\n",
      "Iteration 958, loss = 0.00584480\n",
      "Iteration 959, loss = 0.00580021\n",
      "Iteration 960, loss = 0.00583373\n",
      "Iteration 961, loss = 0.00580713\n",
      "Iteration 962, loss = 0.00579781\n",
      "Iteration 963, loss = 0.00573764\n",
      "Iteration 964, loss = 0.00576288\n",
      "Iteration 965, loss = 0.00574235\n",
      "Iteration 966, loss = 0.00579240\n",
      "Iteration 967, loss = 0.00573167\n",
      "Iteration 968, loss = 0.00580404\n",
      "Iteration 969, loss = 0.00573354\n",
      "Iteration 970, loss = 0.00573864\n",
      "Iteration 971, loss = 0.00570421\n",
      "Iteration 972, loss = 0.00576172\n",
      "Iteration 973, loss = 0.00572310\n",
      "Iteration 974, loss = 0.00568525\n",
      "Iteration 975, loss = 0.00565819\n",
      "Iteration 976, loss = 0.00567688\n",
      "Iteration 977, loss = 0.00573138\n",
      "Iteration 978, loss = 0.00563777\n",
      "Iteration 979, loss = 0.00568361\n",
      "Iteration 980, loss = 0.00578514\n",
      "Iteration 981, loss = 0.00565225\n",
      "Iteration 982, loss = 0.00563453\n",
      "Iteration 983, loss = 0.00568468\n",
      "Iteration 984, loss = 0.00564786\n",
      "Iteration 985, loss = 0.00566312\n",
      "Iteration 986, loss = 0.00563385\n",
      "Iteration 987, loss = 0.00580267\n",
      "Iteration 988, loss = 0.00575117\n",
      "Iteration 989, loss = 0.00564205\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70934424\n",
      "Iteration 2, loss = 0.63658474\n",
      "Iteration 3, loss = 0.57485174\n",
      "Iteration 4, loss = 0.52109077\n",
      "Iteration 5, loss = 0.47493154\n",
      "Iteration 6, loss = 0.43522767\n",
      "Iteration 7, loss = 0.40053373\n",
      "Iteration 8, loss = 0.37001641\n",
      "Iteration 9, loss = 0.34268104\n",
      "Iteration 10, loss = 0.31856606\n",
      "Iteration 11, loss = 0.29675647\n",
      "Iteration 12, loss = 0.27748025\n",
      "Iteration 13, loss = 0.26034467\n",
      "Iteration 14, loss = 0.24499732\n",
      "Iteration 15, loss = 0.23120199\n",
      "Iteration 16, loss = 0.21873316\n",
      "Iteration 17, loss = 0.20750340\n",
      "Iteration 18, loss = 0.19747775\n",
      "Iteration 19, loss = 0.18833864\n",
      "Iteration 20, loss = 0.17987706\n",
      "Iteration 21, loss = 0.17225803\n",
      "Iteration 22, loss = 0.16531440\n",
      "Iteration 23, loss = 0.15892421\n",
      "Iteration 24, loss = 0.15311945\n",
      "Iteration 25, loss = 0.14771541\n",
      "Iteration 26, loss = 0.14273245\n",
      "Iteration 27, loss = 0.13818486\n",
      "Iteration 28, loss = 0.13389380\n",
      "Iteration 29, loss = 0.12988673\n",
      "Iteration 30, loss = 0.12613661\n",
      "Iteration 31, loss = 0.12270474\n",
      "Iteration 32, loss = 0.11941785\n",
      "Iteration 33, loss = 0.11641730\n",
      "Iteration 34, loss = 0.11356441\n",
      "Iteration 35, loss = 0.11091435\n",
      "Iteration 36, loss = 0.10839465\n",
      "Iteration 37, loss = 0.10599780\n",
      "Iteration 38, loss = 0.10375712\n",
      "Iteration 39, loss = 0.10162637\n",
      "Iteration 40, loss = 0.09960546\n",
      "Iteration 41, loss = 0.09768748\n",
      "Iteration 42, loss = 0.09583996\n",
      "Iteration 43, loss = 0.09418863\n",
      "Iteration 44, loss = 0.09251024\n",
      "Iteration 45, loss = 0.09091037\n",
      "Iteration 46, loss = 0.08942017\n",
      "Iteration 47, loss = 0.08812073\n",
      "Iteration 48, loss = 0.08663825\n",
      "Iteration 49, loss = 0.08518583\n",
      "Iteration 50, loss = 0.08393432\n",
      "Iteration 51, loss = 0.08278498\n",
      "Iteration 52, loss = 0.08148440\n",
      "Iteration 53, loss = 0.08038173\n",
      "Iteration 54, loss = 0.07917101\n",
      "Iteration 55, loss = 0.07809501\n",
      "Iteration 56, loss = 0.07710068\n",
      "Iteration 57, loss = 0.07599689\n",
      "Iteration 58, loss = 0.07505368\n",
      "Iteration 59, loss = 0.07413467\n",
      "Iteration 60, loss = 0.07317299\n",
      "Iteration 61, loss = 0.07223779\n",
      "Iteration 62, loss = 0.07138657\n",
      "Iteration 63, loss = 0.07052752\n",
      "Iteration 64, loss = 0.06970343\n",
      "Iteration 65, loss = 0.06889715\n",
      "Iteration 66, loss = 0.06807869\n",
      "Iteration 67, loss = 0.06721436\n",
      "Iteration 68, loss = 0.06649422\n",
      "Iteration 69, loss = 0.06574453\n",
      "Iteration 70, loss = 0.06497807\n",
      "Iteration 71, loss = 0.06426651\n",
      "Iteration 72, loss = 0.06358722\n",
      "Iteration 73, loss = 0.06285651\n",
      "Iteration 74, loss = 0.06220705\n",
      "Iteration 75, loss = 0.06149739\n",
      "Iteration 76, loss = 0.06091360\n",
      "Iteration 77, loss = 0.06019987\n",
      "Iteration 78, loss = 0.05958679\n",
      "Iteration 79, loss = 0.05903017\n",
      "Iteration 80, loss = 0.05837820\n",
      "Iteration 81, loss = 0.05781970\n",
      "Iteration 82, loss = 0.05721192\n",
      "Iteration 83, loss = 0.05667251\n",
      "Iteration 84, loss = 0.05613837\n",
      "Iteration 85, loss = 0.05556316\n",
      "Iteration 86, loss = 0.05506552\n",
      "Iteration 87, loss = 0.05447929\n",
      "Iteration 88, loss = 0.05402473\n",
      "Iteration 89, loss = 0.05355460\n",
      "Iteration 90, loss = 0.05293090\n",
      "Iteration 91, loss = 0.05244714\n",
      "Iteration 92, loss = 0.05198745\n",
      "Iteration 93, loss = 0.05152612\n",
      "Iteration 94, loss = 0.05109411\n",
      "Iteration 95, loss = 0.05052575\n",
      "Iteration 96, loss = 0.05014210\n",
      "Iteration 97, loss = 0.04969159\n",
      "Iteration 98, loss = 0.04921387\n",
      "Iteration 99, loss = 0.04883418\n",
      "Iteration 100, loss = 0.04852866\n",
      "Iteration 101, loss = 0.04794598\n",
      "Iteration 102, loss = 0.04755964\n",
      "Iteration 103, loss = 0.04715775\n",
      "Iteration 104, loss = 0.04677451\n",
      "Iteration 105, loss = 0.04651713\n",
      "Iteration 106, loss = 0.04598118\n",
      "Iteration 107, loss = 0.04561336\n",
      "Iteration 108, loss = 0.04521817\n",
      "Iteration 109, loss = 0.04485655\n",
      "Iteration 110, loss = 0.04447274\n",
      "Iteration 111, loss = 0.04412463\n",
      "Iteration 112, loss = 0.04380320\n",
      "Iteration 113, loss = 0.04344695\n",
      "Iteration 114, loss = 0.04303729\n",
      "Iteration 115, loss = 0.04272957\n",
      "Iteration 116, loss = 0.04242144\n",
      "Iteration 117, loss = 0.04209719\n",
      "Iteration 118, loss = 0.04171036\n",
      "Iteration 119, loss = 0.04140104\n",
      "Iteration 120, loss = 0.04111100\n",
      "Iteration 121, loss = 0.04083393\n",
      "Iteration 122, loss = 0.04051758\n",
      "Iteration 123, loss = 0.04023734\n",
      "Iteration 124, loss = 0.03993008\n",
      "Iteration 125, loss = 0.03969636\n",
      "Iteration 126, loss = 0.03939439\n",
      "Iteration 127, loss = 0.03908851\n",
      "Iteration 128, loss = 0.03881619\n",
      "Iteration 129, loss = 0.03850456\n",
      "Iteration 130, loss = 0.03832593\n",
      "Iteration 131, loss = 0.03799465\n",
      "Iteration 132, loss = 0.03773306\n",
      "Iteration 133, loss = 0.03748380\n",
      "Iteration 134, loss = 0.03719583\n",
      "Iteration 135, loss = 0.03709512\n",
      "Iteration 136, loss = 0.03672311\n",
      "Iteration 137, loss = 0.03650321\n",
      "Iteration 138, loss = 0.03619737\n",
      "Iteration 139, loss = 0.03602385\n",
      "Iteration 140, loss = 0.03581233\n",
      "Iteration 141, loss = 0.03557997\n",
      "Iteration 142, loss = 0.03532940\n",
      "Iteration 143, loss = 0.03506204\n",
      "Iteration 144, loss = 0.03488323\n",
      "Iteration 145, loss = 0.03465661\n",
      "Iteration 146, loss = 0.03439728\n",
      "Iteration 147, loss = 0.03415437\n",
      "Iteration 148, loss = 0.03401723\n",
      "Iteration 149, loss = 0.03372987\n",
      "Iteration 150, loss = 0.03356137\n",
      "Iteration 151, loss = 0.03340112\n",
      "Iteration 152, loss = 0.03314888\n",
      "Iteration 153, loss = 0.03299578\n",
      "Iteration 154, loss = 0.03270679\n",
      "Iteration 155, loss = 0.03254041\n",
      "Iteration 156, loss = 0.03236772\n",
      "Iteration 157, loss = 0.03213517\n",
      "Iteration 158, loss = 0.03200679\n",
      "Iteration 159, loss = 0.03178602\n",
      "Iteration 160, loss = 0.03158439\n",
      "Iteration 161, loss = 0.03140900\n",
      "Iteration 162, loss = 0.03124024\n",
      "Iteration 163, loss = 0.03102640\n",
      "Iteration 164, loss = 0.03095988\n",
      "Iteration 165, loss = 0.03065064\n",
      "Iteration 166, loss = 0.03052400\n",
      "Iteration 167, loss = 0.03038976\n",
      "Iteration 168, loss = 0.03018882\n",
      "Iteration 169, loss = 0.02997998\n",
      "Iteration 170, loss = 0.02985301\n",
      "Iteration 171, loss = 0.02966233\n",
      "Iteration 172, loss = 0.02949095\n",
      "Iteration 173, loss = 0.02938459\n",
      "Iteration 174, loss = 0.02920541\n",
      "Iteration 175, loss = 0.02917393\n",
      "Iteration 176, loss = 0.02895610\n",
      "Iteration 177, loss = 0.02879676\n",
      "Iteration 178, loss = 0.02861528\n",
      "Iteration 179, loss = 0.02845811\n",
      "Iteration 180, loss = 0.02830104\n",
      "Iteration 181, loss = 0.02811713\n",
      "Iteration 182, loss = 0.02801234\n",
      "Iteration 183, loss = 0.02790467\n",
      "Iteration 184, loss = 0.02771158\n",
      "Iteration 185, loss = 0.02758278\n",
      "Iteration 186, loss = 0.02744928\n",
      "Iteration 187, loss = 0.02737809\n",
      "Iteration 188, loss = 0.02719287\n",
      "Iteration 189, loss = 0.02705615\n",
      "Iteration 190, loss = 0.02692292\n",
      "Iteration 191, loss = 0.02676083\n",
      "Iteration 192, loss = 0.02664143\n",
      "Iteration 193, loss = 0.02646175\n",
      "Iteration 194, loss = 0.02636551\n",
      "Iteration 195, loss = 0.02627737\n",
      "Iteration 196, loss = 0.02611514\n",
      "Iteration 197, loss = 0.02602161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 198, loss = 0.02583054\n",
      "Iteration 199, loss = 0.02573679\n",
      "Iteration 200, loss = 0.02560628\n",
      "Iteration 201, loss = 0.02557276\n",
      "Iteration 202, loss = 0.02557248\n",
      "Iteration 203, loss = 0.02526657\n",
      "Iteration 204, loss = 0.02514809\n",
      "Iteration 205, loss = 0.02512171\n",
      "Iteration 206, loss = 0.02490205\n",
      "Iteration 207, loss = 0.02479575\n",
      "Iteration 208, loss = 0.02478633\n",
      "Iteration 209, loss = 0.02454673\n",
      "Iteration 210, loss = 0.02443717\n",
      "Iteration 211, loss = 0.02445431\n",
      "Iteration 212, loss = 0.02421687\n",
      "Iteration 213, loss = 0.02413568\n",
      "Iteration 214, loss = 0.02409156\n",
      "Iteration 215, loss = 0.02391750\n",
      "Iteration 216, loss = 0.02380747\n",
      "Iteration 217, loss = 0.02377556\n",
      "Iteration 218, loss = 0.02360226\n",
      "Iteration 219, loss = 0.02348175\n",
      "Iteration 220, loss = 0.02336143\n",
      "Iteration 221, loss = 0.02332477\n",
      "Iteration 222, loss = 0.02327217\n",
      "Iteration 223, loss = 0.02306499\n",
      "Iteration 224, loss = 0.02300327\n",
      "Iteration 225, loss = 0.02286756\n",
      "Iteration 226, loss = 0.02277646\n",
      "Iteration 227, loss = 0.02266613\n",
      "Iteration 228, loss = 0.02262075\n",
      "Iteration 229, loss = 0.02253975\n",
      "Iteration 230, loss = 0.02246753\n",
      "Iteration 231, loss = 0.02230503\n",
      "Iteration 232, loss = 0.02224655\n",
      "Iteration 233, loss = 0.02212206\n",
      "Iteration 234, loss = 0.02201722\n",
      "Iteration 235, loss = 0.02196348\n",
      "Iteration 236, loss = 0.02185246\n",
      "Iteration 237, loss = 0.02181841\n",
      "Iteration 238, loss = 0.02168974\n",
      "Iteration 239, loss = 0.02154309\n",
      "Iteration 240, loss = 0.02153803\n",
      "Iteration 241, loss = 0.02143002\n",
      "Iteration 242, loss = 0.02135446\n",
      "Iteration 243, loss = 0.02122709\n",
      "Iteration 244, loss = 0.02112550\n",
      "Iteration 245, loss = 0.02105643\n",
      "Iteration 246, loss = 0.02100280\n",
      "Iteration 247, loss = 0.02088582\n",
      "Iteration 248, loss = 0.02084787\n",
      "Iteration 249, loss = 0.02077097\n",
      "Iteration 250, loss = 0.02066074\n",
      "Iteration 251, loss = 0.02059789\n",
      "Iteration 252, loss = 0.02051842\n",
      "Iteration 253, loss = 0.02051743\n",
      "Iteration 254, loss = 0.02037310\n",
      "Iteration 255, loss = 0.02025879\n",
      "Iteration 256, loss = 0.02019382\n",
      "Iteration 257, loss = 0.02010867\n",
      "Iteration 258, loss = 0.01999986\n",
      "Iteration 259, loss = 0.01992603\n",
      "Iteration 260, loss = 0.01997965\n",
      "Iteration 261, loss = 0.01975479\n",
      "Iteration 262, loss = 0.01979894\n",
      "Iteration 263, loss = 0.01964163\n",
      "Iteration 264, loss = 0.01964749\n",
      "Iteration 265, loss = 0.01949353\n",
      "Iteration 266, loss = 0.01943106\n",
      "Iteration 267, loss = 0.01937021\n",
      "Iteration 268, loss = 0.01926682\n",
      "Iteration 269, loss = 0.01922558\n",
      "Iteration 270, loss = 0.01925775\n",
      "Iteration 271, loss = 0.01908812\n",
      "Iteration 272, loss = 0.01908364\n",
      "Iteration 273, loss = 0.01896892\n",
      "Iteration 274, loss = 0.01887991\n",
      "Iteration 275, loss = 0.01883256\n",
      "Iteration 276, loss = 0.01869856\n",
      "Iteration 277, loss = 0.01862763\n",
      "Iteration 278, loss = 0.01854238\n",
      "Iteration 279, loss = 0.01850746\n",
      "Iteration 280, loss = 0.01839638\n",
      "Iteration 281, loss = 0.01834061\n",
      "Iteration 282, loss = 0.01831631\n",
      "Iteration 283, loss = 0.01825148\n",
      "Iteration 284, loss = 0.01815459\n",
      "Iteration 285, loss = 0.01813091\n",
      "Iteration 286, loss = 0.01804577\n",
      "Iteration 287, loss = 0.01807684\n",
      "Iteration 288, loss = 0.01788595\n",
      "Iteration 289, loss = 0.01792024\n",
      "Iteration 290, loss = 0.01784591\n",
      "Iteration 291, loss = 0.01770011\n",
      "Iteration 292, loss = 0.01770935\n",
      "Iteration 293, loss = 0.01757918\n",
      "Iteration 294, loss = 0.01749007\n",
      "Iteration 295, loss = 0.01744657\n",
      "Iteration 296, loss = 0.01745498\n",
      "Iteration 297, loss = 0.01740074\n",
      "Iteration 298, loss = 0.01735057\n",
      "Iteration 299, loss = 0.01719657\n",
      "Iteration 300, loss = 0.01722052\n",
      "Iteration 301, loss = 0.01707729\n",
      "Iteration 302, loss = 0.01710078\n",
      "Iteration 303, loss = 0.01695566\n",
      "Iteration 304, loss = 0.01691858\n",
      "Iteration 305, loss = 0.01689629\n",
      "Iteration 306, loss = 0.01680124\n",
      "Iteration 307, loss = 0.01672929\n",
      "Iteration 308, loss = 0.01673177\n",
      "Iteration 309, loss = 0.01664835\n",
      "Iteration 310, loss = 0.01658037\n",
      "Iteration 311, loss = 0.01658897\n",
      "Iteration 312, loss = 0.01644372\n",
      "Iteration 313, loss = 0.01641073\n",
      "Iteration 314, loss = 0.01635418\n",
      "Iteration 315, loss = 0.01632801\n",
      "Iteration 316, loss = 0.01622628\n",
      "Iteration 317, loss = 0.01618253\n",
      "Iteration 318, loss = 0.01612346\n",
      "Iteration 319, loss = 0.01610712\n",
      "Iteration 320, loss = 0.01612433\n",
      "Iteration 321, loss = 0.01597064\n",
      "Iteration 322, loss = 0.01602944\n",
      "Iteration 323, loss = 0.01596548\n",
      "Iteration 324, loss = 0.01583045\n",
      "Iteration 325, loss = 0.01581128\n",
      "Iteration 326, loss = 0.01572069\n",
      "Iteration 327, loss = 0.01565330\n",
      "Iteration 328, loss = 0.01559370\n",
      "Iteration 329, loss = 0.01559259\n",
      "Iteration 330, loss = 0.01549284\n",
      "Iteration 331, loss = 0.01544816\n",
      "Iteration 332, loss = 0.01547967\n",
      "Iteration 333, loss = 0.01535666\n",
      "Iteration 334, loss = 0.01537166\n",
      "Iteration 335, loss = 0.01530184\n",
      "Iteration 336, loss = 0.01525948\n",
      "Iteration 337, loss = 0.01517190\n",
      "Iteration 338, loss = 0.01514797\n",
      "Iteration 339, loss = 0.01509374\n",
      "Iteration 340, loss = 0.01511610\n",
      "Iteration 341, loss = 0.01509512\n",
      "Iteration 342, loss = 0.01498221\n",
      "Iteration 343, loss = 0.01486594\n",
      "Iteration 344, loss = 0.01481515\n",
      "Iteration 345, loss = 0.01479115\n",
      "Iteration 346, loss = 0.01471582\n",
      "Iteration 347, loss = 0.01471058\n",
      "Iteration 348, loss = 0.01463201\n",
      "Iteration 349, loss = 0.01468495\n",
      "Iteration 350, loss = 0.01467313\n",
      "Iteration 351, loss = 0.01452250\n",
      "Iteration 352, loss = 0.01451237\n",
      "Iteration 353, loss = 0.01439368\n",
      "Iteration 354, loss = 0.01437174\n",
      "Iteration 355, loss = 0.01433603\n",
      "Iteration 356, loss = 0.01430943\n",
      "Iteration 357, loss = 0.01419109\n",
      "Iteration 358, loss = 0.01425358\n",
      "Iteration 359, loss = 0.01414284\n",
      "Iteration 360, loss = 0.01408308\n",
      "Iteration 361, loss = 0.01405643\n",
      "Iteration 362, loss = 0.01403856\n",
      "Iteration 363, loss = 0.01394014\n",
      "Iteration 364, loss = 0.01396957\n",
      "Iteration 365, loss = 0.01389303\n",
      "Iteration 366, loss = 0.01392123\n",
      "Iteration 367, loss = 0.01380063\n",
      "Iteration 368, loss = 0.01375177\n",
      "Iteration 369, loss = 0.01374329\n",
      "Iteration 370, loss = 0.01367612\n",
      "Iteration 371, loss = 0.01365469\n",
      "Iteration 372, loss = 0.01359642\n",
      "Iteration 373, loss = 0.01366555\n",
      "Iteration 374, loss = 0.01355484\n",
      "Iteration 375, loss = 0.01348633\n",
      "Iteration 376, loss = 0.01342126\n",
      "Iteration 377, loss = 0.01341817\n",
      "Iteration 378, loss = 0.01336172\n",
      "Iteration 379, loss = 0.01340686\n",
      "Iteration 380, loss = 0.01328523\n",
      "Iteration 381, loss = 0.01323301\n",
      "Iteration 382, loss = 0.01317238\n",
      "Iteration 383, loss = 0.01311761\n",
      "Iteration 384, loss = 0.01315441\n",
      "Iteration 385, loss = 0.01308316\n",
      "Iteration 386, loss = 0.01301960\n",
      "Iteration 387, loss = 0.01298400\n",
      "Iteration 388, loss = 0.01296159\n",
      "Iteration 389, loss = 0.01294862\n",
      "Iteration 390, loss = 0.01295852\n",
      "Iteration 391, loss = 0.01285967\n",
      "Iteration 392, loss = 0.01283709\n",
      "Iteration 393, loss = 0.01278463\n",
      "Iteration 394, loss = 0.01276044\n",
      "Iteration 395, loss = 0.01273731\n",
      "Iteration 396, loss = 0.01267641\n",
      "Iteration 397, loss = 0.01262975\n",
      "Iteration 398, loss = 0.01259388\n",
      "Iteration 399, loss = 0.01254262\n",
      "Iteration 400, loss = 0.01268330\n",
      "Iteration 401, loss = 0.01246802\n",
      "Iteration 402, loss = 0.01250295\n",
      "Iteration 403, loss = 0.01246925\n",
      "Iteration 404, loss = 0.01243125\n",
      "Iteration 405, loss = 0.01235875\n",
      "Iteration 406, loss = 0.01229715\n",
      "Iteration 407, loss = 0.01224386\n",
      "Iteration 408, loss = 0.01221884\n",
      "Iteration 409, loss = 0.01222246\n",
      "Iteration 410, loss = 0.01214199\n",
      "Iteration 411, loss = 0.01210367\n",
      "Iteration 412, loss = 0.01212908\n",
      "Iteration 413, loss = 0.01205277\n",
      "Iteration 414, loss = 0.01201633\n",
      "Iteration 415, loss = 0.01207046\n",
      "Iteration 416, loss = 0.01193277\n",
      "Iteration 417, loss = 0.01195193\n",
      "Iteration 418, loss = 0.01196856\n",
      "Iteration 419, loss = 0.01189470\n",
      "Iteration 420, loss = 0.01182516\n",
      "Iteration 421, loss = 0.01184676\n",
      "Iteration 422, loss = 0.01180492\n",
      "Iteration 423, loss = 0.01181917\n",
      "Iteration 424, loss = 0.01171022\n",
      "Iteration 425, loss = 0.01168448\n",
      "Iteration 426, loss = 0.01165362\n",
      "Iteration 427, loss = 0.01164767\n",
      "Iteration 428, loss = 0.01164133\n",
      "Iteration 429, loss = 0.01155601\n",
      "Iteration 430, loss = 0.01151652\n",
      "Iteration 431, loss = 0.01152204\n",
      "Iteration 432, loss = 0.01157071\n",
      "Iteration 433, loss = 0.01151205\n",
      "Iteration 434, loss = 0.01140360\n",
      "Iteration 435, loss = 0.01140048\n",
      "Iteration 436, loss = 0.01136087\n",
      "Iteration 437, loss = 0.01133777\n",
      "Iteration 438, loss = 0.01128679\n",
      "Iteration 439, loss = 0.01126302\n",
      "Iteration 440, loss = 0.01132125\n",
      "Iteration 441, loss = 0.01117316\n",
      "Iteration 442, loss = 0.01120802\n",
      "Iteration 443, loss = 0.01117816\n",
      "Iteration 444, loss = 0.01110996\n",
      "Iteration 445, loss = 0.01104751\n",
      "Iteration 446, loss = 0.01106627\n",
      "Iteration 447, loss = 0.01100896\n",
      "Iteration 448, loss = 0.01097515\n",
      "Iteration 449, loss = 0.01100737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 450, loss = 0.01093434\n",
      "Iteration 451, loss = 0.01089014\n",
      "Iteration 452, loss = 0.01087608\n",
      "Iteration 453, loss = 0.01091487\n",
      "Iteration 454, loss = 0.01078364\n",
      "Iteration 455, loss = 0.01077534\n",
      "Iteration 456, loss = 0.01074612\n",
      "Iteration 457, loss = 0.01070396\n",
      "Iteration 458, loss = 0.01072822\n",
      "Iteration 459, loss = 0.01066554\n",
      "Iteration 460, loss = 0.01063203\n",
      "Iteration 461, loss = 0.01058165\n",
      "Iteration 462, loss = 0.01057429\n",
      "Iteration 463, loss = 0.01052479\n",
      "Iteration 464, loss = 0.01048758\n",
      "Iteration 465, loss = 0.01050495\n",
      "Iteration 466, loss = 0.01050252\n",
      "Iteration 467, loss = 0.01054791\n",
      "Iteration 468, loss = 0.01049414\n",
      "Iteration 469, loss = 0.01043096\n",
      "Iteration 470, loss = 0.01036037\n",
      "Iteration 471, loss = 0.01031853\n",
      "Iteration 472, loss = 0.01032830\n",
      "Iteration 473, loss = 0.01032388\n",
      "Iteration 474, loss = 0.01028260\n",
      "Iteration 475, loss = 0.01026995\n",
      "Iteration 476, loss = 0.01015731\n",
      "Iteration 477, loss = 0.01019062\n",
      "Iteration 478, loss = 0.01014359\n",
      "Iteration 479, loss = 0.01008891\n",
      "Iteration 480, loss = 0.01013562\n",
      "Iteration 481, loss = 0.01007922\n",
      "Iteration 482, loss = 0.01008179\n",
      "Iteration 483, loss = 0.01004631\n",
      "Iteration 484, loss = 0.01000145\n",
      "Iteration 485, loss = 0.00994598\n",
      "Iteration 486, loss = 0.00996298\n",
      "Iteration 487, loss = 0.00992765\n",
      "Iteration 488, loss = 0.00993758\n",
      "Iteration 489, loss = 0.00984146\n",
      "Iteration 490, loss = 0.00986998\n",
      "Iteration 491, loss = 0.00985910\n",
      "Iteration 492, loss = 0.00982631\n",
      "Iteration 493, loss = 0.00976501\n",
      "Iteration 494, loss = 0.00975754\n",
      "Iteration 495, loss = 0.00971667\n",
      "Iteration 496, loss = 0.00967970\n",
      "Iteration 497, loss = 0.00969967\n",
      "Iteration 498, loss = 0.00965385\n",
      "Iteration 499, loss = 0.00972359\n",
      "Iteration 500, loss = 0.00958636\n",
      "Iteration 501, loss = 0.00960247\n",
      "Iteration 502, loss = 0.00956339\n",
      "Iteration 503, loss = 0.00954551\n",
      "Iteration 504, loss = 0.00957425\n",
      "Iteration 505, loss = 0.00954274\n",
      "Iteration 506, loss = 0.00946974\n",
      "Iteration 507, loss = 0.00947609\n",
      "Iteration 508, loss = 0.00937276\n",
      "Iteration 509, loss = 0.00948515\n",
      "Iteration 510, loss = 0.00941974\n",
      "Iteration 511, loss = 0.00930355\n",
      "Iteration 512, loss = 0.00938933\n",
      "Iteration 513, loss = 0.00928315\n",
      "Iteration 514, loss = 0.00941858\n",
      "Iteration 515, loss = 0.00923410\n",
      "Iteration 516, loss = 0.00929883\n",
      "Iteration 517, loss = 0.00936447\n",
      "Iteration 518, loss = 0.00919807\n",
      "Iteration 519, loss = 0.00918704\n",
      "Iteration 520, loss = 0.00918969\n",
      "Iteration 521, loss = 0.00914954\n",
      "Iteration 522, loss = 0.00912549\n",
      "Iteration 523, loss = 0.00913276\n",
      "Iteration 524, loss = 0.00905101\n",
      "Iteration 525, loss = 0.00907383\n",
      "Iteration 526, loss = 0.00900350\n",
      "Iteration 527, loss = 0.00913671\n",
      "Iteration 528, loss = 0.00900346\n",
      "Iteration 529, loss = 0.00896529\n",
      "Iteration 530, loss = 0.00895306\n",
      "Iteration 531, loss = 0.00893376\n",
      "Iteration 532, loss = 0.00887379\n",
      "Iteration 533, loss = 0.00889680\n",
      "Iteration 534, loss = 0.00887575\n",
      "Iteration 535, loss = 0.00881964\n",
      "Iteration 536, loss = 0.00878532\n",
      "Iteration 537, loss = 0.00884579\n",
      "Iteration 538, loss = 0.00879444\n",
      "Iteration 539, loss = 0.00876074\n",
      "Iteration 540, loss = 0.00878901\n",
      "Iteration 541, loss = 0.00876136\n",
      "Iteration 542, loss = 0.00879758\n",
      "Iteration 543, loss = 0.00867953\n",
      "Iteration 544, loss = 0.00857777\n",
      "Iteration 545, loss = 0.00870405\n",
      "Iteration 546, loss = 0.00858873\n",
      "Iteration 547, loss = 0.00872098\n",
      "Iteration 548, loss = 0.00856772\n",
      "Iteration 549, loss = 0.00853528\n",
      "Iteration 550, loss = 0.00858339\n",
      "Iteration 551, loss = 0.00857419\n",
      "Iteration 552, loss = 0.00844668\n",
      "Iteration 553, loss = 0.00844353\n",
      "Iteration 554, loss = 0.00847796\n",
      "Iteration 555, loss = 0.00842801\n",
      "Iteration 556, loss = 0.00842220\n",
      "Iteration 557, loss = 0.00836942\n",
      "Iteration 558, loss = 0.00840361\n",
      "Iteration 559, loss = 0.00841364\n",
      "Iteration 560, loss = 0.00831700\n",
      "Iteration 561, loss = 0.00829076\n",
      "Iteration 562, loss = 0.00827535\n",
      "Iteration 563, loss = 0.00826176\n",
      "Iteration 564, loss = 0.00828130\n",
      "Iteration 565, loss = 0.00824835\n",
      "Iteration 566, loss = 0.00820033\n",
      "Iteration 567, loss = 0.00820956\n",
      "Iteration 568, loss = 0.00816647\n",
      "Iteration 569, loss = 0.00819806\n",
      "Iteration 570, loss = 0.00822601\n",
      "Iteration 571, loss = 0.00816809\n",
      "Iteration 572, loss = 0.00808406\n",
      "Iteration 573, loss = 0.00806147\n",
      "Iteration 574, loss = 0.00806153\n",
      "Iteration 575, loss = 0.00802566\n",
      "Iteration 576, loss = 0.00811128\n",
      "Iteration 577, loss = 0.00802031\n",
      "Iteration 578, loss = 0.00799928\n",
      "Iteration 579, loss = 0.00799279\n",
      "Iteration 580, loss = 0.00799318\n",
      "Iteration 581, loss = 0.00791426\n",
      "Iteration 582, loss = 0.00793502\n",
      "Iteration 583, loss = 0.00790817\n",
      "Iteration 584, loss = 0.00790915\n",
      "Iteration 585, loss = 0.00782809\n",
      "Iteration 586, loss = 0.00788794\n",
      "Iteration 587, loss = 0.00782687\n",
      "Iteration 588, loss = 0.00789303\n",
      "Iteration 589, loss = 0.00778448\n",
      "Iteration 590, loss = 0.00777186\n",
      "Iteration 591, loss = 0.00773212\n",
      "Iteration 592, loss = 0.00774555\n",
      "Iteration 593, loss = 0.00772387\n",
      "Iteration 594, loss = 0.00769651\n",
      "Iteration 595, loss = 0.00768047\n",
      "Iteration 596, loss = 0.00765674\n",
      "Iteration 597, loss = 0.00768330\n",
      "Iteration 598, loss = 0.00765180\n",
      "Iteration 599, loss = 0.00761073\n",
      "Iteration 600, loss = 0.00759615\n",
      "Iteration 601, loss = 0.00762396\n",
      "Iteration 602, loss = 0.00752792\n",
      "Iteration 603, loss = 0.00753359\n",
      "Iteration 604, loss = 0.00752520\n",
      "Iteration 605, loss = 0.00748609\n",
      "Iteration 606, loss = 0.00754681\n",
      "Iteration 607, loss = 0.00764738\n",
      "Iteration 608, loss = 0.00748845\n",
      "Iteration 609, loss = 0.00753301\n",
      "Iteration 610, loss = 0.00743368\n",
      "Iteration 611, loss = 0.00744252\n",
      "Iteration 612, loss = 0.00748437\n",
      "Iteration 613, loss = 0.00741007\n",
      "Iteration 614, loss = 0.00740302\n",
      "Iteration 615, loss = 0.00739480\n",
      "Iteration 616, loss = 0.00731620\n",
      "Iteration 617, loss = 0.00733421\n",
      "Iteration 618, loss = 0.00734529\n",
      "Iteration 619, loss = 0.00732748\n",
      "Iteration 620, loss = 0.00734967\n",
      "Iteration 621, loss = 0.00733142\n",
      "Iteration 622, loss = 0.00729853\n",
      "Iteration 623, loss = 0.00729059\n",
      "Iteration 624, loss = 0.00719806\n",
      "Iteration 625, loss = 0.00716200\n",
      "Iteration 626, loss = 0.00716475\n",
      "Iteration 627, loss = 0.00719847\n",
      "Iteration 628, loss = 0.00714962\n",
      "Iteration 629, loss = 0.00717423\n",
      "Iteration 630, loss = 0.00717011\n",
      "Iteration 631, loss = 0.00719561\n",
      "Iteration 632, loss = 0.00712335\n",
      "Iteration 633, loss = 0.00713323\n",
      "Iteration 634, loss = 0.00706685\n",
      "Iteration 635, loss = 0.00704708\n",
      "Iteration 636, loss = 0.00701272\n",
      "Iteration 637, loss = 0.00700798\n",
      "Iteration 638, loss = 0.00708705\n",
      "Iteration 639, loss = 0.00696194\n",
      "Iteration 640, loss = 0.00702772\n",
      "Iteration 641, loss = 0.00703742\n",
      "Iteration 642, loss = 0.00695869\n",
      "Iteration 643, loss = 0.00691903\n",
      "Iteration 644, loss = 0.00689294\n",
      "Iteration 645, loss = 0.00695229\n",
      "Iteration 646, loss = 0.00689580\n",
      "Iteration 647, loss = 0.00688167\n",
      "Iteration 648, loss = 0.00685431\n",
      "Iteration 649, loss = 0.00682919\n",
      "Iteration 650, loss = 0.00682744\n",
      "Iteration 651, loss = 0.00678294\n",
      "Iteration 652, loss = 0.00679370\n",
      "Iteration 653, loss = 0.00682833\n",
      "Iteration 654, loss = 0.00673144\n",
      "Iteration 655, loss = 0.00683882\n",
      "Iteration 656, loss = 0.00672512\n",
      "Iteration 657, loss = 0.00672275\n",
      "Iteration 658, loss = 0.00670700\n",
      "Iteration 659, loss = 0.00667316\n",
      "Iteration 660, loss = 0.00667654\n",
      "Iteration 661, loss = 0.00667763\n",
      "Iteration 662, loss = 0.00672401\n",
      "Iteration 663, loss = 0.00663281\n",
      "Iteration 664, loss = 0.00672226\n",
      "Iteration 665, loss = 0.00679342\n",
      "Iteration 666, loss = 0.00662121\n",
      "Iteration 667, loss = 0.00664603\n",
      "Iteration 668, loss = 0.00656204\n",
      "Iteration 669, loss = 0.00661199\n",
      "Iteration 670, loss = 0.00659391\n",
      "Iteration 671, loss = 0.00665317\n",
      "Iteration 672, loss = 0.00652631\n",
      "Iteration 673, loss = 0.00656305\n",
      "Iteration 674, loss = 0.00654902\n",
      "Iteration 675, loss = 0.00644772\n",
      "Iteration 676, loss = 0.00647413\n",
      "Iteration 677, loss = 0.00644720\n",
      "Iteration 678, loss = 0.00652691\n",
      "Iteration 679, loss = 0.00641115\n",
      "Iteration 680, loss = 0.00649687\n",
      "Iteration 681, loss = 0.00637424\n",
      "Iteration 682, loss = 0.00636618\n",
      "Iteration 683, loss = 0.00635422\n",
      "Iteration 684, loss = 0.00643240\n",
      "Iteration 685, loss = 0.00637587\n",
      "Iteration 686, loss = 0.00631861\n",
      "Iteration 687, loss = 0.00639967\n",
      "Iteration 688, loss = 0.00647160\n",
      "Iteration 689, loss = 0.00629902\n",
      "Iteration 690, loss = 0.00634747\n",
      "Iteration 691, loss = 0.00633824\n",
      "Iteration 692, loss = 0.00631998\n",
      "Iteration 693, loss = 0.00628047\n",
      "Iteration 694, loss = 0.00626554\n",
      "Iteration 695, loss = 0.00620745\n",
      "Iteration 696, loss = 0.00620779\n",
      "Iteration 697, loss = 0.00618453\n",
      "Iteration 698, loss = 0.00624002\n",
      "Iteration 699, loss = 0.00621514\n",
      "Iteration 700, loss = 0.00620371\n",
      "Iteration 701, loss = 0.00615027\n",
      "Iteration 702, loss = 0.00610258\n",
      "Iteration 703, loss = 0.00611715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 704, loss = 0.00619661\n",
      "Iteration 705, loss = 0.00610420\n",
      "Iteration 706, loss = 0.00608813\n",
      "Iteration 707, loss = 0.00607130\n",
      "Iteration 708, loss = 0.00606654\n",
      "Iteration 709, loss = 0.00609106\n",
      "Iteration 710, loss = 0.00608836\n",
      "Iteration 711, loss = 0.00604822\n",
      "Iteration 712, loss = 0.00597894\n",
      "Iteration 713, loss = 0.00602343\n",
      "Iteration 714, loss = 0.00599171\n",
      "Iteration 715, loss = 0.00596874\n",
      "Iteration 716, loss = 0.00593321\n",
      "Iteration 717, loss = 0.00593155\n",
      "Iteration 718, loss = 0.00594205\n",
      "Iteration 719, loss = 0.00593784\n",
      "Iteration 720, loss = 0.00601484\n",
      "Iteration 721, loss = 0.00593534\n",
      "Iteration 722, loss = 0.00588140\n",
      "Iteration 723, loss = 0.00588854\n",
      "Iteration 724, loss = 0.00587969\n",
      "Iteration 725, loss = 0.00588381\n",
      "Iteration 726, loss = 0.00587271\n",
      "Iteration 727, loss = 0.00595830\n",
      "Iteration 728, loss = 0.00579301\n",
      "Iteration 729, loss = 0.00581488\n",
      "Iteration 730, loss = 0.00578333\n",
      "Iteration 731, loss = 0.00586058\n",
      "Iteration 732, loss = 0.00579893\n",
      "Iteration 733, loss = 0.00582371\n",
      "Iteration 734, loss = 0.00577590\n",
      "Iteration 735, loss = 0.00572750\n",
      "Iteration 736, loss = 0.00574856\n",
      "Iteration 737, loss = 0.00572636\n",
      "Iteration 738, loss = 0.00567467\n",
      "Iteration 739, loss = 0.00570871\n",
      "Iteration 740, loss = 0.00567741\n",
      "Iteration 741, loss = 0.00563839\n",
      "Iteration 742, loss = 0.00581064\n",
      "Iteration 743, loss = 0.00566372\n",
      "Iteration 744, loss = 0.00566380\n",
      "Iteration 745, loss = 0.00560516\n",
      "Iteration 746, loss = 0.00565283\n",
      "Iteration 747, loss = 0.00563088\n",
      "Iteration 748, loss = 0.00567617\n",
      "Iteration 749, loss = 0.00558492\n",
      "Iteration 750, loss = 0.00555940\n",
      "Iteration 751, loss = 0.00555066\n",
      "Iteration 752, loss = 0.00556385\n",
      "Iteration 753, loss = 0.00560192\n",
      "Iteration 754, loss = 0.00552487\n",
      "Iteration 755, loss = 0.00555705\n",
      "Iteration 756, loss = 0.00551763\n",
      "Iteration 757, loss = 0.00550838\n",
      "Iteration 758, loss = 0.00550717\n",
      "Iteration 759, loss = 0.00549129\n",
      "Iteration 760, loss = 0.00549706\n",
      "Iteration 761, loss = 0.00545390\n",
      "Iteration 762, loss = 0.00541996\n",
      "Iteration 763, loss = 0.00546843\n",
      "Iteration 764, loss = 0.00542245\n",
      "Iteration 765, loss = 0.00537641\n",
      "Iteration 766, loss = 0.00544339\n",
      "Iteration 767, loss = 0.00537281\n",
      "Iteration 768, loss = 0.00543156\n",
      "Iteration 769, loss = 0.00537512\n",
      "Iteration 770, loss = 0.00538182\n",
      "Iteration 771, loss = 0.00534429\n",
      "Iteration 772, loss = 0.00536348\n",
      "Iteration 773, loss = 0.00530950\n",
      "Iteration 774, loss = 0.00535494\n",
      "Iteration 775, loss = 0.00561926\n",
      "Iteration 776, loss = 0.00534737\n",
      "Iteration 777, loss = 0.00529256\n",
      "Iteration 778, loss = 0.00534682\n",
      "Iteration 779, loss = 0.00525480\n",
      "Iteration 780, loss = 0.00526460\n",
      "Iteration 781, loss = 0.00530568\n",
      "Iteration 782, loss = 0.00523634\n",
      "Iteration 783, loss = 0.00524717\n",
      "Iteration 784, loss = 0.00522972\n",
      "Iteration 785, loss = 0.00522869\n",
      "Iteration 786, loss = 0.00519483\n",
      "Iteration 787, loss = 0.00519494\n",
      "Iteration 788, loss = 0.00519363\n",
      "Iteration 789, loss = 0.00513909\n",
      "Iteration 790, loss = 0.00514319\n",
      "Iteration 791, loss = 0.00521612\n",
      "Iteration 792, loss = 0.00513578\n",
      "Iteration 793, loss = 0.00516085\n",
      "Iteration 794, loss = 0.00513611\n",
      "Iteration 795, loss = 0.00508149\n",
      "Iteration 796, loss = 0.00511783\n",
      "Iteration 797, loss = 0.00513279\n",
      "Iteration 798, loss = 0.00509536\n",
      "Iteration 799, loss = 0.00508966\n",
      "Iteration 800, loss = 0.00507558\n",
      "Iteration 801, loss = 0.00506175\n",
      "Iteration 802, loss = 0.00513854\n",
      "Iteration 803, loss = 0.00503004\n",
      "Iteration 804, loss = 0.00506275\n",
      "Iteration 805, loss = 0.00505528\n",
      "Iteration 806, loss = 0.00503524\n",
      "Iteration 807, loss = 0.00504356\n",
      "Iteration 808, loss = 0.00511882\n",
      "Iteration 809, loss = 0.00511487\n",
      "Iteration 810, loss = 0.00502749\n",
      "Iteration 811, loss = 0.00500176\n",
      "Iteration 812, loss = 0.00497011\n",
      "Iteration 813, loss = 0.00495940\n",
      "Iteration 814, loss = 0.00495129\n",
      "Iteration 815, loss = 0.00496972\n",
      "Iteration 816, loss = 0.00489267\n",
      "Iteration 817, loss = 0.00491923\n",
      "Iteration 818, loss = 0.00488957\n",
      "Iteration 819, loss = 0.00494797\n",
      "Iteration 820, loss = 0.00494111\n",
      "Iteration 821, loss = 0.00491837\n",
      "Iteration 822, loss = 0.00487751\n",
      "Iteration 823, loss = 0.00495068\n",
      "Iteration 824, loss = 0.00485314\n",
      "Iteration 825, loss = 0.00490011\n",
      "Iteration 826, loss = 0.00492313\n",
      "Iteration 827, loss = 0.00486284\n",
      "Iteration 828, loss = 0.00484970\n",
      "Iteration 829, loss = 0.00478637\n",
      "Iteration 830, loss = 0.00480743\n",
      "Iteration 831, loss = 0.00480468\n",
      "Iteration 832, loss = 0.00476548\n",
      "Iteration 833, loss = 0.00474519\n",
      "Iteration 834, loss = 0.00474840\n",
      "Iteration 835, loss = 0.00478565\n",
      "Iteration 836, loss = 0.00475446\n",
      "Iteration 837, loss = 0.00474619\n",
      "Iteration 838, loss = 0.00481368\n",
      "Iteration 839, loss = 0.00470411\n",
      "Iteration 840, loss = 0.00470772\n",
      "Iteration 841, loss = 0.00469422\n",
      "Iteration 842, loss = 0.00472118\n",
      "Iteration 843, loss = 0.00467263\n",
      "Iteration 844, loss = 0.00465103\n",
      "Iteration 845, loss = 0.00466516\n",
      "Iteration 846, loss = 0.00465268\n",
      "Iteration 847, loss = 0.00471416\n",
      "Iteration 848, loss = 0.00465209\n",
      "Iteration 849, loss = 0.00462811\n",
      "Iteration 850, loss = 0.00462544\n",
      "Iteration 851, loss = 0.00464300\n",
      "Iteration 852, loss = 0.00471774\n",
      "Iteration 853, loss = 0.00470205\n",
      "Iteration 854, loss = 0.00471479\n",
      "Iteration 855, loss = 0.00457349\n",
      "Iteration 856, loss = 0.00465403\n",
      "Iteration 857, loss = 0.00481775\n",
      "Iteration 858, loss = 0.00460820\n",
      "Iteration 859, loss = 0.00457929\n",
      "Iteration 860, loss = 0.00457999\n",
      "Iteration 861, loss = 0.00455819\n",
      "Iteration 862, loss = 0.00462578\n",
      "Iteration 863, loss = 0.00452469\n",
      "Iteration 864, loss = 0.00466655\n",
      "Iteration 865, loss = 0.00448237\n",
      "Iteration 866, loss = 0.00449424\n",
      "Iteration 867, loss = 0.00452677\n",
      "Iteration 868, loss = 0.00457083\n",
      "Iteration 869, loss = 0.00452205\n",
      "Iteration 870, loss = 0.00448020\n",
      "Iteration 871, loss = 0.00450104\n",
      "Iteration 872, loss = 0.00445742\n",
      "Iteration 873, loss = 0.00444542\n",
      "Iteration 874, loss = 0.00439878\n",
      "Iteration 875, loss = 0.00443881\n",
      "Iteration 876, loss = 0.00439050\n",
      "Iteration 877, loss = 0.00439763\n",
      "Iteration 878, loss = 0.00443800\n",
      "Iteration 879, loss = 0.00454117\n",
      "Iteration 880, loss = 0.00444112\n",
      "Iteration 881, loss = 0.00443624\n",
      "Iteration 882, loss = 0.00444295\n",
      "Iteration 883, loss = 0.00439262\n",
      "Iteration 884, loss = 0.00433621\n",
      "Iteration 885, loss = 0.00435525\n",
      "Iteration 886, loss = 0.00434613\n",
      "Iteration 887, loss = 0.00434128\n",
      "Iteration 888, loss = 0.00438244\n",
      "Iteration 889, loss = 0.00429888\n",
      "Iteration 890, loss = 0.00435815\n",
      "Iteration 891, loss = 0.00427195\n",
      "Iteration 892, loss = 0.00432637\n",
      "Iteration 893, loss = 0.00428678\n",
      "Iteration 894, loss = 0.00435110\n",
      "Iteration 895, loss = 0.00430098\n",
      "Iteration 896, loss = 0.00428977\n",
      "Iteration 897, loss = 0.00429477\n",
      "Iteration 898, loss = 0.00429960\n",
      "Iteration 899, loss = 0.00427584\n",
      "Iteration 900, loss = 0.00424131\n",
      "Iteration 901, loss = 0.00423095\n",
      "Iteration 902, loss = 0.00421315\n",
      "Iteration 903, loss = 0.00421750\n",
      "Iteration 904, loss = 0.00421302\n",
      "Iteration 905, loss = 0.00417036\n",
      "Iteration 906, loss = 0.00423188\n",
      "Iteration 907, loss = 0.00422840\n",
      "Iteration 908, loss = 0.00415652\n",
      "Iteration 909, loss = 0.00421738\n",
      "Iteration 910, loss = 0.00417254\n",
      "Iteration 911, loss = 0.00414612\n",
      "Iteration 912, loss = 0.00413782\n",
      "Iteration 913, loss = 0.00424509\n",
      "Iteration 914, loss = 0.00414987\n",
      "Iteration 915, loss = 0.00421979\n",
      "Iteration 916, loss = 0.00420129\n",
      "Iteration 917, loss = 0.00411411\n",
      "Iteration 918, loss = 0.00416644\n",
      "Iteration 919, loss = 0.00409577\n",
      "Iteration 920, loss = 0.00409933\n",
      "Iteration 921, loss = 0.00408543\n",
      "Iteration 922, loss = 0.00412125\n",
      "Iteration 923, loss = 0.00406000\n",
      "Iteration 924, loss = 0.00407352\n",
      "Iteration 925, loss = 0.00408113\n",
      "Iteration 926, loss = 0.00404830\n",
      "Iteration 927, loss = 0.00402545\n",
      "Iteration 928, loss = 0.00403170\n",
      "Iteration 929, loss = 0.00410424\n",
      "Iteration 930, loss = 0.00399943\n",
      "Iteration 931, loss = 0.00404611\n",
      "Iteration 932, loss = 0.00404161\n",
      "Iteration 933, loss = 0.00400075\n",
      "Iteration 934, loss = 0.00406717\n",
      "Iteration 935, loss = 0.00397761\n",
      "Iteration 936, loss = 0.00398018\n",
      "Iteration 937, loss = 0.00400983\n",
      "Iteration 938, loss = 0.00396784\n",
      "Iteration 939, loss = 0.00397202\n",
      "Iteration 940, loss = 0.00394312\n",
      "Iteration 941, loss = 0.00398022\n",
      "Iteration 942, loss = 0.00403006\n",
      "Iteration 943, loss = 0.00397415\n",
      "Iteration 944, loss = 0.00398622\n",
      "Iteration 945, loss = 0.00392462\n",
      "Iteration 946, loss = 0.00394407\n",
      "Iteration 947, loss = 0.00390629\n",
      "Iteration 948, loss = 0.00389693\n",
      "Iteration 949, loss = 0.00390046\n",
      "Iteration 950, loss = 0.00390001\n",
      "Iteration 951, loss = 0.00397307\n",
      "Iteration 952, loss = 0.00386955\n",
      "Iteration 953, loss = 0.00386863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 954, loss = 0.00389301\n",
      "Iteration 955, loss = 0.00388536\n",
      "Iteration 956, loss = 0.00387510\n",
      "Iteration 957, loss = 0.00390556\n",
      "Iteration 958, loss = 0.00384786\n",
      "Iteration 959, loss = 0.00385485\n",
      "Iteration 960, loss = 0.00382262\n",
      "Iteration 961, loss = 0.00383277\n",
      "Iteration 962, loss = 0.00380435\n",
      "Iteration 963, loss = 0.00384153\n",
      "Iteration 964, loss = 0.00387671\n",
      "Iteration 965, loss = 0.00380158\n",
      "Iteration 966, loss = 0.00384813\n",
      "Iteration 967, loss = 0.00378084\n",
      "Iteration 968, loss = 0.00376749\n",
      "Iteration 969, loss = 0.00379632\n",
      "Iteration 970, loss = 0.00375047\n",
      "Iteration 971, loss = 0.00374154\n",
      "Iteration 972, loss = 0.00376931\n",
      "Iteration 973, loss = 0.00375094\n",
      "Iteration 974, loss = 0.00371564\n",
      "Iteration 975, loss = 0.00374099\n",
      "Iteration 976, loss = 0.00371668\n",
      "Iteration 977, loss = 0.00369658\n",
      "Iteration 978, loss = 0.00373164\n",
      "Iteration 979, loss = 0.00370144\n",
      "Iteration 980, loss = 0.00375177\n",
      "Iteration 981, loss = 0.00381581\n",
      "Iteration 982, loss = 0.00374837\n",
      "Iteration 983, loss = 0.00368956\n",
      "Iteration 984, loss = 0.00367519\n",
      "Iteration 985, loss = 0.00370298\n",
      "Iteration 986, loss = 0.00366431\n",
      "Iteration 987, loss = 0.00363289\n",
      "Iteration 988, loss = 0.00363163\n",
      "Iteration 989, loss = 0.00365920\n",
      "Iteration 990, loss = 0.00363153\n",
      "Iteration 991, loss = 0.00367590\n",
      "Iteration 992, loss = 0.00362945\n",
      "Iteration 993, loss = 0.00369187\n",
      "Iteration 994, loss = 0.00362901\n",
      "Iteration 995, loss = 0.00363780\n",
      "Iteration 996, loss = 0.00361680\n",
      "Iteration 997, loss = 0.00374492\n",
      "Iteration 998, loss = 0.00360057\n",
      "Iteration 999, loss = 0.00362962\n",
      "Iteration 1000, loss = 0.00360273\n",
      "Iteration 1, loss = 0.60814550\n",
      "Iteration 2, loss = 0.54443023\n",
      "Iteration 3, loss = 0.49064645\n",
      "Iteration 4, loss = 0.44485968\n",
      "Iteration 5, loss = 0.40624953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lindino/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.37292469\n",
      "Iteration 7, loss = 0.34415758\n",
      "Iteration 8, loss = 0.31903556\n",
      "Iteration 9, loss = 0.29665670\n",
      "Iteration 10, loss = 0.27695561\n",
      "Iteration 11, loss = 0.25960379\n",
      "Iteration 12, loss = 0.24388750\n",
      "Iteration 13, loss = 0.23001075\n",
      "Iteration 14, loss = 0.21745534\n",
      "Iteration 15, loss = 0.20623065\n",
      "Iteration 16, loss = 0.19622785\n",
      "Iteration 17, loss = 0.18696661\n",
      "Iteration 18, loss = 0.17872971\n",
      "Iteration 19, loss = 0.17112446\n",
      "Iteration 20, loss = 0.16412671\n",
      "Iteration 21, loss = 0.15785000\n",
      "Iteration 22, loss = 0.15218515\n",
      "Iteration 23, loss = 0.14689363\n",
      "Iteration 24, loss = 0.14212741\n",
      "Iteration 25, loss = 0.13774782\n",
      "Iteration 26, loss = 0.13369370\n",
      "Iteration 27, loss = 0.13006894\n",
      "Iteration 28, loss = 0.12645598\n",
      "Iteration 29, loss = 0.12319395\n",
      "Iteration 30, loss = 0.12013308\n",
      "Iteration 31, loss = 0.11732800\n",
      "Iteration 32, loss = 0.11461399\n",
      "Iteration 33, loss = 0.11215700\n",
      "Iteration 34, loss = 0.10976635\n",
      "Iteration 35, loss = 0.10744917\n",
      "Iteration 36, loss = 0.10530487\n",
      "Iteration 37, loss = 0.10331792\n",
      "Iteration 38, loss = 0.10139355\n",
      "Iteration 39, loss = 0.09966489\n",
      "Iteration 40, loss = 0.09786464\n",
      "Iteration 41, loss = 0.09631379\n",
      "Iteration 42, loss = 0.09470780\n",
      "Iteration 43, loss = 0.09324917\n",
      "Iteration 44, loss = 0.09182013\n",
      "Iteration 45, loss = 0.09046732\n",
      "Iteration 46, loss = 0.08919634\n",
      "Iteration 47, loss = 0.08783777\n",
      "Iteration 48, loss = 0.08663058\n",
      "Iteration 49, loss = 0.08542293\n",
      "Iteration 50, loss = 0.08420115\n",
      "Iteration 51, loss = 0.08309813\n",
      "Iteration 52, loss = 0.08206836\n",
      "Iteration 53, loss = 0.08093921\n",
      "Iteration 54, loss = 0.07995244\n",
      "Iteration 55, loss = 0.07898436\n",
      "Iteration 56, loss = 0.07787389\n",
      "Iteration 57, loss = 0.07689365\n",
      "Iteration 58, loss = 0.07591949\n",
      "Iteration 59, loss = 0.07500191\n",
      "Iteration 60, loss = 0.07411830\n",
      "Iteration 61, loss = 0.07335239\n",
      "Iteration 62, loss = 0.07237405\n",
      "Iteration 63, loss = 0.07148992\n",
      "Iteration 64, loss = 0.07054012\n",
      "Iteration 65, loss = 0.06970221\n",
      "Iteration 66, loss = 0.06893178\n",
      "Iteration 67, loss = 0.06815105\n",
      "Iteration 68, loss = 0.06748950\n",
      "Iteration 69, loss = 0.06671720\n",
      "Iteration 70, loss = 0.06592279\n",
      "Iteration 71, loss = 0.06533232\n",
      "Iteration 72, loss = 0.06452013\n",
      "Iteration 73, loss = 0.06384292\n",
      "Iteration 74, loss = 0.06310476\n",
      "Iteration 75, loss = 0.06242162\n",
      "Iteration 76, loss = 0.06185310\n",
      "Iteration 77, loss = 0.06122157\n",
      "Iteration 78, loss = 0.06072947\n",
      "Iteration 79, loss = 0.06011884\n",
      "Iteration 80, loss = 0.05942934\n",
      "Iteration 81, loss = 0.05892475\n",
      "Iteration 82, loss = 0.05829473\n",
      "Iteration 83, loss = 0.05773366\n",
      "Iteration 84, loss = 0.05719876\n",
      "Iteration 85, loss = 0.05666687\n",
      "Iteration 86, loss = 0.05621129\n",
      "Iteration 87, loss = 0.05567395\n",
      "Iteration 88, loss = 0.05517396\n",
      "Iteration 89, loss = 0.05457879\n",
      "Iteration 90, loss = 0.05409958\n",
      "Iteration 91, loss = 0.05362776\n",
      "Iteration 92, loss = 0.05314156\n",
      "Iteration 93, loss = 0.05272749\n",
      "Iteration 94, loss = 0.05212979\n",
      "Iteration 95, loss = 0.05169878\n",
      "Iteration 96, loss = 0.05124697\n",
      "Iteration 97, loss = 0.05079411\n",
      "Iteration 98, loss = 0.05036761\n",
      "Iteration 99, loss = 0.04992405\n",
      "Iteration 100, loss = 0.04953308\n",
      "Iteration 101, loss = 0.04912970\n",
      "Iteration 102, loss = 0.04869351\n",
      "Iteration 103, loss = 0.04828031\n",
      "Iteration 104, loss = 0.04794174\n",
      "Iteration 105, loss = 0.04742256\n",
      "Iteration 106, loss = 0.04703733\n",
      "Iteration 107, loss = 0.04670386\n",
      "Iteration 108, loss = 0.04635946\n",
      "Iteration 109, loss = 0.04598476\n",
      "Iteration 110, loss = 0.04572764\n",
      "Iteration 111, loss = 0.04526928\n",
      "Iteration 112, loss = 0.04487046\n",
      "Iteration 113, loss = 0.04460966\n",
      "Iteration 114, loss = 0.04419059\n",
      "Iteration 115, loss = 0.04392314\n",
      "Iteration 116, loss = 0.04360332\n",
      "Iteration 117, loss = 0.04336057\n",
      "Iteration 118, loss = 0.04291322\n",
      "Iteration 119, loss = 0.04262519\n",
      "Iteration 120, loss = 0.04226237\n",
      "Iteration 121, loss = 0.04196642\n",
      "Iteration 122, loss = 0.04164390\n",
      "Iteration 123, loss = 0.04136857\n",
      "Iteration 124, loss = 0.04105473\n",
      "Iteration 125, loss = 0.04088196\n",
      "Iteration 126, loss = 0.04050096\n",
      "Iteration 127, loss = 0.04024492\n",
      "Iteration 128, loss = 0.03995921\n",
      "Iteration 129, loss = 0.03969593\n",
      "Iteration 130, loss = 0.03933980\n",
      "Iteration 131, loss = 0.03904281\n",
      "Iteration 132, loss = 0.03891044\n",
      "Iteration 133, loss = 0.03851887\n",
      "Iteration 134, loss = 0.03825636\n",
      "Iteration 135, loss = 0.03797877\n",
      "Iteration 136, loss = 0.03768432\n",
      "Iteration 137, loss = 0.03756097\n",
      "Iteration 138, loss = 0.03732826\n",
      "Iteration 139, loss = 0.03699805\n",
      "Iteration 140, loss = 0.03679288\n",
      "Iteration 141, loss = 0.03651975\n",
      "Iteration 142, loss = 0.03627997\n",
      "Iteration 143, loss = 0.03604779\n",
      "Iteration 144, loss = 0.03591423\n",
      "Iteration 145, loss = 0.03564949\n",
      "Iteration 146, loss = 0.03539559\n",
      "Iteration 147, loss = 0.03523378\n",
      "Iteration 148, loss = 0.03513602\n",
      "Iteration 149, loss = 0.03481267\n",
      "Iteration 150, loss = 0.03461080\n",
      "Iteration 151, loss = 0.03440325\n",
      "Iteration 152, loss = 0.03421943\n",
      "Iteration 153, loss = 0.03399654\n",
      "Iteration 154, loss = 0.03386354\n",
      "Iteration 155, loss = 0.03367539\n",
      "Iteration 156, loss = 0.03343098\n",
      "Iteration 157, loss = 0.03328294\n",
      "Iteration 158, loss = 0.03304874\n",
      "Iteration 159, loss = 0.03282404\n",
      "Iteration 160, loss = 0.03270646\n",
      "Iteration 161, loss = 0.03258521\n",
      "Iteration 162, loss = 0.03235715\n",
      "Iteration 163, loss = 0.03220976\n",
      "Iteration 164, loss = 0.03197604\n",
      "Iteration 165, loss = 0.03186589\n",
      "Iteration 166, loss = 0.03168565\n",
      "Iteration 167, loss = 0.03146079\n",
      "Iteration 168, loss = 0.03130467\n",
      "Iteration 169, loss = 0.03117510\n",
      "Iteration 170, loss = 0.03099166\n",
      "Iteration 171, loss = 0.03081788\n",
      "Iteration 172, loss = 0.03069240\n",
      "Iteration 173, loss = 0.03048266\n",
      "Iteration 174, loss = 0.03037932\n",
      "Iteration 175, loss = 0.03025422\n",
      "Iteration 176, loss = 0.03004686\n",
      "Iteration 177, loss = 0.02998858\n",
      "Iteration 178, loss = 0.02971623\n",
      "Iteration 179, loss = 0.02968405\n",
      "Iteration 180, loss = 0.02952442\n",
      "Iteration 181, loss = 0.02928667\n",
      "Iteration 182, loss = 0.02918514\n",
      "Iteration 183, loss = 0.02896798\n",
      "Iteration 184, loss = 0.02891399\n",
      "Iteration 185, loss = 0.02871376\n",
      "Iteration 186, loss = 0.02865299\n",
      "Iteration 187, loss = 0.02844950\n",
      "Iteration 188, loss = 0.02832191\n",
      "Iteration 189, loss = 0.02819828\n",
      "Iteration 190, loss = 0.02805087\n",
      "Iteration 191, loss = 0.02791330\n",
      "Iteration 192, loss = 0.02778679\n",
      "Iteration 193, loss = 0.02765151\n",
      "Iteration 194, loss = 0.02764672\n",
      "Iteration 195, loss = 0.02738652\n",
      "Iteration 196, loss = 0.02729804\n",
      "Iteration 197, loss = 0.02714462\n",
      "Iteration 198, loss = 0.02711998\n",
      "Iteration 199, loss = 0.02694243\n",
      "Iteration 200, loss = 0.02691024\n",
      "Iteration 201, loss = 0.02680088\n",
      "Iteration 202, loss = 0.02653851\n",
      "Iteration 203, loss = 0.02639599\n",
      "Iteration 204, loss = 0.02633883\n",
      "Iteration 205, loss = 0.02620500\n",
      "Iteration 206, loss = 0.02607232\n",
      "Iteration 207, loss = 0.02600775\n",
      "Iteration 208, loss = 0.02587635\n",
      "Iteration 209, loss = 0.02572913\n",
      "Iteration 210, loss = 0.02573994\n",
      "Iteration 211, loss = 0.02557028\n",
      "Iteration 212, loss = 0.02545365\n",
      "Iteration 213, loss = 0.02534475\n",
      "Iteration 214, loss = 0.02523649\n",
      "Iteration 215, loss = 0.02507581\n",
      "Iteration 216, loss = 0.02497564\n",
      "Iteration 217, loss = 0.02486320\n",
      "Iteration 218, loss = 0.02478411\n",
      "Iteration 219, loss = 0.02464015\n",
      "Iteration 220, loss = 0.02454836\n",
      "Iteration 221, loss = 0.02450529\n",
      "Iteration 222, loss = 0.02436717\n",
      "Iteration 223, loss = 0.02432744\n",
      "Iteration 224, loss = 0.02422342\n",
      "Iteration 225, loss = 0.02410492\n",
      "Iteration 226, loss = 0.02402007\n",
      "Iteration 227, loss = 0.02389186\n",
      "Iteration 228, loss = 0.02382408\n",
      "Iteration 229, loss = 0.02368729\n",
      "Iteration 230, loss = 0.02358118\n",
      "Iteration 231, loss = 0.02355373\n",
      "Iteration 232, loss = 0.02353778\n",
      "Iteration 233, loss = 0.02331271\n",
      "Iteration 234, loss = 0.02322518\n",
      "Iteration 235, loss = 0.02317064\n",
      "Iteration 236, loss = 0.02311881\n",
      "Iteration 237, loss = 0.02296116\n",
      "Iteration 238, loss = 0.02290858\n",
      "Iteration 239, loss = 0.02279523\n",
      "Iteration 240, loss = 0.02273309\n",
      "Iteration 241, loss = 0.02262752\n",
      "Iteration 242, loss = 0.02258854\n",
      "Iteration 243, loss = 0.02245586\n",
      "Iteration 244, loss = 0.02234565\n",
      "Iteration 245, loss = 0.02226276\n",
      "Iteration 246, loss = 0.02223610\n",
      "Iteration 247, loss = 0.02210613\n",
      "Iteration 248, loss = 0.02200733\n",
      "Iteration 249, loss = 0.02194146\n",
      "Iteration 250, loss = 0.02182026\n",
      "Iteration 251, loss = 0.02181592\n",
      "Iteration 252, loss = 0.02172648\n",
      "Iteration 253, loss = 0.02161328\n",
      "Iteration 254, loss = 0.02151199\n",
      "Iteration 255, loss = 0.02143841\n",
      "Iteration 256, loss = 0.02134504\n",
      "Iteration 257, loss = 0.02124437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 258, loss = 0.02124530\n",
      "Iteration 259, loss = 0.02120389\n",
      "Iteration 260, loss = 0.02101283\n",
      "Iteration 261, loss = 0.02121772\n",
      "Iteration 262, loss = 0.02094292\n",
      "Iteration 263, loss = 0.02081774\n",
      "Iteration 264, loss = 0.02075726\n",
      "Iteration 265, loss = 0.02065830\n",
      "Iteration 266, loss = 0.02057521\n",
      "Iteration 267, loss = 0.02052155\n",
      "Iteration 268, loss = 0.02046984\n",
      "Iteration 269, loss = 0.02038362\n",
      "Iteration 270, loss = 0.02034945\n",
      "Iteration 271, loss = 0.02021299\n",
      "Iteration 272, loss = 0.02019727\n",
      "Iteration 273, loss = 0.02014079\n",
      "Iteration 274, loss = 0.02004916\n",
      "Iteration 275, loss = 0.01995959\n",
      "Iteration 276, loss = 0.01989324\n",
      "Iteration 277, loss = 0.01983600\n",
      "Iteration 278, loss = 0.01979536\n",
      "Iteration 279, loss = 0.01979396\n",
      "Iteration 280, loss = 0.01961007\n",
      "Iteration 281, loss = 0.01960629\n",
      "Iteration 282, loss = 0.01954605\n",
      "Iteration 283, loss = 0.01953662\n",
      "Iteration 284, loss = 0.01952030\n",
      "Iteration 285, loss = 0.01931645\n",
      "Iteration 286, loss = 0.01920651\n",
      "Iteration 287, loss = 0.01934333\n",
      "Iteration 288, loss = 0.01905659\n",
      "Iteration 289, loss = 0.01904201\n",
      "Iteration 290, loss = 0.01908603\n",
      "Iteration 291, loss = 0.01886697\n",
      "Iteration 292, loss = 0.01888776\n",
      "Iteration 293, loss = 0.01889412\n",
      "Iteration 294, loss = 0.01878340\n",
      "Iteration 295, loss = 0.01873967\n",
      "Iteration 296, loss = 0.01861203\n",
      "Iteration 297, loss = 0.01853182\n",
      "Iteration 298, loss = 0.01846244\n",
      "Iteration 299, loss = 0.01855392\n",
      "Iteration 300, loss = 0.01836266\n",
      "Iteration 301, loss = 0.01834617\n",
      "Iteration 302, loss = 0.01824174\n",
      "Iteration 303, loss = 0.01818764\n",
      "Iteration 304, loss = 0.01819524\n",
      "Iteration 305, loss = 0.01807729\n",
      "Iteration 306, loss = 0.01801015\n",
      "Iteration 307, loss = 0.01796301\n",
      "Iteration 308, loss = 0.01786077\n",
      "Iteration 309, loss = 0.01780715\n",
      "Iteration 310, loss = 0.01778967\n",
      "Iteration 311, loss = 0.01768651\n",
      "Iteration 312, loss = 0.01770167\n",
      "Iteration 313, loss = 0.01762199\n",
      "Iteration 314, loss = 0.01760881\n",
      "Iteration 315, loss = 0.01758712\n",
      "Iteration 316, loss = 0.01745858\n",
      "Iteration 317, loss = 0.01737027\n",
      "Iteration 318, loss = 0.01732327\n",
      "Iteration 319, loss = 0.01727540\n",
      "Iteration 320, loss = 0.01725603\n",
      "Iteration 321, loss = 0.01717581\n",
      "Iteration 322, loss = 0.01710297\n",
      "Iteration 323, loss = 0.01724601\n",
      "Iteration 324, loss = 0.01699678\n",
      "Iteration 325, loss = 0.01698366\n",
      "Iteration 326, loss = 0.01695725\n",
      "Iteration 327, loss = 0.01687684\n",
      "Iteration 328, loss = 0.01685144\n",
      "Iteration 329, loss = 0.01694604\n",
      "Iteration 330, loss = 0.01691335\n",
      "Iteration 331, loss = 0.01675009\n",
      "Iteration 332, loss = 0.01664018\n",
      "Iteration 333, loss = 0.01660347\n",
      "Iteration 334, loss = 0.01662283\n",
      "Iteration 335, loss = 0.01645116\n",
      "Iteration 336, loss = 0.01641544\n",
      "Iteration 337, loss = 0.01644205\n",
      "Iteration 338, loss = 0.01637019\n",
      "Iteration 339, loss = 0.01629020\n",
      "Iteration 340, loss = 0.01631712\n",
      "Iteration 341, loss = 0.01615747\n",
      "Iteration 342, loss = 0.01617649\n",
      "Iteration 343, loss = 0.01634586\n",
      "Iteration 344, loss = 0.01604844\n",
      "Iteration 345, loss = 0.01604888\n",
      "Iteration 346, loss = 0.01602159\n",
      "Iteration 347, loss = 0.01600650\n",
      "Iteration 348, loss = 0.01604574\n",
      "Iteration 349, loss = 0.01582805\n",
      "Iteration 350, loss = 0.01583415\n",
      "Iteration 351, loss = 0.01571316\n",
      "Iteration 352, loss = 0.01567706\n",
      "Iteration 353, loss = 0.01574503\n",
      "Iteration 354, loss = 0.01557722\n",
      "Iteration 355, loss = 0.01555110\n",
      "Iteration 356, loss = 0.01551114\n",
      "Iteration 357, loss = 0.01549054\n",
      "Iteration 358, loss = 0.01554390\n",
      "Iteration 359, loss = 0.01544209\n",
      "Iteration 360, loss = 0.01539718\n",
      "Iteration 361, loss = 0.01532366\n",
      "Iteration 362, loss = 0.01524519\n",
      "Iteration 363, loss = 0.01527095\n",
      "Iteration 364, loss = 0.01520293\n",
      "Iteration 365, loss = 0.01521406\n",
      "Iteration 366, loss = 0.01520608\n",
      "Iteration 367, loss = 0.01505859\n",
      "Iteration 368, loss = 0.01502297\n",
      "Iteration 369, loss = 0.01495338\n",
      "Iteration 370, loss = 0.01494044\n",
      "Iteration 371, loss = 0.01489388\n",
      "Iteration 372, loss = 0.01491997\n",
      "Iteration 373, loss = 0.01487668\n",
      "Iteration 374, loss = 0.01486227\n",
      "Iteration 375, loss = 0.01496725\n",
      "Iteration 376, loss = 0.01471366\n",
      "Iteration 377, loss = 0.01469280\n",
      "Iteration 378, loss = 0.01465623\n",
      "Iteration 379, loss = 0.01455940\n",
      "Iteration 380, loss = 0.01453465\n",
      "Iteration 381, loss = 0.01461986\n",
      "Iteration 382, loss = 0.01447335\n",
      "Iteration 383, loss = 0.01442885\n",
      "Iteration 384, loss = 0.01442763\n",
      "Iteration 385, loss = 0.01438466\n",
      "Iteration 386, loss = 0.01435118\n",
      "Iteration 387, loss = 0.01426051\n",
      "Iteration 388, loss = 0.01428142\n",
      "Iteration 389, loss = 0.01421327\n",
      "Iteration 390, loss = 0.01416651\n",
      "Iteration 391, loss = 0.01420716\n",
      "Iteration 392, loss = 0.01406469\n",
      "Iteration 393, loss = 0.01409391\n",
      "Iteration 394, loss = 0.01402058\n",
      "Iteration 395, loss = 0.01405490\n",
      "Iteration 396, loss = 0.01400073\n",
      "Iteration 397, loss = 0.01400495\n",
      "Iteration 398, loss = 0.01386713\n",
      "Iteration 399, loss = 0.01389188\n",
      "Iteration 400, loss = 0.01382913\n",
      "Iteration 401, loss = 0.01380938\n",
      "Iteration 402, loss = 0.01382186\n",
      "Iteration 403, loss = 0.01369420\n",
      "Iteration 404, loss = 0.01372887\n",
      "Iteration 405, loss = 0.01368872\n",
      "Iteration 406, loss = 0.01360559\n",
      "Iteration 407, loss = 0.01355507\n",
      "Iteration 408, loss = 0.01356050\n",
      "Iteration 409, loss = 0.01355290\n",
      "Iteration 410, loss = 0.01355001\n",
      "Iteration 411, loss = 0.01350667\n",
      "Iteration 412, loss = 0.01336939\n",
      "Iteration 413, loss = 0.01349071\n",
      "Iteration 414, loss = 0.01332348\n",
      "Iteration 415, loss = 0.01351772\n",
      "Iteration 416, loss = 0.01335567\n",
      "Iteration 417, loss = 0.01332723\n",
      "Iteration 418, loss = 0.01330231\n",
      "Iteration 419, loss = 0.01320571\n",
      "Iteration 420, loss = 0.01318048\n",
      "Iteration 421, loss = 0.01307023\n",
      "Iteration 422, loss = 0.01311842\n",
      "Iteration 423, loss = 0.01306863\n",
      "Iteration 424, loss = 0.01298304\n",
      "Iteration 425, loss = 0.01300077\n",
      "Iteration 426, loss = 0.01302214\n",
      "Iteration 427, loss = 0.01289733\n",
      "Iteration 428, loss = 0.01293477\n",
      "Iteration 429, loss = 0.01285668\n",
      "Iteration 430, loss = 0.01285115\n",
      "Iteration 431, loss = 0.01285910\n",
      "Iteration 432, loss = 0.01281089\n",
      "Iteration 433, loss = 0.01271375\n",
      "Iteration 434, loss = 0.01277815\n",
      "Iteration 435, loss = 0.01268395\n",
      "Iteration 436, loss = 0.01268413\n",
      "Iteration 437, loss = 0.01258419\n",
      "Iteration 438, loss = 0.01256736\n",
      "Iteration 439, loss = 0.01256701\n",
      "Iteration 440, loss = 0.01254667\n",
      "Iteration 441, loss = 0.01246101\n",
      "Iteration 442, loss = 0.01260241\n",
      "Iteration 443, loss = 0.01244291\n",
      "Iteration 444, loss = 0.01243473\n",
      "Iteration 445, loss = 0.01239801\n",
      "Iteration 446, loss = 0.01234594\n",
      "Iteration 447, loss = 0.01236812\n",
      "Iteration 448, loss = 0.01227800\n",
      "Iteration 449, loss = 0.01224768\n",
      "Iteration 450, loss = 0.01224537\n",
      "Iteration 451, loss = 0.01219657\n",
      "Iteration 452, loss = 0.01221444\n",
      "Iteration 453, loss = 0.01219118\n",
      "Iteration 454, loss = 0.01217753\n",
      "Iteration 455, loss = 0.01210299\n",
      "Iteration 456, loss = 0.01205722\n",
      "Iteration 457, loss = 0.01204706\n",
      "Iteration 458, loss = 0.01206200\n",
      "Iteration 459, loss = 0.01203137\n",
      "Iteration 460, loss = 0.01196052\n",
      "Iteration 461, loss = 0.01196628\n",
      "Iteration 462, loss = 0.01192102\n",
      "Iteration 463, loss = 0.01192952\n",
      "Iteration 464, loss = 0.01181247\n",
      "Iteration 465, loss = 0.01184781\n",
      "Iteration 466, loss = 0.01176814\n",
      "Iteration 467, loss = 0.01179841\n",
      "Iteration 468, loss = 0.01180438\n",
      "Iteration 469, loss = 0.01168596\n",
      "Iteration 470, loss = 0.01174684\n",
      "Iteration 471, loss = 0.01174674\n",
      "Iteration 472, loss = 0.01170582\n",
      "Iteration 473, loss = 0.01159512\n",
      "Iteration 474, loss = 0.01169365\n",
      "Iteration 475, loss = 0.01154108\n",
      "Iteration 476, loss = 0.01148375\n",
      "Iteration 477, loss = 0.01150038\n",
      "Iteration 478, loss = 0.01147269\n",
      "Iteration 479, loss = 0.01149331\n",
      "Iteration 480, loss = 0.01143967\n",
      "Iteration 481, loss = 0.01142221\n",
      "Iteration 482, loss = 0.01145005\n",
      "Iteration 483, loss = 0.01152079\n",
      "Iteration 484, loss = 0.01140211\n",
      "Iteration 485, loss = 0.01133575\n",
      "Iteration 486, loss = 0.01139189\n",
      "Iteration 487, loss = 0.01133695\n",
      "Iteration 488, loss = 0.01129001\n",
      "Iteration 489, loss = 0.01122083\n",
      "Iteration 490, loss = 0.01122415\n",
      "Iteration 491, loss = 0.01114798\n",
      "Iteration 492, loss = 0.01112183\n",
      "Iteration 493, loss = 0.01119553\n",
      "Iteration 494, loss = 0.01115074\n",
      "Iteration 495, loss = 0.01106498\n",
      "Iteration 496, loss = 0.01105886\n",
      "Iteration 497, loss = 0.01098684\n",
      "Iteration 498, loss = 0.01111552\n",
      "Iteration 499, loss = 0.01104013\n",
      "Iteration 500, loss = 0.01099175\n",
      "Iteration 501, loss = 0.01099542\n",
      "Iteration 502, loss = 0.01098730\n",
      "Iteration 503, loss = 0.01098925\n",
      "Iteration 504, loss = 0.01084951\n",
      "Iteration 505, loss = 0.01082951\n",
      "Iteration 506, loss = 0.01080355\n",
      "Iteration 507, loss = 0.01078279\n",
      "Iteration 508, loss = 0.01087082\n",
      "Iteration 509, loss = 0.01073978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 510, loss = 0.01072634\n",
      "Iteration 511, loss = 0.01078513\n",
      "Iteration 512, loss = 0.01068270\n",
      "Iteration 513, loss = 0.01070182\n",
      "Iteration 514, loss = 0.01065191\n",
      "Iteration 515, loss = 0.01065291\n",
      "Iteration 516, loss = 0.01061266\n",
      "Iteration 517, loss = 0.01058866\n",
      "Iteration 518, loss = 0.01058577\n",
      "Iteration 519, loss = 0.01048536\n",
      "Iteration 520, loss = 0.01062905\n",
      "Iteration 521, loss = 0.01051613\n",
      "Iteration 522, loss = 0.01050602\n",
      "Iteration 523, loss = 0.01043800\n",
      "Iteration 524, loss = 0.01040277\n",
      "Iteration 525, loss = 0.01044451\n",
      "Iteration 526, loss = 0.01037063\n",
      "Iteration 527, loss = 0.01032827\n",
      "Iteration 528, loss = 0.01030968\n",
      "Iteration 529, loss = 0.01031671\n",
      "Iteration 530, loss = 0.01029189\n",
      "Iteration 531, loss = 0.01024443\n",
      "Iteration 532, loss = 0.01027082\n",
      "Iteration 533, loss = 0.01021082\n",
      "Iteration 534, loss = 0.01017114\n",
      "Iteration 535, loss = 0.01018910\n",
      "Iteration 536, loss = 0.01024445\n",
      "Iteration 537, loss = 0.01041139\n",
      "Iteration 538, loss = 0.01011796\n",
      "Iteration 539, loss = 0.01005544\n",
      "Iteration 540, loss = 0.01009944\n",
      "Iteration 541, loss = 0.01009691\n",
      "Iteration 542, loss = 0.01000106\n",
      "Iteration 543, loss = 0.00999079\n",
      "Iteration 544, loss = 0.01003105\n",
      "Iteration 545, loss = 0.00998046\n",
      "Iteration 546, loss = 0.00997073\n",
      "Iteration 547, loss = 0.00999820\n",
      "Iteration 548, loss = 0.00997030\n",
      "Iteration 549, loss = 0.00992559\n",
      "Iteration 550, loss = 0.00986013\n",
      "Iteration 551, loss = 0.00994870\n",
      "Iteration 552, loss = 0.00982400\n",
      "Iteration 553, loss = 0.00989592\n",
      "Iteration 554, loss = 0.00977193\n",
      "Iteration 555, loss = 0.00978323\n",
      "Iteration 556, loss = 0.00980284\n",
      "Iteration 557, loss = 0.00969335\n",
      "Iteration 558, loss = 0.00975480\n",
      "Iteration 559, loss = 0.00976128\n",
      "Iteration 560, loss = 0.00978150\n",
      "Iteration 561, loss = 0.00963182\n",
      "Iteration 562, loss = 0.00993292\n",
      "Iteration 563, loss = 0.00961895\n",
      "Iteration 564, loss = 0.00959052\n",
      "Iteration 565, loss = 0.00964960\n",
      "Iteration 566, loss = 0.00971608\n",
      "Iteration 567, loss = 0.00970493\n",
      "Iteration 568, loss = 0.00952516\n",
      "Iteration 569, loss = 0.00954577\n",
      "Iteration 570, loss = 0.00947175\n",
      "Iteration 571, loss = 0.00950903\n",
      "Iteration 572, loss = 0.00952406\n",
      "Iteration 573, loss = 0.00940920\n",
      "Iteration 574, loss = 0.00943000\n",
      "Iteration 575, loss = 0.00946015\n",
      "Iteration 576, loss = 0.00946660\n",
      "Iteration 577, loss = 0.00940913\n",
      "Iteration 578, loss = 0.00942750\n",
      "Iteration 579, loss = 0.00939911\n",
      "Iteration 580, loss = 0.00928809\n",
      "Iteration 581, loss = 0.00931111\n",
      "Iteration 582, loss = 0.00926566\n",
      "Iteration 583, loss = 0.00945349\n",
      "Iteration 584, loss = 0.00921520\n",
      "Iteration 585, loss = 0.00919692\n",
      "Iteration 586, loss = 0.00920468\n",
      "Iteration 587, loss = 0.00930020\n",
      "Iteration 588, loss = 0.00918686\n",
      "Iteration 589, loss = 0.00932613\n",
      "Iteration 590, loss = 0.00917346\n",
      "Iteration 591, loss = 0.00911600\n",
      "Iteration 592, loss = 0.00909893\n",
      "Iteration 593, loss = 0.00912823\n",
      "Iteration 594, loss = 0.00907966\n",
      "Iteration 595, loss = 0.00902918\n",
      "Iteration 596, loss = 0.00900251\n",
      "Iteration 597, loss = 0.00899406\n",
      "Iteration 598, loss = 0.00896339\n",
      "Iteration 599, loss = 0.00900059\n",
      "Iteration 600, loss = 0.00890263\n",
      "Iteration 601, loss = 0.00893536\n",
      "Iteration 602, loss = 0.00894748\n",
      "Iteration 603, loss = 0.00889584\n",
      "Iteration 604, loss = 0.00903387\n",
      "Iteration 605, loss = 0.00884629\n",
      "Iteration 606, loss = 0.00899194\n",
      "Iteration 607, loss = 0.00887539\n",
      "Iteration 608, loss = 0.00893834\n",
      "Iteration 609, loss = 0.00879683\n",
      "Iteration 610, loss = 0.00878679\n",
      "Iteration 611, loss = 0.00877076\n",
      "Iteration 612, loss = 0.00877173\n",
      "Iteration 613, loss = 0.00877995\n",
      "Iteration 614, loss = 0.00883415\n",
      "Iteration 615, loss = 0.00869734\n",
      "Iteration 616, loss = 0.00867709\n",
      "Iteration 617, loss = 0.00866833\n",
      "Iteration 618, loss = 0.00870005\n",
      "Iteration 619, loss = 0.00873465\n",
      "Iteration 620, loss = 0.00863981\n",
      "Iteration 621, loss = 0.00867918\n",
      "Iteration 622, loss = 0.00860562\n",
      "Iteration 623, loss = 0.00860727\n",
      "Iteration 624, loss = 0.00866407\n",
      "Iteration 625, loss = 0.00853856\n",
      "Iteration 626, loss = 0.00861786\n",
      "Iteration 627, loss = 0.00854718\n",
      "Iteration 628, loss = 0.00850810\n",
      "Iteration 629, loss = 0.00856125\n",
      "Iteration 630, loss = 0.00855145\n",
      "Iteration 631, loss = 0.00852029\n",
      "Iteration 632, loss = 0.00852348\n",
      "Iteration 633, loss = 0.00843972\n",
      "Iteration 634, loss = 0.00846845\n",
      "Iteration 635, loss = 0.00841032\n",
      "Iteration 636, loss = 0.00838563\n",
      "Iteration 637, loss = 0.00843731\n",
      "Iteration 638, loss = 0.00834312\n",
      "Iteration 639, loss = 0.00833477\n",
      "Iteration 640, loss = 0.00844434\n",
      "Iteration 641, loss = 0.00832461\n",
      "Iteration 642, loss = 0.00837413\n",
      "Iteration 643, loss = 0.00829914\n",
      "Iteration 644, loss = 0.00829997\n",
      "Iteration 645, loss = 0.00826267\n",
      "Iteration 646, loss = 0.00827446\n",
      "Iteration 647, loss = 0.00820796\n",
      "Iteration 648, loss = 0.00826141\n",
      "Iteration 649, loss = 0.00819439\n",
      "Iteration 650, loss = 0.00821149\n",
      "Iteration 651, loss = 0.00818416\n",
      "Iteration 652, loss = 0.00825563\n",
      "Iteration 653, loss = 0.00820973\n",
      "Iteration 654, loss = 0.00821447\n",
      "Iteration 655, loss = 0.00808124\n",
      "Iteration 656, loss = 0.00821437\n",
      "Iteration 657, loss = 0.00804025\n",
      "Iteration 658, loss = 0.00807510\n",
      "Iteration 659, loss = 0.00813535\n",
      "Iteration 660, loss = 0.00807278\n",
      "Iteration 661, loss = 0.00803973\n",
      "Iteration 662, loss = 0.00798350\n",
      "Iteration 663, loss = 0.00795136\n",
      "Iteration 664, loss = 0.00801510\n",
      "Iteration 665, loss = 0.00795136\n",
      "Iteration 666, loss = 0.00794113\n",
      "Iteration 667, loss = 0.00803488\n",
      "Iteration 668, loss = 0.00796113\n",
      "Iteration 669, loss = 0.00793735\n",
      "Iteration 670, loss = 0.00793409\n",
      "Iteration 671, loss = 0.00788730\n",
      "Iteration 672, loss = 0.00783578\n",
      "Iteration 673, loss = 0.00787977\n",
      "Iteration 674, loss = 0.00786079\n",
      "Iteration 675, loss = 0.00786223\n",
      "Iteration 676, loss = 0.00792830\n",
      "Iteration 677, loss = 0.00782837\n",
      "Iteration 678, loss = 0.00793355\n",
      "Iteration 679, loss = 0.00776361\n",
      "Iteration 680, loss = 0.00798104\n",
      "Iteration 681, loss = 0.00779854\n",
      "Iteration 682, loss = 0.00777802\n",
      "Iteration 683, loss = 0.00774832\n",
      "Iteration 684, loss = 0.00771228\n",
      "Iteration 685, loss = 0.00775344\n",
      "Iteration 686, loss = 0.00774548\n",
      "Iteration 687, loss = 0.00768478\n",
      "Iteration 688, loss = 0.00769632\n",
      "Iteration 689, loss = 0.00788679\n",
      "Iteration 690, loss = 0.00767307\n",
      "Iteration 691, loss = 0.00760168\n",
      "Iteration 692, loss = 0.00761229\n",
      "Iteration 693, loss = 0.00757083\n",
      "Iteration 694, loss = 0.00758160\n",
      "Iteration 695, loss = 0.00764936\n",
      "Iteration 696, loss = 0.00761012\n",
      "Iteration 697, loss = 0.00756285\n",
      "Iteration 698, loss = 0.00754235\n",
      "Iteration 699, loss = 0.00757470\n",
      "Iteration 700, loss = 0.00752981\n",
      "Iteration 701, loss = 0.00748542\n",
      "Iteration 702, loss = 0.00749257\n",
      "Iteration 703, loss = 0.00745365\n",
      "Iteration 704, loss = 0.00752318\n",
      "Iteration 705, loss = 0.00749218\n",
      "Iteration 706, loss = 0.00762202\n",
      "Iteration 707, loss = 0.00737374\n",
      "Iteration 708, loss = 0.00742816\n",
      "Iteration 709, loss = 0.00743603\n",
      "Iteration 710, loss = 0.00738064\n",
      "Iteration 711, loss = 0.00737722\n",
      "Iteration 712, loss = 0.00737522\n",
      "Iteration 713, loss = 0.00731873\n",
      "Iteration 714, loss = 0.00732523\n",
      "Iteration 715, loss = 0.00734965\n",
      "Iteration 716, loss = 0.00730656\n",
      "Iteration 717, loss = 0.00739300\n",
      "Iteration 718, loss = 0.00731720\n",
      "Iteration 719, loss = 0.00730376\n",
      "Iteration 720, loss = 0.00732123\n",
      "Iteration 721, loss = 0.00732297\n",
      "Iteration 722, loss = 0.00733197\n",
      "Iteration 723, loss = 0.00723672\n",
      "Iteration 724, loss = 0.00724600\n",
      "Iteration 725, loss = 0.00730979\n",
      "Iteration 726, loss = 0.00734775\n",
      "Iteration 727, loss = 0.00715908\n",
      "Iteration 728, loss = 0.00715414\n",
      "Iteration 729, loss = 0.00723813\n",
      "Iteration 730, loss = 0.00718224\n",
      "Iteration 731, loss = 0.00721907\n",
      "Iteration 732, loss = 0.00729737\n",
      "Iteration 733, loss = 0.00721196\n",
      "Iteration 734, loss = 0.00733502\n",
      "Iteration 735, loss = 0.00707701\n",
      "Iteration 736, loss = 0.00708141\n",
      "Iteration 737, loss = 0.00705240\n",
      "Iteration 738, loss = 0.00711000\n",
      "Iteration 739, loss = 0.00702056\n",
      "Iteration 740, loss = 0.00715050\n",
      "Iteration 741, loss = 0.00710041\n",
      "Iteration 742, loss = 0.00706839\n",
      "Iteration 743, loss = 0.00700672\n",
      "Iteration 744, loss = 0.00705508\n",
      "Iteration 745, loss = 0.00699801\n",
      "Iteration 746, loss = 0.00701884\n",
      "Iteration 747, loss = 0.00706378\n",
      "Iteration 748, loss = 0.00693875\n",
      "Iteration 749, loss = 0.00694845\n",
      "Iteration 750, loss = 0.00695847\n",
      "Iteration 751, loss = 0.00693643\n",
      "Iteration 752, loss = 0.00691025\n",
      "Iteration 753, loss = 0.00688287\n",
      "Iteration 754, loss = 0.00692366\n",
      "Iteration 755, loss = 0.00684885\n",
      "Iteration 756, loss = 0.00682984\n",
      "Iteration 757, loss = 0.00688860\n",
      "Iteration 758, loss = 0.00680872\n",
      "Iteration 759, loss = 0.00681220\n",
      "Iteration 760, loss = 0.00678751\n",
      "Iteration 761, loss = 0.00680651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 762, loss = 0.00683960\n",
      "Iteration 763, loss = 0.00682946\n",
      "Iteration 764, loss = 0.00678880\n",
      "Iteration 765, loss = 0.00675095\n",
      "Iteration 766, loss = 0.00681290\n",
      "Iteration 767, loss = 0.00701183\n",
      "Iteration 768, loss = 0.00673667\n",
      "Iteration 769, loss = 0.00674469\n",
      "Iteration 770, loss = 0.00683242\n",
      "Iteration 771, loss = 0.00669930\n",
      "Iteration 772, loss = 0.00676978\n",
      "Iteration 773, loss = 0.00668076\n",
      "Iteration 774, loss = 0.00669861\n",
      "Iteration 775, loss = 0.00666716\n",
      "Iteration 776, loss = 0.00667429\n",
      "Iteration 777, loss = 0.00665057\n",
      "Iteration 778, loss = 0.00670513\n",
      "Iteration 779, loss = 0.00661354\n",
      "Iteration 780, loss = 0.00673201\n",
      "Iteration 781, loss = 0.00670486\n",
      "Iteration 782, loss = 0.00669272\n",
      "Iteration 783, loss = 0.00658758\n",
      "Iteration 784, loss = 0.00664709\n",
      "Iteration 785, loss = 0.00657114\n",
      "Iteration 786, loss = 0.00656996\n",
      "Iteration 787, loss = 0.00649654\n",
      "Iteration 788, loss = 0.00651472\n",
      "Iteration 789, loss = 0.00648281\n",
      "Iteration 790, loss = 0.00654872\n",
      "Iteration 791, loss = 0.00654076\n",
      "Iteration 792, loss = 0.00657966\n",
      "Iteration 793, loss = 0.00658119\n",
      "Iteration 794, loss = 0.00647955\n",
      "Iteration 795, loss = 0.00653954\n",
      "Iteration 796, loss = 0.00640564\n",
      "Iteration 797, loss = 0.00650014\n",
      "Iteration 798, loss = 0.00655622\n",
      "Iteration 799, loss = 0.00642852\n",
      "Iteration 800, loss = 0.00647230\n",
      "Iteration 801, loss = 0.00636509\n",
      "Iteration 802, loss = 0.00643743\n",
      "Iteration 803, loss = 0.00640353\n",
      "Iteration 804, loss = 0.00640849\n",
      "Iteration 805, loss = 0.00639495\n",
      "Iteration 806, loss = 0.00634810\n",
      "Iteration 807, loss = 0.00632390\n",
      "Iteration 808, loss = 0.00640821\n",
      "Iteration 809, loss = 0.00637589\n",
      "Iteration 810, loss = 0.00634278\n",
      "Iteration 811, loss = 0.00631858\n",
      "Iteration 812, loss = 0.00629077\n",
      "Iteration 813, loss = 0.00637850\n",
      "Iteration 814, loss = 0.00631100\n",
      "Iteration 815, loss = 0.00626363\n",
      "Iteration 816, loss = 0.00633470\n",
      "Iteration 817, loss = 0.00626839\n",
      "Iteration 818, loss = 0.00633587\n",
      "Iteration 819, loss = 0.00630146\n",
      "Iteration 820, loss = 0.00624013\n",
      "Iteration 821, loss = 0.00626997\n",
      "Iteration 822, loss = 0.00620135\n",
      "Iteration 823, loss = 0.00632324\n",
      "Iteration 824, loss = 0.00619176\n",
      "Iteration 825, loss = 0.00621282\n",
      "Iteration 826, loss = 0.00615755\n",
      "Iteration 827, loss = 0.00624518\n",
      "Iteration 828, loss = 0.00618128\n",
      "Iteration 829, loss = 0.00614878\n",
      "Iteration 830, loss = 0.00616974\n",
      "Iteration 831, loss = 0.00611178\n",
      "Iteration 832, loss = 0.00611594\n",
      "Iteration 833, loss = 0.00609918\n",
      "Iteration 834, loss = 0.00614526\n",
      "Iteration 835, loss = 0.00609036\n",
      "Iteration 836, loss = 0.00617488\n",
      "Iteration 837, loss = 0.00607557\n",
      "Iteration 838, loss = 0.00617610\n",
      "Iteration 839, loss = 0.00605912\n",
      "Iteration 840, loss = 0.00615869\n",
      "Iteration 841, loss = 0.00601985\n",
      "Iteration 842, loss = 0.00616571\n",
      "Iteration 843, loss = 0.00622373\n",
      "Iteration 844, loss = 0.00603015\n",
      "Iteration 845, loss = 0.00605159\n",
      "Iteration 846, loss = 0.00602625\n",
      "Iteration 847, loss = 0.00611506\n",
      "Iteration 848, loss = 0.00602287\n",
      "Iteration 849, loss = 0.00606248\n",
      "Iteration 850, loss = 0.00607910\n",
      "Iteration 851, loss = 0.00600726\n",
      "Iteration 852, loss = 0.00601429\n",
      "Iteration 853, loss = 0.00593877\n",
      "Iteration 854, loss = 0.00598268\n",
      "Iteration 855, loss = 0.00592763\n",
      "Iteration 856, loss = 0.00597962\n",
      "Iteration 857, loss = 0.00587220\n",
      "Iteration 858, loss = 0.00595194\n",
      "Iteration 859, loss = 0.00589501\n",
      "Iteration 860, loss = 0.00588155\n",
      "Iteration 861, loss = 0.00590865\n",
      "Iteration 862, loss = 0.00591652\n",
      "Iteration 863, loss = 0.00583979\n",
      "Iteration 864, loss = 0.00589858\n",
      "Iteration 865, loss = 0.00580254\n",
      "Iteration 866, loss = 0.00590516\n",
      "Iteration 867, loss = 0.00594500\n",
      "Iteration 868, loss = 0.00583610\n",
      "Iteration 869, loss = 0.00581715\n",
      "Iteration 870, loss = 0.00582820\n",
      "Iteration 871, loss = 0.00577634\n",
      "Iteration 872, loss = 0.00581067\n",
      "Iteration 873, loss = 0.00577142\n",
      "Iteration 874, loss = 0.00582907\n",
      "Iteration 875, loss = 0.00587166\n",
      "Iteration 876, loss = 0.00590507\n",
      "Iteration 877, loss = 0.00584838\n",
      "Iteration 878, loss = 0.00577863\n",
      "Iteration 879, loss = 0.00573876\n",
      "Iteration 880, loss = 0.00578043\n",
      "Iteration 881, loss = 0.00574257\n",
      "Iteration 882, loss = 0.00567039\n",
      "Iteration 883, loss = 0.00570744\n",
      "Iteration 884, loss = 0.00579946\n",
      "Iteration 885, loss = 0.00583793\n",
      "Iteration 886, loss = 0.00573149\n",
      "Iteration 887, loss = 0.00565779\n",
      "Iteration 888, loss = 0.00602788\n",
      "Iteration 889, loss = 0.00564424\n",
      "Iteration 890, loss = 0.00566062\n",
      "Iteration 891, loss = 0.00567884\n",
      "Iteration 892, loss = 0.00567247\n",
      "Iteration 893, loss = 0.00567087\n",
      "Iteration 894, loss = 0.00562034\n",
      "Iteration 895, loss = 0.00567868\n",
      "Iteration 896, loss = 0.00559953\n",
      "Iteration 897, loss = 0.00564353\n",
      "Iteration 898, loss = 0.00560360\n",
      "Iteration 899, loss = 0.00567896\n",
      "Iteration 900, loss = 0.00553931\n",
      "Iteration 901, loss = 0.00555791\n",
      "Iteration 902, loss = 0.00560192\n",
      "Iteration 903, loss = 0.00551871\n",
      "Iteration 904, loss = 0.00563162\n",
      "Iteration 905, loss = 0.00549646\n",
      "Iteration 906, loss = 0.00551490\n",
      "Iteration 907, loss = 0.00554673\n",
      "Iteration 908, loss = 0.00555026\n",
      "Iteration 909, loss = 0.00551869\n",
      "Iteration 910, loss = 0.00548454\n",
      "Iteration 911, loss = 0.00551331\n",
      "Iteration 912, loss = 0.00545676\n",
      "Iteration 913, loss = 0.00552250\n",
      "Iteration 914, loss = 0.00545603\n",
      "Iteration 915, loss = 0.00547846\n",
      "Iteration 916, loss = 0.00555866\n",
      "Iteration 917, loss = 0.00543688\n",
      "Iteration 918, loss = 0.00551786\n",
      "Iteration 919, loss = 0.00539506\n",
      "Iteration 920, loss = 0.00539280\n",
      "Iteration 921, loss = 0.00540135\n",
      "Iteration 922, loss = 0.00543718\n",
      "Iteration 923, loss = 0.00542120\n",
      "Iteration 924, loss = 0.00542252\n",
      "Iteration 925, loss = 0.00537347\n",
      "Iteration 926, loss = 0.00542567\n",
      "Iteration 927, loss = 0.00534344\n",
      "Iteration 928, loss = 0.00544126\n",
      "Iteration 929, loss = 0.00532804\n",
      "Iteration 930, loss = 0.00536062\n",
      "Iteration 931, loss = 0.00538440\n",
      "Iteration 932, loss = 0.00535469\n",
      "Iteration 933, loss = 0.00540075\n",
      "Iteration 934, loss = 0.00551049\n",
      "Iteration 935, loss = 0.00529933\n",
      "Iteration 936, loss = 0.00544031\n",
      "Iteration 937, loss = 0.00530340\n",
      "Iteration 938, loss = 0.00527662\n",
      "Iteration 939, loss = 0.00528786\n",
      "Iteration 940, loss = 0.00527007\n",
      "Iteration 941, loss = 0.00532575\n",
      "Iteration 942, loss = 0.00525144\n",
      "Iteration 943, loss = 0.00532262\n",
      "Iteration 944, loss = 0.00537560\n",
      "Iteration 945, loss = 0.00531247\n",
      "Iteration 946, loss = 0.00522886\n",
      "Iteration 947, loss = 0.00529791\n",
      "Iteration 948, loss = 0.00529731\n",
      "Iteration 949, loss = 0.00525101\n",
      "Iteration 950, loss = 0.00532399\n",
      "Iteration 951, loss = 0.00518632\n",
      "Iteration 952, loss = 0.00518029\n",
      "Iteration 953, loss = 0.00521206\n",
      "Iteration 954, loss = 0.00520200\n",
      "Iteration 955, loss = 0.00516412\n",
      "Iteration 956, loss = 0.00518762\n",
      "Iteration 957, loss = 0.00517083\n",
      "Iteration 958, loss = 0.00517423\n",
      "Iteration 959, loss = 0.00514424\n",
      "Iteration 960, loss = 0.00517921\n",
      "Iteration 961, loss = 0.00511778\n",
      "Iteration 962, loss = 0.00516871\n",
      "Iteration 963, loss = 0.00516332\n",
      "Iteration 964, loss = 0.00509001\n",
      "Iteration 965, loss = 0.00514527\n",
      "Iteration 966, loss = 0.00528272\n",
      "Iteration 967, loss = 0.00506636\n",
      "Iteration 968, loss = 0.00504110\n",
      "Iteration 969, loss = 0.00509396\n",
      "Iteration 970, loss = 0.00509479\n",
      "Iteration 971, loss = 0.00509294\n",
      "Iteration 972, loss = 0.00508566\n",
      "Iteration 973, loss = 0.00503389\n",
      "Iteration 974, loss = 0.00504559\n",
      "Iteration 975, loss = 0.00503042\n",
      "Iteration 976, loss = 0.00501838\n",
      "Iteration 977, loss = 0.00508695\n",
      "Iteration 978, loss = 0.00501720\n",
      "Iteration 979, loss = 0.00503251\n",
      "Iteration 980, loss = 0.00500587\n",
      "Iteration 981, loss = 0.00499692\n",
      "Iteration 982, loss = 0.00504494\n",
      "Iteration 983, loss = 0.00499655\n",
      "Iteration 984, loss = 0.00498603\n",
      "Iteration 985, loss = 0.00508536\n",
      "Iteration 986, loss = 0.00501928\n",
      "Iteration 987, loss = 0.00506308\n",
      "Iteration 988, loss = 0.00502664\n",
      "Iteration 989, loss = 0.00496614\n",
      "Iteration 990, loss = 0.00501388\n",
      "Iteration 991, loss = 0.00506488\n",
      "Iteration 992, loss = 0.00494572\n",
      "Iteration 993, loss = 0.00495147\n",
      "Iteration 994, loss = 0.00499336\n",
      "Iteration 995, loss = 0.00518605\n",
      "Iteration 996, loss = 0.00486873\n",
      "Iteration 997, loss = 0.00499692\n",
      "Iteration 998, loss = 0.00500430\n",
      "Iteration 999, loss = 0.00495542\n",
      "Iteration 1000, loss = 0.00496084\n",
      "Iteration 1, loss = 0.72327394\n",
      "Iteration 2, loss = 0.64080284\n",
      "Iteration 3, loss = 0.57116170\n",
      "Iteration 4, loss = 0.51495255\n",
      "Iteration 5, loss = 0.46742148\n",
      "Iteration 6, loss = 0.42825378\n",
      "Iteration 7, loss = 0.39422829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lindino/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.36479702\n",
      "Iteration 9, loss = 0.33885937\n",
      "Iteration 10, loss = 0.31560565\n",
      "Iteration 11, loss = 0.29487085\n",
      "Iteration 12, loss = 0.27600570\n",
      "Iteration 13, loss = 0.25922467\n",
      "Iteration 14, loss = 0.24392637\n",
      "Iteration 15, loss = 0.23026435\n",
      "Iteration 16, loss = 0.21772829\n",
      "Iteration 17, loss = 0.20660848\n",
      "Iteration 18, loss = 0.19654745\n",
      "Iteration 19, loss = 0.18752744\n",
      "Iteration 20, loss = 0.17930671\n",
      "Iteration 21, loss = 0.17196959\n",
      "Iteration 22, loss = 0.16538188\n",
      "Iteration 23, loss = 0.15933551\n",
      "Iteration 24, loss = 0.15378998\n",
      "Iteration 25, loss = 0.14869733\n",
      "Iteration 26, loss = 0.14405391\n",
      "Iteration 27, loss = 0.13983003\n",
      "Iteration 28, loss = 0.13583187\n",
      "Iteration 29, loss = 0.13229451\n",
      "Iteration 30, loss = 0.12883671\n",
      "Iteration 31, loss = 0.12562389\n",
      "Iteration 32, loss = 0.12269454\n",
      "Iteration 33, loss = 0.11990508\n",
      "Iteration 34, loss = 0.11726255\n",
      "Iteration 35, loss = 0.11494754\n",
      "Iteration 36, loss = 0.11261320\n",
      "Iteration 37, loss = 0.11037811\n",
      "Iteration 38, loss = 0.10830890\n",
      "Iteration 39, loss = 0.10639531\n",
      "Iteration 40, loss = 0.10447720\n",
      "Iteration 41, loss = 0.10280822\n",
      "Iteration 42, loss = 0.10107652\n",
      "Iteration 43, loss = 0.09956263\n",
      "Iteration 44, loss = 0.09800918\n",
      "Iteration 45, loss = 0.09654001\n",
      "Iteration 46, loss = 0.09503528\n",
      "Iteration 47, loss = 0.09367575\n",
      "Iteration 48, loss = 0.09239872\n",
      "Iteration 49, loss = 0.09117381\n",
      "Iteration 50, loss = 0.08989115\n",
      "Iteration 51, loss = 0.08863080\n",
      "Iteration 52, loss = 0.08749973\n",
      "Iteration 53, loss = 0.08628886\n",
      "Iteration 54, loss = 0.08515757\n",
      "Iteration 55, loss = 0.08417891\n",
      "Iteration 56, loss = 0.08311679\n",
      "Iteration 57, loss = 0.08208441\n",
      "Iteration 58, loss = 0.08111866\n",
      "Iteration 59, loss = 0.08017887\n",
      "Iteration 60, loss = 0.07924069\n",
      "Iteration 61, loss = 0.07836839\n",
      "Iteration 62, loss = 0.07762528\n",
      "Iteration 63, loss = 0.07673482\n",
      "Iteration 64, loss = 0.07589031\n",
      "Iteration 65, loss = 0.07507533\n",
      "Iteration 66, loss = 0.07438168\n",
      "Iteration 67, loss = 0.07354190\n",
      "Iteration 68, loss = 0.07290142\n",
      "Iteration 69, loss = 0.07219913\n",
      "Iteration 70, loss = 0.07150028\n",
      "Iteration 71, loss = 0.07084772\n",
      "Iteration 72, loss = 0.07005356\n",
      "Iteration 73, loss = 0.06944993\n",
      "Iteration 74, loss = 0.06885759\n",
      "Iteration 75, loss = 0.06821838\n",
      "Iteration 76, loss = 0.06754824\n",
      "Iteration 77, loss = 0.06694188\n",
      "Iteration 78, loss = 0.06641753\n",
      "Iteration 79, loss = 0.06579184\n",
      "Iteration 80, loss = 0.06535097\n",
      "Iteration 81, loss = 0.06464584\n",
      "Iteration 82, loss = 0.06424842\n",
      "Iteration 83, loss = 0.06359648\n",
      "Iteration 84, loss = 0.06303098\n",
      "Iteration 85, loss = 0.06247553\n",
      "Iteration 86, loss = 0.06207622\n",
      "Iteration 87, loss = 0.06143895\n",
      "Iteration 88, loss = 0.06095379\n",
      "Iteration 89, loss = 0.06047363\n",
      "Iteration 90, loss = 0.06001491\n",
      "Iteration 91, loss = 0.05955959\n",
      "Iteration 92, loss = 0.05917669\n",
      "Iteration 93, loss = 0.05850757\n",
      "Iteration 94, loss = 0.05812561\n",
      "Iteration 95, loss = 0.05766273\n",
      "Iteration 96, loss = 0.05715449\n",
      "Iteration 97, loss = 0.05671567\n",
      "Iteration 98, loss = 0.05631061\n",
      "Iteration 99, loss = 0.05591314\n",
      "Iteration 100, loss = 0.05560604\n",
      "Iteration 101, loss = 0.05502870\n",
      "Iteration 102, loss = 0.05458094\n",
      "Iteration 103, loss = 0.05421274\n",
      "Iteration 104, loss = 0.05371441\n",
      "Iteration 105, loss = 0.05335178\n",
      "Iteration 106, loss = 0.05288329\n",
      "Iteration 107, loss = 0.05249628\n",
      "Iteration 108, loss = 0.05218279\n",
      "Iteration 109, loss = 0.05174427\n",
      "Iteration 110, loss = 0.05144217\n",
      "Iteration 111, loss = 0.05104512\n",
      "Iteration 112, loss = 0.05060696\n",
      "Iteration 113, loss = 0.05022780\n",
      "Iteration 114, loss = 0.04983804\n",
      "Iteration 115, loss = 0.04956834\n",
      "Iteration 116, loss = 0.04921325\n",
      "Iteration 117, loss = 0.04875386\n",
      "Iteration 118, loss = 0.04842953\n",
      "Iteration 119, loss = 0.04808206\n",
      "Iteration 120, loss = 0.04776549\n",
      "Iteration 121, loss = 0.04741045\n",
      "Iteration 122, loss = 0.04700795\n",
      "Iteration 123, loss = 0.04673601\n",
      "Iteration 124, loss = 0.04635746\n",
      "Iteration 125, loss = 0.04600694\n",
      "Iteration 126, loss = 0.04567832\n",
      "Iteration 127, loss = 0.04538996\n",
      "Iteration 128, loss = 0.04516943\n",
      "Iteration 129, loss = 0.04483503\n",
      "Iteration 130, loss = 0.04442544\n",
      "Iteration 131, loss = 0.04416750\n",
      "Iteration 132, loss = 0.04386768\n",
      "Iteration 133, loss = 0.04358060\n",
      "Iteration 134, loss = 0.04324679\n",
      "Iteration 135, loss = 0.04298259\n",
      "Iteration 136, loss = 0.04267999\n",
      "Iteration 137, loss = 0.04235325\n",
      "Iteration 138, loss = 0.04216803\n",
      "Iteration 139, loss = 0.04189946\n",
      "Iteration 140, loss = 0.04163776\n",
      "Iteration 141, loss = 0.04133024\n",
      "Iteration 142, loss = 0.04104387\n",
      "Iteration 143, loss = 0.04076810\n",
      "Iteration 144, loss = 0.04051677\n",
      "Iteration 145, loss = 0.04030515\n",
      "Iteration 146, loss = 0.03999674\n",
      "Iteration 147, loss = 0.03973087\n",
      "Iteration 148, loss = 0.03941176\n",
      "Iteration 149, loss = 0.03924301\n",
      "Iteration 150, loss = 0.03902634\n",
      "Iteration 151, loss = 0.03881587\n",
      "Iteration 152, loss = 0.03843512\n",
      "Iteration 153, loss = 0.03831115\n",
      "Iteration 154, loss = 0.03810804\n",
      "Iteration 155, loss = 0.03788117\n",
      "Iteration 156, loss = 0.03753724\n",
      "Iteration 157, loss = 0.03733547\n",
      "Iteration 158, loss = 0.03718228\n",
      "Iteration 159, loss = 0.03688364\n",
      "Iteration 160, loss = 0.03662619\n",
      "Iteration 161, loss = 0.03642083\n",
      "Iteration 162, loss = 0.03626237\n",
      "Iteration 163, loss = 0.03606107\n",
      "Iteration 164, loss = 0.03585531\n",
      "Iteration 165, loss = 0.03563556\n",
      "Iteration 166, loss = 0.03543608\n",
      "Iteration 167, loss = 0.03527923\n",
      "Iteration 168, loss = 0.03514111\n",
      "Iteration 169, loss = 0.03484471\n",
      "Iteration 170, loss = 0.03457007\n",
      "Iteration 171, loss = 0.03446051\n",
      "Iteration 172, loss = 0.03432911\n",
      "Iteration 173, loss = 0.03403262\n",
      "Iteration 174, loss = 0.03385059\n",
      "Iteration 175, loss = 0.03368261\n",
      "Iteration 176, loss = 0.03347501\n",
      "Iteration 177, loss = 0.03330687\n",
      "Iteration 178, loss = 0.03319314\n",
      "Iteration 179, loss = 0.03298708\n",
      "Iteration 180, loss = 0.03278103\n",
      "Iteration 181, loss = 0.03277809\n",
      "Iteration 182, loss = 0.03248520\n",
      "Iteration 183, loss = 0.03233963\n",
      "Iteration 184, loss = 0.03215161\n",
      "Iteration 185, loss = 0.03198223\n",
      "Iteration 186, loss = 0.03178531\n",
      "Iteration 187, loss = 0.03166693\n",
      "Iteration 188, loss = 0.03149600\n",
      "Iteration 189, loss = 0.03132789\n",
      "Iteration 190, loss = 0.03117188\n",
      "Iteration 191, loss = 0.03107892\n",
      "Iteration 192, loss = 0.03094485\n",
      "Iteration 193, loss = 0.03074572\n",
      "Iteration 194, loss = 0.03058110\n",
      "Iteration 195, loss = 0.03043215\n",
      "Iteration 196, loss = 0.03030672\n",
      "Iteration 197, loss = 0.03018413\n",
      "Iteration 198, loss = 0.03006307\n",
      "Iteration 199, loss = 0.02989597\n",
      "Iteration 200, loss = 0.02976490\n",
      "Iteration 201, loss = 0.02959853\n",
      "Iteration 202, loss = 0.02946462\n",
      "Iteration 203, loss = 0.02936599\n",
      "Iteration 204, loss = 0.02924302\n",
      "Iteration 205, loss = 0.02903852\n",
      "Iteration 206, loss = 0.02897101\n",
      "Iteration 207, loss = 0.02879444\n",
      "Iteration 208, loss = 0.02867384\n",
      "Iteration 209, loss = 0.02851848\n",
      "Iteration 210, loss = 0.02841554\n",
      "Iteration 211, loss = 0.02831342\n",
      "Iteration 212, loss = 0.02816089\n",
      "Iteration 213, loss = 0.02802939\n",
      "Iteration 214, loss = 0.02798971\n",
      "Iteration 215, loss = 0.02787823\n",
      "Iteration 216, loss = 0.02767672\n",
      "Iteration 217, loss = 0.02763416\n",
      "Iteration 218, loss = 0.02751628\n",
      "Iteration 219, loss = 0.02740070\n",
      "Iteration 220, loss = 0.02718493\n",
      "Iteration 221, loss = 0.02706462\n",
      "Iteration 222, loss = 0.02699684\n",
      "Iteration 223, loss = 0.02686798\n",
      "Iteration 224, loss = 0.02682350\n",
      "Iteration 225, loss = 0.02659869\n",
      "Iteration 226, loss = 0.02649843\n",
      "Iteration 227, loss = 0.02657594\n",
      "Iteration 228, loss = 0.02632693\n",
      "Iteration 229, loss = 0.02622883\n",
      "Iteration 230, loss = 0.02610852\n",
      "Iteration 231, loss = 0.02601779\n",
      "Iteration 232, loss = 0.02600682\n",
      "Iteration 233, loss = 0.02583810\n",
      "Iteration 234, loss = 0.02567449\n",
      "Iteration 235, loss = 0.02562590\n",
      "Iteration 236, loss = 0.02543859\n",
      "Iteration 237, loss = 0.02534954\n",
      "Iteration 238, loss = 0.02526898\n",
      "Iteration 239, loss = 0.02518704\n",
      "Iteration 240, loss = 0.02507073\n",
      "Iteration 241, loss = 0.02500213\n",
      "Iteration 242, loss = 0.02485978\n",
      "Iteration 243, loss = 0.02475454\n",
      "Iteration 244, loss = 0.02467349\n",
      "Iteration 245, loss = 0.02468788\n",
      "Iteration 246, loss = 0.02458093\n",
      "Iteration 247, loss = 0.02438419\n",
      "Iteration 248, loss = 0.02433873\n",
      "Iteration 249, loss = 0.02438609\n",
      "Iteration 250, loss = 0.02412502\n",
      "Iteration 251, loss = 0.02413593\n",
      "Iteration 252, loss = 0.02392039\n",
      "Iteration 253, loss = 0.02379772\n",
      "Iteration 254, loss = 0.02378207\n",
      "Iteration 255, loss = 0.02380352\n",
      "Iteration 256, loss = 0.02357895\n",
      "Iteration 257, loss = 0.02346572\n",
      "Iteration 258, loss = 0.02337950\n",
      "Iteration 259, loss = 0.02344512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 260, loss = 0.02323734\n",
      "Iteration 261, loss = 0.02320683\n",
      "Iteration 262, loss = 0.02308545\n",
      "Iteration 263, loss = 0.02302083\n",
      "Iteration 264, loss = 0.02288416\n",
      "Iteration 265, loss = 0.02280662\n",
      "Iteration 266, loss = 0.02271751\n",
      "Iteration 267, loss = 0.02274343\n",
      "Iteration 268, loss = 0.02261610\n",
      "Iteration 269, loss = 0.02247212\n",
      "Iteration 270, loss = 0.02243411\n",
      "Iteration 271, loss = 0.02243797\n",
      "Iteration 272, loss = 0.02229157\n",
      "Iteration 273, loss = 0.02215153\n",
      "Iteration 274, loss = 0.02214349\n",
      "Iteration 275, loss = 0.02210086\n",
      "Iteration 276, loss = 0.02196424\n",
      "Iteration 277, loss = 0.02186456\n",
      "Iteration 278, loss = 0.02177410\n",
      "Iteration 279, loss = 0.02173326\n",
      "Iteration 280, loss = 0.02171829\n",
      "Iteration 281, loss = 0.02156202\n",
      "Iteration 282, loss = 0.02151281\n",
      "Iteration 283, loss = 0.02139136\n",
      "Iteration 284, loss = 0.02139776\n",
      "Iteration 285, loss = 0.02150839\n",
      "Iteration 286, loss = 0.02122686\n",
      "Iteration 287, loss = 0.02112445\n",
      "Iteration 288, loss = 0.02107713\n",
      "Iteration 289, loss = 0.02099686\n",
      "Iteration 290, loss = 0.02103331\n",
      "Iteration 291, loss = 0.02087163\n",
      "Iteration 292, loss = 0.02083002\n",
      "Iteration 293, loss = 0.02078821\n",
      "Iteration 294, loss = 0.02067931\n",
      "Iteration 295, loss = 0.02066926\n",
      "Iteration 296, loss = 0.02055075\n",
      "Iteration 297, loss = 0.02049663\n",
      "Iteration 298, loss = 0.02037433\n",
      "Iteration 299, loss = 0.02031351\n",
      "Iteration 300, loss = 0.02027373\n",
      "Iteration 301, loss = 0.02021631\n",
      "Iteration 302, loss = 0.02020556\n",
      "Iteration 303, loss = 0.02011442\n",
      "Iteration 304, loss = 0.02003612\n",
      "Iteration 305, loss = 0.01996428\n",
      "Iteration 306, loss = 0.01989707\n",
      "Iteration 307, loss = 0.01980794\n",
      "Iteration 308, loss = 0.01975530\n",
      "Iteration 309, loss = 0.01968682\n",
      "Iteration 310, loss = 0.01974058\n",
      "Iteration 311, loss = 0.01967226\n",
      "Iteration 312, loss = 0.01948540\n",
      "Iteration 313, loss = 0.01944843\n",
      "Iteration 314, loss = 0.01944487\n",
      "Iteration 315, loss = 0.01931931\n",
      "Iteration 316, loss = 0.01934358\n",
      "Iteration 317, loss = 0.01921137\n",
      "Iteration 318, loss = 0.01916439\n",
      "Iteration 319, loss = 0.01914351\n",
      "Iteration 320, loss = 0.01908257\n",
      "Iteration 321, loss = 0.01915179\n",
      "Iteration 322, loss = 0.01894365\n",
      "Iteration 323, loss = 0.01889732\n",
      "Iteration 324, loss = 0.01886973\n",
      "Iteration 325, loss = 0.01880841\n",
      "Iteration 326, loss = 0.01871417\n",
      "Iteration 327, loss = 0.01863027\n",
      "Iteration 328, loss = 0.01880134\n",
      "Iteration 329, loss = 0.01865012\n",
      "Iteration 330, loss = 0.01864616\n",
      "Iteration 331, loss = 0.01855605\n",
      "Iteration 332, loss = 0.01837142\n",
      "Iteration 333, loss = 0.01834602\n",
      "Iteration 334, loss = 0.01825327\n",
      "Iteration 335, loss = 0.01822245\n",
      "Iteration 336, loss = 0.01824841\n",
      "Iteration 337, loss = 0.01814781\n",
      "Iteration 338, loss = 0.01817450\n",
      "Iteration 339, loss = 0.01801925\n",
      "Iteration 340, loss = 0.01795915\n",
      "Iteration 341, loss = 0.01791604\n",
      "Iteration 342, loss = 0.01794431\n",
      "Iteration 343, loss = 0.01778823\n",
      "Iteration 344, loss = 0.01781079\n",
      "Iteration 345, loss = 0.01780530\n",
      "Iteration 346, loss = 0.01770971\n",
      "Iteration 347, loss = 0.01768128\n",
      "Iteration 348, loss = 0.01756430\n",
      "Iteration 349, loss = 0.01761249\n",
      "Iteration 350, loss = 0.01764341\n",
      "Iteration 351, loss = 0.01743633\n",
      "Iteration 352, loss = 0.01739756\n",
      "Iteration 353, loss = 0.01746299\n",
      "Iteration 354, loss = 0.01731462\n",
      "Iteration 355, loss = 0.01720521\n",
      "Iteration 356, loss = 0.01725067\n",
      "Iteration 357, loss = 0.01711522\n",
      "Iteration 358, loss = 0.01705616\n",
      "Iteration 359, loss = 0.01703909\n",
      "Iteration 360, loss = 0.01705696\n",
      "Iteration 361, loss = 0.01689791\n",
      "Iteration 362, loss = 0.01695508\n",
      "Iteration 363, loss = 0.01689284\n",
      "Iteration 364, loss = 0.01680845\n",
      "Iteration 365, loss = 0.01679371\n",
      "Iteration 366, loss = 0.01672445\n",
      "Iteration 367, loss = 0.01667723\n",
      "Iteration 368, loss = 0.01662755\n",
      "Iteration 369, loss = 0.01656855\n",
      "Iteration 370, loss = 0.01655443\n",
      "Iteration 371, loss = 0.01649942\n",
      "Iteration 372, loss = 0.01667677\n",
      "Iteration 373, loss = 0.01652552\n",
      "Iteration 374, loss = 0.01639381\n",
      "Iteration 375, loss = 0.01632661\n",
      "Iteration 376, loss = 0.01631588\n",
      "Iteration 377, loss = 0.01623248\n",
      "Iteration 378, loss = 0.01618649\n",
      "Iteration 379, loss = 0.01614562\n",
      "Iteration 380, loss = 0.01611856\n",
      "Iteration 381, loss = 0.01606655\n",
      "Iteration 382, loss = 0.01615440\n",
      "Iteration 383, loss = 0.01597927\n",
      "Iteration 384, loss = 0.01594171\n",
      "Iteration 385, loss = 0.01590688\n",
      "Iteration 386, loss = 0.01588460\n",
      "Iteration 387, loss = 0.01586608\n",
      "Iteration 388, loss = 0.01581394\n",
      "Iteration 389, loss = 0.01580647\n",
      "Iteration 390, loss = 0.01579395\n",
      "Iteration 391, loss = 0.01573317\n",
      "Iteration 392, loss = 0.01572151\n",
      "Iteration 393, loss = 0.01562796\n",
      "Iteration 394, loss = 0.01555131\n",
      "Iteration 395, loss = 0.01553994\n",
      "Iteration 396, loss = 0.01555451\n",
      "Iteration 397, loss = 0.01545832\n",
      "Iteration 398, loss = 0.01539726\n",
      "Iteration 399, loss = 0.01560866\n",
      "Iteration 400, loss = 0.01531349\n",
      "Iteration 401, loss = 0.01530329\n",
      "Iteration 402, loss = 0.01529851\n",
      "Iteration 403, loss = 0.01524561\n",
      "Iteration 404, loss = 0.01517347\n",
      "Iteration 405, loss = 0.01511842\n",
      "Iteration 406, loss = 0.01510630\n",
      "Iteration 407, loss = 0.01508361\n",
      "Iteration 408, loss = 0.01513928\n",
      "Iteration 409, loss = 0.01494979\n",
      "Iteration 410, loss = 0.01500142\n",
      "Iteration 411, loss = 0.01488053\n",
      "Iteration 412, loss = 0.01490377\n",
      "Iteration 413, loss = 0.01494892\n",
      "Iteration 414, loss = 0.01485608\n",
      "Iteration 415, loss = 0.01483204\n",
      "Iteration 416, loss = 0.01475881\n",
      "Iteration 417, loss = 0.01477308\n",
      "Iteration 418, loss = 0.01465092\n",
      "Iteration 419, loss = 0.01463474\n",
      "Iteration 420, loss = 0.01463880\n",
      "Iteration 421, loss = 0.01464446\n",
      "Iteration 422, loss = 0.01455791\n",
      "Iteration 423, loss = 0.01451348\n",
      "Iteration 424, loss = 0.01446071\n",
      "Iteration 425, loss = 0.01443099\n",
      "Iteration 426, loss = 0.01443379\n",
      "Iteration 427, loss = 0.01436208\n",
      "Iteration 428, loss = 0.01432089\n",
      "Iteration 429, loss = 0.01430379\n",
      "Iteration 430, loss = 0.01424873\n",
      "Iteration 431, loss = 0.01427514\n",
      "Iteration 432, loss = 0.01420314\n",
      "Iteration 433, loss = 0.01419343\n",
      "Iteration 434, loss = 0.01418139\n",
      "Iteration 435, loss = 0.01411361\n",
      "Iteration 436, loss = 0.01406686\n",
      "Iteration 437, loss = 0.01410691\n",
      "Iteration 438, loss = 0.01402487\n",
      "Iteration 439, loss = 0.01409739\n",
      "Iteration 440, loss = 0.01400434\n",
      "Iteration 441, loss = 0.01391871\n",
      "Iteration 442, loss = 0.01392196\n",
      "Iteration 443, loss = 0.01398793\n",
      "Iteration 444, loss = 0.01376186\n",
      "Iteration 445, loss = 0.01381429\n",
      "Iteration 446, loss = 0.01372971\n",
      "Iteration 447, loss = 0.01374123\n",
      "Iteration 448, loss = 0.01375681\n",
      "Iteration 449, loss = 0.01365448\n",
      "Iteration 450, loss = 0.01364058\n",
      "Iteration 451, loss = 0.01364803\n",
      "Iteration 452, loss = 0.01358484\n",
      "Iteration 453, loss = 0.01356562\n",
      "Iteration 454, loss = 0.01352567\n",
      "Iteration 455, loss = 0.01350632\n",
      "Iteration 456, loss = 0.01347882\n",
      "Iteration 457, loss = 0.01341397\n",
      "Iteration 458, loss = 0.01337779\n",
      "Iteration 459, loss = 0.01336448\n",
      "Iteration 460, loss = 0.01330606\n",
      "Iteration 461, loss = 0.01333000\n",
      "Iteration 462, loss = 0.01322944\n",
      "Iteration 463, loss = 0.01323406\n",
      "Iteration 464, loss = 0.01324207\n",
      "Iteration 465, loss = 0.01324874\n",
      "Iteration 466, loss = 0.01316606\n",
      "Iteration 467, loss = 0.01309827\n",
      "Iteration 468, loss = 0.01306870\n",
      "Iteration 469, loss = 0.01312907\n",
      "Iteration 470, loss = 0.01304262\n",
      "Iteration 471, loss = 0.01303503\n",
      "Iteration 472, loss = 0.01296497\n",
      "Iteration 473, loss = 0.01297185\n",
      "Iteration 474, loss = 0.01298276\n",
      "Iteration 475, loss = 0.01290300\n",
      "Iteration 476, loss = 0.01284722\n",
      "Iteration 477, loss = 0.01287873\n",
      "Iteration 478, loss = 0.01280813\n",
      "Iteration 479, loss = 0.01288118\n",
      "Iteration 480, loss = 0.01279375\n",
      "Iteration 481, loss = 0.01274809\n",
      "Iteration 482, loss = 0.01278059\n",
      "Iteration 483, loss = 0.01269365\n",
      "Iteration 484, loss = 0.01264354\n",
      "Iteration 485, loss = 0.01260940\n",
      "Iteration 486, loss = 0.01259731\n",
      "Iteration 487, loss = 0.01257514\n",
      "Iteration 488, loss = 0.01258151\n",
      "Iteration 489, loss = 0.01259102\n",
      "Iteration 490, loss = 0.01245672\n",
      "Iteration 491, loss = 0.01242395\n",
      "Iteration 492, loss = 0.01245685\n",
      "Iteration 493, loss = 0.01242815\n",
      "Iteration 494, loss = 0.01246858\n",
      "Iteration 495, loss = 0.01239380\n",
      "Iteration 496, loss = 0.01232881\n",
      "Iteration 497, loss = 0.01231839\n",
      "Iteration 498, loss = 0.01225865\n",
      "Iteration 499, loss = 0.01224424\n",
      "Iteration 500, loss = 0.01223832\n",
      "Iteration 501, loss = 0.01220691\n",
      "Iteration 502, loss = 0.01217864\n",
      "Iteration 503, loss = 0.01209796\n",
      "Iteration 504, loss = 0.01215595\n",
      "Iteration 505, loss = 0.01207827\n",
      "Iteration 506, loss = 0.01205667\n",
      "Iteration 507, loss = 0.01215940\n",
      "Iteration 508, loss = 0.01200787\n",
      "Iteration 509, loss = 0.01200942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 510, loss = 0.01197806\n",
      "Iteration 511, loss = 0.01196182\n",
      "Iteration 512, loss = 0.01202092\n",
      "Iteration 513, loss = 0.01196486\n",
      "Iteration 514, loss = 0.01208145\n",
      "Iteration 515, loss = 0.01197643\n",
      "Iteration 516, loss = 0.01180543\n",
      "Iteration 517, loss = 0.01184025\n",
      "Iteration 518, loss = 0.01191939\n",
      "Iteration 519, loss = 0.01176841\n",
      "Iteration 520, loss = 0.01173494\n",
      "Iteration 521, loss = 0.01169691\n",
      "Iteration 522, loss = 0.01167967\n",
      "Iteration 523, loss = 0.01174484\n",
      "Iteration 524, loss = 0.01165459\n",
      "Iteration 525, loss = 0.01176431\n",
      "Iteration 526, loss = 0.01161544\n",
      "Iteration 527, loss = 0.01161925\n",
      "Iteration 528, loss = 0.01180628\n",
      "Iteration 529, loss = 0.01149419\n",
      "Iteration 530, loss = 0.01145776\n",
      "Iteration 531, loss = 0.01147524\n",
      "Iteration 532, loss = 0.01143238\n",
      "Iteration 533, loss = 0.01142133\n",
      "Iteration 534, loss = 0.01147653\n",
      "Iteration 535, loss = 0.01156544\n",
      "Iteration 536, loss = 0.01139877\n",
      "Iteration 537, loss = 0.01138085\n",
      "Iteration 538, loss = 0.01126651\n",
      "Iteration 539, loss = 0.01133636\n",
      "Iteration 540, loss = 0.01125866\n",
      "Iteration 541, loss = 0.01120177\n",
      "Iteration 542, loss = 0.01123318\n",
      "Iteration 543, loss = 0.01116793\n",
      "Iteration 544, loss = 0.01114391\n",
      "Iteration 545, loss = 0.01115401\n",
      "Iteration 546, loss = 0.01115914\n",
      "Iteration 547, loss = 0.01116581\n",
      "Iteration 548, loss = 0.01112908\n",
      "Iteration 549, loss = 0.01114993\n",
      "Iteration 550, loss = 0.01102844\n",
      "Iteration 551, loss = 0.01109779\n",
      "Iteration 552, loss = 0.01104431\n",
      "Iteration 553, loss = 0.01102423\n",
      "Iteration 554, loss = 0.01094805\n",
      "Iteration 555, loss = 0.01103632\n",
      "Iteration 556, loss = 0.01097960\n",
      "Iteration 557, loss = 0.01088254\n",
      "Iteration 558, loss = 0.01089895\n",
      "Iteration 559, loss = 0.01089052\n",
      "Iteration 560, loss = 0.01087904\n",
      "Iteration 561, loss = 0.01089013\n",
      "Iteration 562, loss = 0.01085402\n",
      "Iteration 563, loss = 0.01080900\n",
      "Iteration 564, loss = 0.01077222\n",
      "Iteration 565, loss = 0.01074814\n",
      "Iteration 566, loss = 0.01070430\n",
      "Iteration 567, loss = 0.01073213\n",
      "Iteration 568, loss = 0.01064203\n",
      "Iteration 569, loss = 0.01061741\n",
      "Iteration 570, loss = 0.01070357\n",
      "Iteration 571, loss = 0.01058153\n",
      "Iteration 572, loss = 0.01065695\n",
      "Iteration 573, loss = 0.01052615\n",
      "Iteration 574, loss = 0.01062444\n",
      "Iteration 575, loss = 0.01056421\n",
      "Iteration 576, loss = 0.01053790\n",
      "Iteration 577, loss = 0.01045239\n",
      "Iteration 578, loss = 0.01045886\n",
      "Iteration 579, loss = 0.01044681\n",
      "Iteration 580, loss = 0.01042100\n",
      "Iteration 581, loss = 0.01042421\n",
      "Iteration 582, loss = 0.01042112\n",
      "Iteration 583, loss = 0.01049090\n",
      "Iteration 584, loss = 0.01036465\n",
      "Iteration 585, loss = 0.01033461\n",
      "Iteration 586, loss = 0.01035156\n",
      "Iteration 587, loss = 0.01034037\n",
      "Iteration 588, loss = 0.01030597\n",
      "Iteration 589, loss = 0.01032847\n",
      "Iteration 590, loss = 0.01027504\n",
      "Iteration 591, loss = 0.01021083\n",
      "Iteration 592, loss = 0.01023737\n",
      "Iteration 593, loss = 0.01016331\n",
      "Iteration 594, loss = 0.01017844\n",
      "Iteration 595, loss = 0.01011083\n",
      "Iteration 596, loss = 0.01015191\n",
      "Iteration 597, loss = 0.01018226\n",
      "Iteration 598, loss = 0.01010030\n",
      "Iteration 599, loss = 0.01008568\n",
      "Iteration 600, loss = 0.01010451\n",
      "Iteration 601, loss = 0.01002652\n",
      "Iteration 602, loss = 0.01015016\n",
      "Iteration 603, loss = 0.01013506\n",
      "Iteration 604, loss = 0.01016805\n",
      "Iteration 605, loss = 0.00997178\n",
      "Iteration 606, loss = 0.00992550\n",
      "Iteration 607, loss = 0.01000670\n",
      "Iteration 608, loss = 0.00987344\n",
      "Iteration 609, loss = 0.00986837\n",
      "Iteration 610, loss = 0.00989500\n",
      "Iteration 611, loss = 0.00993239\n",
      "Iteration 612, loss = 0.00983915\n",
      "Iteration 613, loss = 0.00996323\n",
      "Iteration 614, loss = 0.00979228\n",
      "Iteration 615, loss = 0.00987710\n",
      "Iteration 616, loss = 0.00981573\n",
      "Iteration 617, loss = 0.00982475\n",
      "Iteration 618, loss = 0.00972201\n",
      "Iteration 619, loss = 0.00982082\n",
      "Iteration 620, loss = 0.00968188\n",
      "Iteration 621, loss = 0.00971947\n",
      "Iteration 622, loss = 0.00965760\n",
      "Iteration 623, loss = 0.00963684\n",
      "Iteration 624, loss = 0.00976055\n",
      "Iteration 625, loss = 0.00958244\n",
      "Iteration 626, loss = 0.00956482\n",
      "Iteration 627, loss = 0.00963211\n",
      "Iteration 628, loss = 0.00962368\n",
      "Iteration 629, loss = 0.00957685\n",
      "Iteration 630, loss = 0.00955650\n",
      "Iteration 631, loss = 0.00949750\n",
      "Iteration 632, loss = 0.00951003\n",
      "Iteration 633, loss = 0.00951862\n",
      "Iteration 634, loss = 0.00946741\n",
      "Iteration 635, loss = 0.00950184\n",
      "Iteration 636, loss = 0.00947979\n",
      "Iteration 637, loss = 0.00945276\n",
      "Iteration 638, loss = 0.00942202\n",
      "Iteration 639, loss = 0.00938995\n",
      "Iteration 640, loss = 0.00935826\n",
      "Iteration 641, loss = 0.00944339\n",
      "Iteration 642, loss = 0.00934400\n",
      "Iteration 643, loss = 0.00929144\n",
      "Iteration 644, loss = 0.00928070\n",
      "Iteration 645, loss = 0.00930299\n",
      "Iteration 646, loss = 0.00929228\n",
      "Iteration 647, loss = 0.00930217\n",
      "Iteration 648, loss = 0.00929678\n",
      "Iteration 649, loss = 0.00921012\n",
      "Iteration 650, loss = 0.00933148\n",
      "Iteration 651, loss = 0.00924850\n",
      "Iteration 652, loss = 0.00926615\n",
      "Iteration 653, loss = 0.00924704\n",
      "Iteration 654, loss = 0.00922944\n",
      "Iteration 655, loss = 0.00915111\n",
      "Iteration 656, loss = 0.00915187\n",
      "Iteration 657, loss = 0.00907603\n",
      "Iteration 658, loss = 0.00921166\n",
      "Iteration 659, loss = 0.00907561\n",
      "Iteration 660, loss = 0.00912229\n",
      "Iteration 661, loss = 0.00908647\n",
      "Iteration 662, loss = 0.00900509\n",
      "Iteration 663, loss = 0.00908064\n",
      "Iteration 664, loss = 0.00901327\n",
      "Iteration 665, loss = 0.00904667\n",
      "Iteration 666, loss = 0.00908392\n",
      "Iteration 667, loss = 0.00891964\n",
      "Iteration 668, loss = 0.00892420\n",
      "Iteration 669, loss = 0.00894507\n",
      "Iteration 670, loss = 0.00894674\n",
      "Iteration 671, loss = 0.00888448\n",
      "Iteration 672, loss = 0.00884288\n",
      "Iteration 673, loss = 0.00893744\n",
      "Iteration 674, loss = 0.00881870\n",
      "Iteration 675, loss = 0.00889235\n",
      "Iteration 676, loss = 0.00884708\n",
      "Iteration 677, loss = 0.00878305\n",
      "Iteration 678, loss = 0.00878510\n",
      "Iteration 679, loss = 0.00875743\n",
      "Iteration 680, loss = 0.00881733\n",
      "Iteration 681, loss = 0.00868846\n",
      "Iteration 682, loss = 0.00877547\n",
      "Iteration 683, loss = 0.00878804\n",
      "Iteration 684, loss = 0.00872827\n",
      "Iteration 685, loss = 0.00865564\n",
      "Iteration 686, loss = 0.00864931\n",
      "Iteration 687, loss = 0.00863912\n",
      "Iteration 688, loss = 0.00862773\n",
      "Iteration 689, loss = 0.00861729\n",
      "Iteration 690, loss = 0.00857611\n",
      "Iteration 691, loss = 0.00858842\n",
      "Iteration 692, loss = 0.00854334\n",
      "Iteration 693, loss = 0.00854817\n",
      "Iteration 694, loss = 0.00859270\n",
      "Iteration 695, loss = 0.00857883\n",
      "Iteration 696, loss = 0.00848294\n",
      "Iteration 697, loss = 0.00849344\n",
      "Iteration 698, loss = 0.00852880\n",
      "Iteration 699, loss = 0.00848480\n",
      "Iteration 700, loss = 0.00850974\n",
      "Iteration 701, loss = 0.00844919\n",
      "Iteration 702, loss = 0.00843359\n",
      "Iteration 703, loss = 0.00839461\n",
      "Iteration 704, loss = 0.00844691\n",
      "Iteration 705, loss = 0.00847572\n",
      "Iteration 706, loss = 0.00840152\n",
      "Iteration 707, loss = 0.00836622\n",
      "Iteration 708, loss = 0.00832851\n",
      "Iteration 709, loss = 0.00836861\n",
      "Iteration 710, loss = 0.00830195\n",
      "Iteration 711, loss = 0.00836830\n",
      "Iteration 712, loss = 0.00825828\n",
      "Iteration 713, loss = 0.00837436\n",
      "Iteration 714, loss = 0.00828156\n",
      "Iteration 715, loss = 0.00827550\n",
      "Iteration 716, loss = 0.00834810\n",
      "Iteration 717, loss = 0.00818682\n",
      "Iteration 718, loss = 0.00823693\n",
      "Iteration 719, loss = 0.00822497\n",
      "Iteration 720, loss = 0.00820645\n",
      "Iteration 721, loss = 0.00816256\n",
      "Iteration 722, loss = 0.00820518\n",
      "Iteration 723, loss = 0.00814002\n",
      "Iteration 724, loss = 0.00819238\n",
      "Iteration 725, loss = 0.00816507\n",
      "Iteration 726, loss = 0.00807918\n",
      "Iteration 727, loss = 0.00810049\n",
      "Iteration 728, loss = 0.00811519\n",
      "Iteration 729, loss = 0.00806057\n",
      "Iteration 730, loss = 0.00807179\n",
      "Iteration 731, loss = 0.00802786\n",
      "Iteration 732, loss = 0.00815327\n",
      "Iteration 733, loss = 0.00805264\n",
      "Iteration 734, loss = 0.00803079\n",
      "Iteration 735, loss = 0.00800669\n",
      "Iteration 736, loss = 0.00797370\n",
      "Iteration 737, loss = 0.00794510\n",
      "Iteration 738, loss = 0.00795373\n",
      "Iteration 739, loss = 0.00794615\n",
      "Iteration 740, loss = 0.00795029\n",
      "Iteration 741, loss = 0.00790994\n",
      "Iteration 742, loss = 0.00789959\n",
      "Iteration 743, loss = 0.00789404\n",
      "Iteration 744, loss = 0.00784822\n",
      "Iteration 745, loss = 0.00808881\n",
      "Iteration 746, loss = 0.00794984\n",
      "Iteration 747, loss = 0.00784290\n",
      "Iteration 748, loss = 0.00792887\n",
      "Iteration 749, loss = 0.00786168\n",
      "Iteration 750, loss = 0.00783660\n",
      "Iteration 751, loss = 0.00780828\n",
      "Iteration 752, loss = 0.00782603\n",
      "Iteration 753, loss = 0.00771998\n",
      "Iteration 754, loss = 0.00781624\n",
      "Iteration 755, loss = 0.00780444\n",
      "Iteration 756, loss = 0.00772674\n",
      "Iteration 757, loss = 0.00771917\n",
      "Iteration 758, loss = 0.00768413\n",
      "Iteration 759, loss = 0.00768919\n",
      "Iteration 760, loss = 0.00771678\n",
      "Iteration 761, loss = 0.00769941\n",
      "Iteration 762, loss = 0.00767721\n",
      "Iteration 763, loss = 0.00769137\n",
      "Iteration 764, loss = 0.00764316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 765, loss = 0.00762916\n",
      "Iteration 766, loss = 0.00770585\n",
      "Iteration 767, loss = 0.00758020\n",
      "Iteration 768, loss = 0.00763220\n",
      "Iteration 769, loss = 0.00759546\n",
      "Iteration 770, loss = 0.00756891\n",
      "Iteration 771, loss = 0.00761615\n",
      "Iteration 772, loss = 0.00750517\n",
      "Iteration 773, loss = 0.00754551\n",
      "Iteration 774, loss = 0.00758390\n",
      "Iteration 775, loss = 0.00749273\n",
      "Iteration 776, loss = 0.00745018\n",
      "Iteration 777, loss = 0.00752284\n",
      "Iteration 778, loss = 0.00750295\n",
      "Iteration 779, loss = 0.00754714\n",
      "Iteration 780, loss = 0.00746788\n",
      "Iteration 781, loss = 0.00739781\n",
      "Iteration 782, loss = 0.00757502\n",
      "Iteration 783, loss = 0.00744916\n",
      "Iteration 784, loss = 0.00741676\n",
      "Iteration 785, loss = 0.00742782\n",
      "Iteration 786, loss = 0.00735916\n",
      "Iteration 787, loss = 0.00738178\n",
      "Iteration 788, loss = 0.00740937\n",
      "Iteration 789, loss = 0.00741567\n",
      "Iteration 790, loss = 0.00740905\n",
      "Iteration 791, loss = 0.00742779\n",
      "Iteration 792, loss = 0.00733758\n",
      "Iteration 793, loss = 0.00727513\n",
      "Iteration 794, loss = 0.00727285\n",
      "Iteration 795, loss = 0.00730513\n",
      "Iteration 796, loss = 0.00723172\n",
      "Iteration 797, loss = 0.00724789\n",
      "Iteration 798, loss = 0.00727438\n",
      "Iteration 799, loss = 0.00724146\n",
      "Iteration 800, loss = 0.00717914\n",
      "Iteration 801, loss = 0.00719293\n",
      "Iteration 802, loss = 0.00716911\n",
      "Iteration 803, loss = 0.00715150\n",
      "Iteration 804, loss = 0.00715549\n",
      "Iteration 805, loss = 0.00716046\n",
      "Iteration 806, loss = 0.00719750\n",
      "Iteration 807, loss = 0.00715398\n",
      "Iteration 808, loss = 0.00716583\n",
      "Iteration 809, loss = 0.00715092\n",
      "Iteration 810, loss = 0.00709849\n",
      "Iteration 811, loss = 0.00714343\n",
      "Iteration 812, loss = 0.00714265\n",
      "Iteration 813, loss = 0.00707618\n",
      "Iteration 814, loss = 0.00706996\n",
      "Iteration 815, loss = 0.00702139\n",
      "Iteration 816, loss = 0.00710756\n",
      "Iteration 817, loss = 0.00703979\n",
      "Iteration 818, loss = 0.00710770\n",
      "Iteration 819, loss = 0.00716264\n",
      "Iteration 820, loss = 0.00708425\n",
      "Iteration 821, loss = 0.00704306\n",
      "Iteration 822, loss = 0.00698467\n",
      "Iteration 823, loss = 0.00700223\n",
      "Iteration 824, loss = 0.00695909\n",
      "Iteration 825, loss = 0.00694515\n",
      "Iteration 826, loss = 0.00697419\n",
      "Iteration 827, loss = 0.00690504\n",
      "Iteration 828, loss = 0.00704831\n",
      "Iteration 829, loss = 0.00692649\n",
      "Iteration 830, loss = 0.00693169\n",
      "Iteration 831, loss = 0.00696497\n",
      "Iteration 832, loss = 0.00687913\n",
      "Iteration 833, loss = 0.00686333\n",
      "Iteration 834, loss = 0.00685433\n",
      "Iteration 835, loss = 0.00682073\n",
      "Iteration 836, loss = 0.00684411\n",
      "Iteration 837, loss = 0.00684108\n",
      "Iteration 838, loss = 0.00686380\n",
      "Iteration 839, loss = 0.00686833\n",
      "Iteration 840, loss = 0.00678584\n",
      "Iteration 841, loss = 0.00682261\n",
      "Iteration 842, loss = 0.00676772\n",
      "Iteration 843, loss = 0.00681649\n",
      "Iteration 844, loss = 0.00678341\n",
      "Iteration 845, loss = 0.00682383\n",
      "Iteration 846, loss = 0.00680965\n",
      "Iteration 847, loss = 0.00677248\n",
      "Iteration 848, loss = 0.00672215\n",
      "Iteration 849, loss = 0.00667901\n",
      "Iteration 850, loss = 0.00669101\n",
      "Iteration 851, loss = 0.00670964\n",
      "Iteration 852, loss = 0.00666027\n",
      "Iteration 853, loss = 0.00663753\n",
      "Iteration 854, loss = 0.00664980\n",
      "Iteration 855, loss = 0.00663713\n",
      "Iteration 856, loss = 0.00663678\n",
      "Iteration 857, loss = 0.00659842\n",
      "Iteration 858, loss = 0.00661977\n",
      "Iteration 859, loss = 0.00657171\n",
      "Iteration 860, loss = 0.00667634\n",
      "Iteration 861, loss = 0.00656033\n",
      "Iteration 862, loss = 0.00654462\n",
      "Iteration 863, loss = 0.00661630\n",
      "Iteration 864, loss = 0.00672918\n",
      "Iteration 865, loss = 0.00664955\n",
      "Iteration 866, loss = 0.00655081\n",
      "Iteration 867, loss = 0.00654448\n",
      "Iteration 868, loss = 0.00652548\n",
      "Iteration 869, loss = 0.00658854\n",
      "Iteration 870, loss = 0.00665421\n",
      "Iteration 871, loss = 0.00654232\n",
      "Iteration 872, loss = 0.00659999\n",
      "Iteration 873, loss = 0.00653668\n",
      "Iteration 874, loss = 0.00649383\n",
      "Iteration 875, loss = 0.00647094\n",
      "Iteration 876, loss = 0.00650568\n",
      "Iteration 877, loss = 0.00647085\n",
      "Iteration 878, loss = 0.00645497\n",
      "Iteration 879, loss = 0.00650155\n",
      "Iteration 880, loss = 0.00644233\n",
      "Iteration 881, loss = 0.00644373\n",
      "Iteration 882, loss = 0.00641463\n",
      "Iteration 883, loss = 0.00637482\n",
      "Iteration 884, loss = 0.00639697\n",
      "Iteration 885, loss = 0.00649579\n",
      "Iteration 886, loss = 0.00640741\n",
      "Iteration 887, loss = 0.00632394\n",
      "Iteration 888, loss = 0.00644480\n",
      "Iteration 889, loss = 0.00635862\n",
      "Iteration 890, loss = 0.00637117\n",
      "Iteration 891, loss = 0.00642739\n",
      "Iteration 892, loss = 0.00641772\n",
      "Iteration 893, loss = 0.00637851\n",
      "Iteration 894, loss = 0.00633484\n",
      "Iteration 895, loss = 0.00625125\n",
      "Iteration 896, loss = 0.00625509\n",
      "Iteration 897, loss = 0.00631542\n",
      "Iteration 898, loss = 0.00630072\n",
      "Iteration 899, loss = 0.00626309\n",
      "Iteration 900, loss = 0.00626844\n",
      "Iteration 901, loss = 0.00629160\n",
      "Iteration 902, loss = 0.00623798\n",
      "Iteration 903, loss = 0.00628536\n",
      "Iteration 904, loss = 0.00619091\n",
      "Iteration 905, loss = 0.00625943\n",
      "Iteration 906, loss = 0.00621726\n",
      "Iteration 907, loss = 0.00624670\n",
      "Iteration 908, loss = 0.00620509\n",
      "Iteration 909, loss = 0.00617333\n",
      "Iteration 910, loss = 0.00617583\n",
      "Iteration 911, loss = 0.00617234\n",
      "Iteration 912, loss = 0.00613891\n",
      "Iteration 913, loss = 0.00617273\n",
      "Iteration 914, loss = 0.00610776\n",
      "Iteration 915, loss = 0.00616943\n",
      "Iteration 916, loss = 0.00615412\n",
      "Iteration 917, loss = 0.00615662\n",
      "Iteration 918, loss = 0.00625599\n",
      "Iteration 919, loss = 0.00605383\n",
      "Iteration 920, loss = 0.00605795\n",
      "Iteration 921, loss = 0.00606333\n",
      "Iteration 922, loss = 0.00609271\n",
      "Iteration 923, loss = 0.00618648\n",
      "Iteration 924, loss = 0.00605015\n",
      "Iteration 925, loss = 0.00602177\n",
      "Iteration 926, loss = 0.00608714\n",
      "Iteration 927, loss = 0.00598818\n",
      "Iteration 928, loss = 0.00605264\n",
      "Iteration 929, loss = 0.00605579\n",
      "Iteration 930, loss = 0.00599805\n",
      "Iteration 931, loss = 0.00602787\n",
      "Iteration 932, loss = 0.00597686\n",
      "Iteration 933, loss = 0.00600343\n",
      "Iteration 934, loss = 0.00596587\n",
      "Iteration 935, loss = 0.00599088\n",
      "Iteration 936, loss = 0.00596235\n",
      "Iteration 937, loss = 0.00602518\n",
      "Iteration 938, loss = 0.00589886\n",
      "Iteration 939, loss = 0.00603463\n",
      "Iteration 940, loss = 0.00604421\n",
      "Iteration 941, loss = 0.00591434\n",
      "Iteration 942, loss = 0.00590378\n",
      "Iteration 943, loss = 0.00591319\n",
      "Iteration 944, loss = 0.00587538\n",
      "Iteration 945, loss = 0.00585170\n",
      "Iteration 946, loss = 0.00590131\n",
      "Iteration 947, loss = 0.00590661\n",
      "Iteration 948, loss = 0.00584567\n",
      "Iteration 949, loss = 0.00592851\n",
      "Iteration 950, loss = 0.00580495\n",
      "Iteration 951, loss = 0.00580181\n",
      "Iteration 952, loss = 0.00583550\n",
      "Iteration 953, loss = 0.00580951\n",
      "Iteration 954, loss = 0.00590909\n",
      "Iteration 955, loss = 0.00590983\n",
      "Iteration 956, loss = 0.00580109\n",
      "Iteration 957, loss = 0.00586991\n",
      "Iteration 958, loss = 0.00574391\n",
      "Iteration 959, loss = 0.00575837\n",
      "Iteration 960, loss = 0.00578098\n",
      "Iteration 961, loss = 0.00581142\n",
      "Iteration 962, loss = 0.00576133\n",
      "Iteration 963, loss = 0.00574925\n",
      "Iteration 964, loss = 0.00571138\n",
      "Iteration 965, loss = 0.00573966\n",
      "Iteration 966, loss = 0.00572466\n",
      "Iteration 967, loss = 0.00571135\n",
      "Iteration 968, loss = 0.00573599\n",
      "Iteration 969, loss = 0.00568944\n",
      "Iteration 970, loss = 0.00573538\n",
      "Iteration 971, loss = 0.00579057\n",
      "Iteration 972, loss = 0.00566625\n",
      "Iteration 973, loss = 0.00565930\n",
      "Iteration 974, loss = 0.00564851\n",
      "Iteration 975, loss = 0.00563511\n",
      "Iteration 976, loss = 0.00569168\n",
      "Iteration 977, loss = 0.00570088\n",
      "Iteration 978, loss = 0.00571055\n",
      "Iteration 979, loss = 0.00563011\n",
      "Iteration 980, loss = 0.00576292\n",
      "Iteration 981, loss = 0.00564318\n",
      "Iteration 982, loss = 0.00557074\n",
      "Iteration 983, loss = 0.00558517\n",
      "Iteration 984, loss = 0.00553904\n",
      "Iteration 985, loss = 0.00558546\n",
      "Iteration 986, loss = 0.00559451\n",
      "Iteration 987, loss = 0.00555441\n",
      "Iteration 988, loss = 0.00553481\n",
      "Iteration 989, loss = 0.00554192\n",
      "Iteration 990, loss = 0.00552803\n",
      "Iteration 991, loss = 0.00559836\n",
      "Iteration 992, loss = 0.00550084\n",
      "Iteration 993, loss = 0.00562877\n",
      "Iteration 994, loss = 0.00552101\n",
      "Iteration 995, loss = 0.00556474\n",
      "Iteration 996, loss = 0.00551789\n",
      "Iteration 997, loss = 0.00559107\n",
      "Iteration 998, loss = 0.00545144\n",
      "Iteration 999, loss = 0.00554727\n",
      "Iteration 1000, loss = 0.00589198\n",
      "Iteration 1, loss = 0.75646546\n",
      "Iteration 2, loss = 0.67166116\n",
      "Iteration 3, loss = 0.59727672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lindino/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.53500624\n",
      "Iteration 5, loss = 0.48212798\n",
      "Iteration 6, loss = 0.43728457\n",
      "Iteration 7, loss = 0.39939367\n",
      "Iteration 8, loss = 0.36719495\n",
      "Iteration 9, loss = 0.33944930\n",
      "Iteration 10, loss = 0.31573889\n",
      "Iteration 11, loss = 0.29494681\n",
      "Iteration 12, loss = 0.27684423\n",
      "Iteration 13, loss = 0.26103421\n",
      "Iteration 14, loss = 0.24681272\n",
      "Iteration 15, loss = 0.23410951\n",
      "Iteration 16, loss = 0.22275525\n",
      "Iteration 17, loss = 0.21239112\n",
      "Iteration 18, loss = 0.20298541\n",
      "Iteration 19, loss = 0.19427359\n",
      "Iteration 20, loss = 0.18653081\n",
      "Iteration 21, loss = 0.17927900\n",
      "Iteration 22, loss = 0.17260929\n",
      "Iteration 23, loss = 0.16667683\n",
      "Iteration 24, loss = 0.16093875\n",
      "Iteration 25, loss = 0.15567814\n",
      "Iteration 26, loss = 0.15076273\n",
      "Iteration 27, loss = 0.14636656\n",
      "Iteration 28, loss = 0.14196641\n",
      "Iteration 29, loss = 0.13805559\n",
      "Iteration 30, loss = 0.13425898\n",
      "Iteration 31, loss = 0.13079134\n",
      "Iteration 32, loss = 0.12742091\n",
      "Iteration 33, loss = 0.12436462\n",
      "Iteration 34, loss = 0.12124730\n",
      "Iteration 35, loss = 0.11844896\n",
      "Iteration 36, loss = 0.11576666\n",
      "Iteration 37, loss = 0.11328133\n",
      "Iteration 38, loss = 0.11080763\n",
      "Iteration 39, loss = 0.10842866\n",
      "Iteration 40, loss = 0.10629843\n",
      "Iteration 41, loss = 0.10420902\n",
      "Iteration 42, loss = 0.10219091\n",
      "Iteration 43, loss = 0.10028736\n",
      "Iteration 44, loss = 0.09845602\n",
      "Iteration 45, loss = 0.09664396\n",
      "Iteration 46, loss = 0.09503247\n",
      "Iteration 47, loss = 0.09335313\n",
      "Iteration 48, loss = 0.09186142\n",
      "Iteration 49, loss = 0.09039325\n",
      "Iteration 50, loss = 0.08900449\n",
      "Iteration 51, loss = 0.08759851\n",
      "Iteration 52, loss = 0.08623342\n",
      "Iteration 53, loss = 0.08501702\n",
      "Iteration 54, loss = 0.08377728\n",
      "Iteration 55, loss = 0.08254143\n",
      "Iteration 56, loss = 0.08131866\n",
      "Iteration 57, loss = 0.08020344\n",
      "Iteration 58, loss = 0.07904004\n",
      "Iteration 59, loss = 0.07793591\n",
      "Iteration 60, loss = 0.07680981\n",
      "Iteration 61, loss = 0.07585767\n",
      "Iteration 62, loss = 0.07475837\n",
      "Iteration 63, loss = 0.07367194\n",
      "Iteration 64, loss = 0.07270993\n",
      "Iteration 65, loss = 0.07171388\n",
      "Iteration 66, loss = 0.07077642\n",
      "Iteration 67, loss = 0.06985287\n",
      "Iteration 68, loss = 0.06905355\n",
      "Iteration 69, loss = 0.06805760\n",
      "Iteration 70, loss = 0.06721071\n",
      "Iteration 71, loss = 0.06632005\n",
      "Iteration 72, loss = 0.06559276\n",
      "Iteration 73, loss = 0.06469672\n",
      "Iteration 74, loss = 0.06393186\n",
      "Iteration 75, loss = 0.06313861\n",
      "Iteration 76, loss = 0.06240865\n",
      "Iteration 77, loss = 0.06159408\n",
      "Iteration 78, loss = 0.06086006\n",
      "Iteration 79, loss = 0.06006225\n",
      "Iteration 80, loss = 0.05950498\n",
      "Iteration 81, loss = 0.05865296\n",
      "Iteration 82, loss = 0.05801017\n",
      "Iteration 83, loss = 0.05737706\n",
      "Iteration 84, loss = 0.05668570\n",
      "Iteration 85, loss = 0.05614932\n",
      "Iteration 86, loss = 0.05551562\n",
      "Iteration 87, loss = 0.05491091\n",
      "Iteration 88, loss = 0.05425355\n",
      "Iteration 89, loss = 0.05375691\n",
      "Iteration 90, loss = 0.05317975\n",
      "Iteration 91, loss = 0.05263185\n",
      "Iteration 92, loss = 0.05212048\n",
      "Iteration 93, loss = 0.05159023\n",
      "Iteration 94, loss = 0.05100539\n",
      "Iteration 95, loss = 0.05046974\n",
      "Iteration 96, loss = 0.05006412\n",
      "Iteration 97, loss = 0.04962759\n",
      "Iteration 98, loss = 0.04907637\n",
      "Iteration 99, loss = 0.04850625\n",
      "Iteration 100, loss = 0.04816865\n",
      "Iteration 101, loss = 0.04763411\n",
      "Iteration 102, loss = 0.04717497\n",
      "Iteration 103, loss = 0.04676512\n",
      "Iteration 104, loss = 0.04650088\n",
      "Iteration 105, loss = 0.04587208\n",
      "Iteration 106, loss = 0.04549649\n",
      "Iteration 107, loss = 0.04511400\n",
      "Iteration 108, loss = 0.04469022\n",
      "Iteration 109, loss = 0.04429417\n",
      "Iteration 110, loss = 0.04394955\n",
      "Iteration 111, loss = 0.04361988\n",
      "Iteration 112, loss = 0.04316452\n",
      "Iteration 113, loss = 0.04285013\n",
      "Iteration 114, loss = 0.04245037\n",
      "Iteration 115, loss = 0.04210871\n",
      "Iteration 116, loss = 0.04176512\n",
      "Iteration 117, loss = 0.04138648\n",
      "Iteration 118, loss = 0.04113238\n",
      "Iteration 119, loss = 0.04075241\n",
      "Iteration 120, loss = 0.04039543\n",
      "Iteration 121, loss = 0.04010984\n",
      "Iteration 122, loss = 0.03976207\n",
      "Iteration 123, loss = 0.03948039\n",
      "Iteration 124, loss = 0.03916441\n",
      "Iteration 125, loss = 0.03889459\n",
      "Iteration 126, loss = 0.03856977\n",
      "Iteration 127, loss = 0.03832416\n",
      "Iteration 128, loss = 0.03801533\n",
      "Iteration 129, loss = 0.03776994\n",
      "Iteration 130, loss = 0.03751429\n",
      "Iteration 131, loss = 0.03724093\n",
      "Iteration 132, loss = 0.03691937\n",
      "Iteration 133, loss = 0.03667502\n",
      "Iteration 134, loss = 0.03641115\n",
      "Iteration 135, loss = 0.03613760\n",
      "Iteration 136, loss = 0.03588971\n",
      "Iteration 137, loss = 0.03567403\n",
      "Iteration 138, loss = 0.03541767\n",
      "Iteration 139, loss = 0.03511586\n",
      "Iteration 140, loss = 0.03504258\n",
      "Iteration 141, loss = 0.03465042\n",
      "Iteration 142, loss = 0.03441125\n",
      "Iteration 143, loss = 0.03418095\n",
      "Iteration 144, loss = 0.03397759\n",
      "Iteration 145, loss = 0.03376657\n",
      "Iteration 146, loss = 0.03355281\n",
      "Iteration 147, loss = 0.03329293\n",
      "Iteration 148, loss = 0.03316509\n",
      "Iteration 149, loss = 0.03291927\n",
      "Iteration 150, loss = 0.03265744\n",
      "Iteration 151, loss = 0.03242576\n",
      "Iteration 152, loss = 0.03224561\n",
      "Iteration 153, loss = 0.03202813\n",
      "Iteration 154, loss = 0.03185117\n",
      "Iteration 155, loss = 0.03164962\n",
      "Iteration 156, loss = 0.03143224\n",
      "Iteration 157, loss = 0.03123226\n",
      "Iteration 158, loss = 0.03104334\n",
      "Iteration 159, loss = 0.03085259\n",
      "Iteration 160, loss = 0.03069655\n",
      "Iteration 161, loss = 0.03050914\n",
      "Iteration 162, loss = 0.03031939\n",
      "Iteration 163, loss = 0.03023398\n",
      "Iteration 164, loss = 0.03005390\n",
      "Iteration 165, loss = 0.02983702\n",
      "Iteration 166, loss = 0.02969537\n",
      "Iteration 167, loss = 0.02945573\n",
      "Iteration 168, loss = 0.02926992\n",
      "Iteration 169, loss = 0.02910486\n",
      "Iteration 170, loss = 0.02892585\n",
      "Iteration 171, loss = 0.02879189\n",
      "Iteration 172, loss = 0.02859007\n",
      "Iteration 173, loss = 0.02843139\n",
      "Iteration 174, loss = 0.02825400\n",
      "Iteration 175, loss = 0.02814652\n",
      "Iteration 176, loss = 0.02797571\n",
      "Iteration 177, loss = 0.02787401\n",
      "Iteration 178, loss = 0.02765906\n",
      "Iteration 179, loss = 0.02752083\n",
      "Iteration 180, loss = 0.02742811\n",
      "Iteration 181, loss = 0.02734461\n",
      "Iteration 182, loss = 0.02708712\n",
      "Iteration 183, loss = 0.02695209\n",
      "Iteration 184, loss = 0.02683730\n",
      "Iteration 185, loss = 0.02669293\n",
      "Iteration 186, loss = 0.02656012\n",
      "Iteration 187, loss = 0.02642939\n",
      "Iteration 188, loss = 0.02628583\n",
      "Iteration 189, loss = 0.02626976\n",
      "Iteration 190, loss = 0.02602397\n",
      "Iteration 191, loss = 0.02591845\n",
      "Iteration 192, loss = 0.02575676\n",
      "Iteration 193, loss = 0.02565385\n",
      "Iteration 194, loss = 0.02554870\n",
      "Iteration 195, loss = 0.02553939\n",
      "Iteration 196, loss = 0.02528981\n",
      "Iteration 197, loss = 0.02522594\n",
      "Iteration 198, loss = 0.02504158\n",
      "Iteration 199, loss = 0.02489950\n",
      "Iteration 200, loss = 0.02483333\n",
      "Iteration 201, loss = 0.02468289\n",
      "Iteration 202, loss = 0.02460562\n",
      "Iteration 203, loss = 0.02443593\n",
      "Iteration 204, loss = 0.02437169\n",
      "Iteration 205, loss = 0.02422205\n",
      "Iteration 206, loss = 0.02410531\n",
      "Iteration 207, loss = 0.02399211\n",
      "Iteration 208, loss = 0.02391735\n",
      "Iteration 209, loss = 0.02380320\n",
      "Iteration 210, loss = 0.02369624\n",
      "Iteration 211, loss = 0.02364485\n",
      "Iteration 212, loss = 0.02347446\n",
      "Iteration 213, loss = 0.02335503\n",
      "Iteration 214, loss = 0.02323709\n",
      "Iteration 215, loss = 0.02315939\n",
      "Iteration 216, loss = 0.02309110\n",
      "Iteration 217, loss = 0.02293526\n",
      "Iteration 218, loss = 0.02288671\n",
      "Iteration 219, loss = 0.02280588\n",
      "Iteration 220, loss = 0.02265867\n",
      "Iteration 221, loss = 0.02264583\n",
      "Iteration 222, loss = 0.02243989\n",
      "Iteration 223, loss = 0.02244940\n",
      "Iteration 224, loss = 0.02236300\n",
      "Iteration 225, loss = 0.02217760\n",
      "Iteration 226, loss = 0.02214409\n",
      "Iteration 227, loss = 0.02209521\n",
      "Iteration 228, loss = 0.02199434\n",
      "Iteration 229, loss = 0.02184726\n",
      "Iteration 230, loss = 0.02173650\n",
      "Iteration 231, loss = 0.02163494\n",
      "Iteration 232, loss = 0.02163144\n",
      "Iteration 233, loss = 0.02143883\n",
      "Iteration 234, loss = 0.02137933\n",
      "Iteration 235, loss = 0.02126446\n",
      "Iteration 236, loss = 0.02117588\n",
      "Iteration 237, loss = 0.02109193\n",
      "Iteration 238, loss = 0.02101067\n",
      "Iteration 239, loss = 0.02094930\n",
      "Iteration 240, loss = 0.02094456\n",
      "Iteration 241, loss = 0.02079912\n",
      "Iteration 242, loss = 0.02071260\n",
      "Iteration 243, loss = 0.02058634\n",
      "Iteration 244, loss = 0.02051210\n",
      "Iteration 245, loss = 0.02051252\n",
      "Iteration 246, loss = 0.02045376\n",
      "Iteration 247, loss = 0.02036929\n",
      "Iteration 248, loss = 0.02022724\n",
      "Iteration 249, loss = 0.02018112\n",
      "Iteration 250, loss = 0.02006958\n",
      "Iteration 251, loss = 0.01998983\n",
      "Iteration 252, loss = 0.02006731\n",
      "Iteration 253, loss = 0.01989231\n",
      "Iteration 254, loss = 0.01981038\n",
      "Iteration 255, loss = 0.01971660\n",
      "Iteration 256, loss = 0.01959071\n",
      "Iteration 257, loss = 0.01952697\n",
      "Iteration 258, loss = 0.01974011\n",
      "Iteration 259, loss = 0.01949237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 260, loss = 0.01932647\n",
      "Iteration 261, loss = 0.01935984\n",
      "Iteration 262, loss = 0.01931308\n",
      "Iteration 263, loss = 0.01911925\n",
      "Iteration 264, loss = 0.01902833\n",
      "Iteration 265, loss = 0.01896547\n",
      "Iteration 266, loss = 0.01889032\n",
      "Iteration 267, loss = 0.01889739\n",
      "Iteration 268, loss = 0.01873488\n",
      "Iteration 269, loss = 0.01878511\n",
      "Iteration 270, loss = 0.01862208\n",
      "Iteration 271, loss = 0.01857082\n",
      "Iteration 272, loss = 0.01849511\n",
      "Iteration 273, loss = 0.01848554\n",
      "Iteration 274, loss = 0.01834522\n",
      "Iteration 275, loss = 0.01830727\n",
      "Iteration 276, loss = 0.01821202\n",
      "Iteration 277, loss = 0.01817702\n",
      "Iteration 278, loss = 0.01811774\n",
      "Iteration 279, loss = 0.01814024\n",
      "Iteration 280, loss = 0.01796520\n",
      "Iteration 281, loss = 0.01793784\n",
      "Iteration 282, loss = 0.01790070\n",
      "Iteration 283, loss = 0.01782635\n",
      "Iteration 284, loss = 0.01773019\n",
      "Iteration 285, loss = 0.01773708\n",
      "Iteration 286, loss = 0.01760862\n",
      "Iteration 287, loss = 0.01762766\n",
      "Iteration 288, loss = 0.01755486\n",
      "Iteration 289, loss = 0.01744432\n",
      "Iteration 290, loss = 0.01738615\n",
      "Iteration 291, loss = 0.01733526\n",
      "Iteration 292, loss = 0.01735721\n",
      "Iteration 293, loss = 0.01726516\n",
      "Iteration 294, loss = 0.01729316\n",
      "Iteration 295, loss = 0.01728735\n",
      "Iteration 296, loss = 0.01711452\n",
      "Iteration 297, loss = 0.01695103\n",
      "Iteration 298, loss = 0.01694309\n",
      "Iteration 299, loss = 0.01690207\n",
      "Iteration 300, loss = 0.01684009\n",
      "Iteration 301, loss = 0.01674277\n",
      "Iteration 302, loss = 0.01672321\n",
      "Iteration 303, loss = 0.01674488\n",
      "Iteration 304, loss = 0.01655831\n",
      "Iteration 305, loss = 0.01658633\n",
      "Iteration 306, loss = 0.01648297\n",
      "Iteration 307, loss = 0.01647597\n",
      "Iteration 308, loss = 0.01642827\n",
      "Iteration 309, loss = 0.01634333\n",
      "Iteration 310, loss = 0.01628638\n",
      "Iteration 311, loss = 0.01633626\n",
      "Iteration 312, loss = 0.01621615\n",
      "Iteration 313, loss = 0.01622732\n",
      "Iteration 314, loss = 0.01608753\n",
      "Iteration 315, loss = 0.01608725\n",
      "Iteration 316, loss = 0.01609813\n",
      "Iteration 317, loss = 0.01591247\n",
      "Iteration 318, loss = 0.01593586\n",
      "Iteration 319, loss = 0.01582847\n",
      "Iteration 320, loss = 0.01580405\n",
      "Iteration 321, loss = 0.01574357\n",
      "Iteration 322, loss = 0.01573912\n",
      "Iteration 323, loss = 0.01564590\n",
      "Iteration 324, loss = 0.01561719\n",
      "Iteration 325, loss = 0.01553748\n",
      "Iteration 326, loss = 0.01547693\n",
      "Iteration 327, loss = 0.01546997\n",
      "Iteration 328, loss = 0.01543004\n",
      "Iteration 329, loss = 0.01537683\n",
      "Iteration 330, loss = 0.01531698\n",
      "Iteration 331, loss = 0.01529375\n",
      "Iteration 332, loss = 0.01524768\n",
      "Iteration 333, loss = 0.01516736\n",
      "Iteration 334, loss = 0.01514891\n",
      "Iteration 335, loss = 0.01505181\n",
      "Iteration 336, loss = 0.01508834\n",
      "Iteration 337, loss = 0.01500733\n",
      "Iteration 338, loss = 0.01492848\n",
      "Iteration 339, loss = 0.01491064\n",
      "Iteration 340, loss = 0.01488457\n",
      "Iteration 341, loss = 0.01481908\n",
      "Iteration 342, loss = 0.01479196\n",
      "Iteration 343, loss = 0.01475947\n",
      "Iteration 344, loss = 0.01469858\n",
      "Iteration 345, loss = 0.01463092\n",
      "Iteration 346, loss = 0.01465882\n",
      "Iteration 347, loss = 0.01465923\n",
      "Iteration 348, loss = 0.01455423\n",
      "Iteration 349, loss = 0.01447426\n",
      "Iteration 350, loss = 0.01446881\n",
      "Iteration 351, loss = 0.01444716\n",
      "Iteration 352, loss = 0.01438318\n",
      "Iteration 353, loss = 0.01435999\n",
      "Iteration 354, loss = 0.01426777\n",
      "Iteration 355, loss = 0.01424258\n",
      "Iteration 356, loss = 0.01419281\n",
      "Iteration 357, loss = 0.01422570\n",
      "Iteration 358, loss = 0.01413462\n",
      "Iteration 359, loss = 0.01407589\n",
      "Iteration 360, loss = 0.01410132\n",
      "Iteration 361, loss = 0.01403499\n",
      "Iteration 362, loss = 0.01407360\n",
      "Iteration 363, loss = 0.01392473\n",
      "Iteration 364, loss = 0.01389673\n",
      "Iteration 365, loss = 0.01391751\n",
      "Iteration 366, loss = 0.01384203\n",
      "Iteration 367, loss = 0.01380534\n",
      "Iteration 368, loss = 0.01392243\n",
      "Iteration 369, loss = 0.01372443\n",
      "Iteration 370, loss = 0.01369731\n",
      "Iteration 371, loss = 0.01367671\n",
      "Iteration 372, loss = 0.01362270\n",
      "Iteration 373, loss = 0.01354132\n",
      "Iteration 374, loss = 0.01353621\n",
      "Iteration 375, loss = 0.01352989\n",
      "Iteration 376, loss = 0.01341254\n",
      "Iteration 377, loss = 0.01345054\n",
      "Iteration 378, loss = 0.01340534\n",
      "Iteration 379, loss = 0.01330270\n",
      "Iteration 380, loss = 0.01332358\n",
      "Iteration 381, loss = 0.01349240\n",
      "Iteration 382, loss = 0.01325921\n",
      "Iteration 383, loss = 0.01314974\n",
      "Iteration 384, loss = 0.01321146\n",
      "Iteration 385, loss = 0.01312712\n",
      "Iteration 386, loss = 0.01313642\n",
      "Iteration 387, loss = 0.01307007\n",
      "Iteration 388, loss = 0.01303292\n",
      "Iteration 389, loss = 0.01297055\n",
      "Iteration 390, loss = 0.01294566\n",
      "Iteration 391, loss = 0.01294449\n",
      "Iteration 392, loss = 0.01288352\n",
      "Iteration 393, loss = 0.01284601\n",
      "Iteration 394, loss = 0.01283568\n",
      "Iteration 395, loss = 0.01292930\n",
      "Iteration 396, loss = 0.01271584\n",
      "Iteration 397, loss = 0.01271833\n",
      "Iteration 398, loss = 0.01268539\n",
      "Iteration 399, loss = 0.01269654\n",
      "Iteration 400, loss = 0.01259271\n",
      "Iteration 401, loss = 0.01259139\n",
      "Iteration 402, loss = 0.01258258\n",
      "Iteration 403, loss = 0.01255357\n",
      "Iteration 404, loss = 0.01248623\n",
      "Iteration 405, loss = 0.01254747\n",
      "Iteration 406, loss = 0.01244479\n",
      "Iteration 407, loss = 0.01242766\n",
      "Iteration 408, loss = 0.01244411\n",
      "Iteration 409, loss = 0.01230938\n",
      "Iteration 410, loss = 0.01227153\n",
      "Iteration 411, loss = 0.01228819\n",
      "Iteration 412, loss = 0.01226531\n",
      "Iteration 413, loss = 0.01217094\n",
      "Iteration 414, loss = 0.01229646\n",
      "Iteration 415, loss = 0.01211519\n",
      "Iteration 416, loss = 0.01210714\n",
      "Iteration 417, loss = 0.01208734\n",
      "Iteration 418, loss = 0.01205018\n",
      "Iteration 419, loss = 0.01202308\n",
      "Iteration 420, loss = 0.01207005\n",
      "Iteration 421, loss = 0.01194811\n",
      "Iteration 422, loss = 0.01195644\n",
      "Iteration 423, loss = 0.01193999\n",
      "Iteration 424, loss = 0.01191344\n",
      "Iteration 425, loss = 0.01185374\n",
      "Iteration 426, loss = 0.01180653\n",
      "Iteration 427, loss = 0.01185075\n",
      "Iteration 428, loss = 0.01174335\n",
      "Iteration 429, loss = 0.01171541\n",
      "Iteration 430, loss = 0.01176284\n",
      "Iteration 431, loss = 0.01168170\n",
      "Iteration 432, loss = 0.01163507\n",
      "Iteration 433, loss = 0.01164505\n",
      "Iteration 434, loss = 0.01159868\n",
      "Iteration 435, loss = 0.01155458\n",
      "Iteration 436, loss = 0.01159086\n",
      "Iteration 437, loss = 0.01152688\n",
      "Iteration 438, loss = 0.01154028\n",
      "Iteration 439, loss = 0.01154794\n",
      "Iteration 440, loss = 0.01141479\n",
      "Iteration 441, loss = 0.01137466\n",
      "Iteration 442, loss = 0.01146827\n",
      "Iteration 443, loss = 0.01131939\n",
      "Iteration 444, loss = 0.01135635\n",
      "Iteration 445, loss = 0.01131860\n",
      "Iteration 446, loss = 0.01133862\n",
      "Iteration 447, loss = 0.01121618\n",
      "Iteration 448, loss = 0.01118223\n",
      "Iteration 449, loss = 0.01118856\n",
      "Iteration 450, loss = 0.01120283\n",
      "Iteration 451, loss = 0.01127641\n",
      "Iteration 452, loss = 0.01110701\n",
      "Iteration 453, loss = 0.01107535\n",
      "Iteration 454, loss = 0.01108043\n",
      "Iteration 455, loss = 0.01107041\n",
      "Iteration 456, loss = 0.01100546\n",
      "Iteration 457, loss = 0.01101677\n",
      "Iteration 458, loss = 0.01099432\n",
      "Iteration 459, loss = 0.01095046\n",
      "Iteration 460, loss = 0.01090527\n",
      "Iteration 461, loss = 0.01091016\n",
      "Iteration 462, loss = 0.01083810\n",
      "Iteration 463, loss = 0.01080011\n",
      "Iteration 464, loss = 0.01082838\n",
      "Iteration 465, loss = 0.01079768\n",
      "Iteration 466, loss = 0.01075528\n",
      "Iteration 467, loss = 0.01083746\n",
      "Iteration 468, loss = 0.01075003\n",
      "Iteration 469, loss = 0.01068365\n",
      "Iteration 470, loss = 0.01062478\n",
      "Iteration 471, loss = 0.01062264\n",
      "Iteration 472, loss = 0.01058980\n",
      "Iteration 473, loss = 0.01063478\n",
      "Iteration 474, loss = 0.01053999\n",
      "Iteration 475, loss = 0.01054890\n",
      "Iteration 476, loss = 0.01048971\n",
      "Iteration 477, loss = 0.01048610\n",
      "Iteration 478, loss = 0.01052760\n",
      "Iteration 479, loss = 0.01042391\n",
      "Iteration 480, loss = 0.01041807\n",
      "Iteration 481, loss = 0.01040982\n",
      "Iteration 482, loss = 0.01034928\n",
      "Iteration 483, loss = 0.01036760\n",
      "Iteration 484, loss = 0.01042565\n",
      "Iteration 485, loss = 0.01030036\n",
      "Iteration 486, loss = 0.01030646\n",
      "Iteration 487, loss = 0.01031041\n",
      "Iteration 488, loss = 0.01022722\n",
      "Iteration 489, loss = 0.01017619\n",
      "Iteration 490, loss = 0.01018158\n",
      "Iteration 491, loss = 0.01014142\n",
      "Iteration 492, loss = 0.01025239\n",
      "Iteration 493, loss = 0.01017802\n",
      "Iteration 494, loss = 0.01014034\n",
      "Iteration 495, loss = 0.01014043\n",
      "Iteration 496, loss = 0.01006254\n",
      "Iteration 497, loss = 0.01001817\n",
      "Iteration 498, loss = 0.01006739\n",
      "Iteration 499, loss = 0.01000325\n",
      "Iteration 500, loss = 0.01011004\n",
      "Iteration 501, loss = 0.00998022\n",
      "Iteration 502, loss = 0.01008134\n",
      "Iteration 503, loss = 0.00989511\n",
      "Iteration 504, loss = 0.00993177\n",
      "Iteration 505, loss = 0.00982520\n",
      "Iteration 506, loss = 0.00989346\n",
      "Iteration 507, loss = 0.00979014\n",
      "Iteration 508, loss = 0.00991960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 509, loss = 0.00973418\n",
      "Iteration 510, loss = 0.00970400\n",
      "Iteration 511, loss = 0.00973297\n",
      "Iteration 512, loss = 0.00981305\n",
      "Iteration 513, loss = 0.00968919\n",
      "Iteration 514, loss = 0.00969238\n",
      "Iteration 515, loss = 0.00966697\n",
      "Iteration 516, loss = 0.00957502\n",
      "Iteration 517, loss = 0.00960822\n",
      "Iteration 518, loss = 0.00955610\n",
      "Iteration 519, loss = 0.00955843\n",
      "Iteration 520, loss = 0.00951598\n",
      "Iteration 521, loss = 0.00951184\n",
      "Iteration 522, loss = 0.00950089\n",
      "Iteration 523, loss = 0.00950519\n",
      "Iteration 524, loss = 0.00942071\n",
      "Iteration 525, loss = 0.00948736\n",
      "Iteration 526, loss = 0.00959760\n",
      "Iteration 527, loss = 0.00945011\n",
      "Iteration 528, loss = 0.00942077\n",
      "Iteration 529, loss = 0.00935640\n",
      "Iteration 530, loss = 0.00940691\n",
      "Iteration 531, loss = 0.00932531\n",
      "Iteration 532, loss = 0.00933124\n",
      "Iteration 533, loss = 0.00928836\n",
      "Iteration 534, loss = 0.00928129\n",
      "Iteration 535, loss = 0.00932641\n",
      "Iteration 536, loss = 0.00929604\n",
      "Iteration 537, loss = 0.00923017\n",
      "Iteration 538, loss = 0.00929036\n",
      "Iteration 539, loss = 0.00921531\n",
      "Iteration 540, loss = 0.00916058\n",
      "Iteration 541, loss = 0.00912285\n",
      "Iteration 542, loss = 0.00911718\n",
      "Iteration 543, loss = 0.00919948\n",
      "Iteration 544, loss = 0.00916904\n",
      "Iteration 545, loss = 0.00910089\n",
      "Iteration 546, loss = 0.00902490\n",
      "Iteration 547, loss = 0.00901851\n",
      "Iteration 548, loss = 0.00911098\n",
      "Iteration 549, loss = 0.00899172\n",
      "Iteration 550, loss = 0.00896783\n",
      "Iteration 551, loss = 0.00894931\n",
      "Iteration 552, loss = 0.00897812\n",
      "Iteration 553, loss = 0.00892950\n",
      "Iteration 554, loss = 0.00902728\n",
      "Iteration 555, loss = 0.00895096\n",
      "Iteration 556, loss = 0.00895116\n",
      "Iteration 557, loss = 0.00884865\n",
      "Iteration 558, loss = 0.00886937\n",
      "Iteration 559, loss = 0.00881840\n",
      "Iteration 560, loss = 0.00876181\n",
      "Iteration 561, loss = 0.00877116\n",
      "Iteration 562, loss = 0.00875365\n",
      "Iteration 563, loss = 0.00871072\n",
      "Iteration 564, loss = 0.00876557\n",
      "Iteration 565, loss = 0.00871517\n",
      "Iteration 566, loss = 0.00868176\n",
      "Iteration 567, loss = 0.00868402\n",
      "Iteration 568, loss = 0.00866259\n",
      "Iteration 569, loss = 0.00863220\n",
      "Iteration 570, loss = 0.00868048\n",
      "Iteration 571, loss = 0.00863895\n",
      "Iteration 572, loss = 0.00858826\n",
      "Iteration 573, loss = 0.00854037\n",
      "Iteration 574, loss = 0.00854302\n",
      "Iteration 575, loss = 0.00853767\n",
      "Iteration 576, loss = 0.00851298\n",
      "Iteration 577, loss = 0.00852496\n",
      "Iteration 578, loss = 0.00854649\n",
      "Iteration 579, loss = 0.00845642\n",
      "Iteration 580, loss = 0.00840904\n",
      "Iteration 581, loss = 0.00845088\n",
      "Iteration 582, loss = 0.00849505\n",
      "Iteration 583, loss = 0.00837214\n",
      "Iteration 584, loss = 0.00841448\n",
      "Iteration 585, loss = 0.00841378\n",
      "Iteration 586, loss = 0.00836923\n",
      "Iteration 587, loss = 0.00828408\n",
      "Iteration 588, loss = 0.00851145\n",
      "Iteration 589, loss = 0.00852263\n",
      "Iteration 590, loss = 0.00829916\n",
      "Iteration 591, loss = 0.00826525\n",
      "Iteration 592, loss = 0.00831699\n",
      "Iteration 593, loss = 0.00829049\n",
      "Iteration 594, loss = 0.00819984\n",
      "Iteration 595, loss = 0.00832168\n",
      "Iteration 596, loss = 0.00818734\n",
      "Iteration 597, loss = 0.00810561\n",
      "Iteration 598, loss = 0.00830650\n",
      "Iteration 599, loss = 0.00827862\n",
      "Iteration 600, loss = 0.00821567\n",
      "Iteration 601, loss = 0.00812546\n",
      "Iteration 602, loss = 0.00813392\n",
      "Iteration 603, loss = 0.00804880\n",
      "Iteration 604, loss = 0.00816154\n",
      "Iteration 605, loss = 0.00806546\n",
      "Iteration 606, loss = 0.00807099\n",
      "Iteration 607, loss = 0.00800410\n",
      "Iteration 608, loss = 0.00805800\n",
      "Iteration 609, loss = 0.00797381\n",
      "Iteration 610, loss = 0.00808638\n",
      "Iteration 611, loss = 0.00796661\n",
      "Iteration 612, loss = 0.00799256\n",
      "Iteration 613, loss = 0.00793245\n",
      "Iteration 614, loss = 0.00790670\n",
      "Iteration 615, loss = 0.00787700\n",
      "Iteration 616, loss = 0.00790291\n",
      "Iteration 617, loss = 0.00791669\n",
      "Iteration 618, loss = 0.00790024\n",
      "Iteration 619, loss = 0.00782523\n",
      "Iteration 620, loss = 0.00784709\n",
      "Iteration 621, loss = 0.00784455\n",
      "Iteration 622, loss = 0.00790460\n",
      "Iteration 623, loss = 0.00775841\n",
      "Iteration 624, loss = 0.00780569\n",
      "Iteration 625, loss = 0.00777887\n",
      "Iteration 626, loss = 0.00772791\n",
      "Iteration 627, loss = 0.00771919\n",
      "Iteration 628, loss = 0.00770242\n",
      "Iteration 629, loss = 0.00776139\n",
      "Iteration 630, loss = 0.00764148\n",
      "Iteration 631, loss = 0.00772102\n",
      "Iteration 632, loss = 0.00772201\n",
      "Iteration 633, loss = 0.00794284\n",
      "Iteration 634, loss = 0.00761783\n",
      "Iteration 635, loss = 0.00762613\n",
      "Iteration 636, loss = 0.00764449\n",
      "Iteration 637, loss = 0.00761976\n",
      "Iteration 638, loss = 0.00755230\n",
      "Iteration 639, loss = 0.00750653\n",
      "Iteration 640, loss = 0.00754743\n",
      "Iteration 641, loss = 0.00763561\n",
      "Iteration 642, loss = 0.00757663\n",
      "Iteration 643, loss = 0.00752832\n",
      "Iteration 644, loss = 0.00748330\n",
      "Iteration 645, loss = 0.00753161\n",
      "Iteration 646, loss = 0.00750820\n",
      "Iteration 647, loss = 0.00745073\n",
      "Iteration 648, loss = 0.00744613\n",
      "Iteration 649, loss = 0.00743740\n",
      "Iteration 650, loss = 0.00743830\n",
      "Iteration 651, loss = 0.00739739\n",
      "Iteration 652, loss = 0.00738208\n",
      "Iteration 653, loss = 0.00738924\n",
      "Iteration 654, loss = 0.00742231\n",
      "Iteration 655, loss = 0.00733441\n",
      "Iteration 656, loss = 0.00730565\n",
      "Iteration 657, loss = 0.00729381\n",
      "Iteration 658, loss = 0.00730210\n",
      "Iteration 659, loss = 0.00729024\n",
      "Iteration 660, loss = 0.00725977\n",
      "Iteration 661, loss = 0.00734204\n",
      "Iteration 662, loss = 0.00724019\n",
      "Iteration 663, loss = 0.00727868\n",
      "Iteration 664, loss = 0.00718916\n",
      "Iteration 665, loss = 0.00729066\n",
      "Iteration 666, loss = 0.00718637\n",
      "Iteration 667, loss = 0.00716486\n",
      "Iteration 668, loss = 0.00715175\n",
      "Iteration 669, loss = 0.00724249\n",
      "Iteration 670, loss = 0.00735876\n",
      "Iteration 671, loss = 0.00717944\n",
      "Iteration 672, loss = 0.00720517\n",
      "Iteration 673, loss = 0.00711066\n",
      "Iteration 674, loss = 0.00712808\n",
      "Iteration 675, loss = 0.00726227\n",
      "Iteration 676, loss = 0.00703435\n",
      "Iteration 677, loss = 0.00709168\n",
      "Iteration 678, loss = 0.00708415\n",
      "Iteration 679, loss = 0.00707409\n",
      "Iteration 680, loss = 0.00706878\n",
      "Iteration 681, loss = 0.00699829\n",
      "Iteration 682, loss = 0.00698031\n",
      "Iteration 683, loss = 0.00704634\n",
      "Iteration 684, loss = 0.00699820\n",
      "Iteration 685, loss = 0.00697057\n",
      "Iteration 686, loss = 0.00700170\n",
      "Iteration 687, loss = 0.00693112\n",
      "Iteration 688, loss = 0.00695575\n",
      "Iteration 689, loss = 0.00692025\n",
      "Iteration 690, loss = 0.00690447\n",
      "Iteration 691, loss = 0.00688841\n",
      "Iteration 692, loss = 0.00685845\n",
      "Iteration 693, loss = 0.00685032\n",
      "Iteration 694, loss = 0.00692063\n",
      "Iteration 695, loss = 0.00684940\n",
      "Iteration 696, loss = 0.00700349\n",
      "Iteration 697, loss = 0.00694262\n",
      "Iteration 698, loss = 0.00685410\n",
      "Iteration 699, loss = 0.00679325\n",
      "Iteration 700, loss = 0.00679549\n",
      "Iteration 701, loss = 0.00678662\n",
      "Iteration 702, loss = 0.00682304\n",
      "Iteration 703, loss = 0.00673221\n",
      "Iteration 704, loss = 0.00684110\n",
      "Iteration 705, loss = 0.00673254\n",
      "Iteration 706, loss = 0.00675675\n",
      "Iteration 707, loss = 0.00678887\n",
      "Iteration 708, loss = 0.00665141\n",
      "Iteration 709, loss = 0.00665052\n",
      "Iteration 710, loss = 0.00666334\n",
      "Iteration 711, loss = 0.00665017\n",
      "Iteration 712, loss = 0.00662976\n",
      "Iteration 713, loss = 0.00667076\n",
      "Iteration 714, loss = 0.00660593\n",
      "Iteration 715, loss = 0.00665158\n",
      "Iteration 716, loss = 0.00667117\n",
      "Iteration 717, loss = 0.00656593\n",
      "Iteration 718, loss = 0.00660687\n",
      "Iteration 719, loss = 0.00655482\n",
      "Iteration 720, loss = 0.00659680\n",
      "Iteration 721, loss = 0.00659057\n",
      "Iteration 722, loss = 0.00653362\n",
      "Iteration 723, loss = 0.00656218\n",
      "Iteration 724, loss = 0.00653903\n",
      "Iteration 725, loss = 0.00650234\n",
      "Iteration 726, loss = 0.00661926\n",
      "Iteration 727, loss = 0.00649740\n",
      "Iteration 728, loss = 0.00655753\n",
      "Iteration 729, loss = 0.00651002\n",
      "Iteration 730, loss = 0.00642870\n",
      "Iteration 731, loss = 0.00652555\n",
      "Iteration 732, loss = 0.00648282\n",
      "Iteration 733, loss = 0.00641211\n",
      "Iteration 734, loss = 0.00644438\n",
      "Iteration 735, loss = 0.00635509\n",
      "Iteration 736, loss = 0.00638674\n",
      "Iteration 737, loss = 0.00637850\n",
      "Iteration 738, loss = 0.00633984\n",
      "Iteration 739, loss = 0.00638325\n",
      "Iteration 740, loss = 0.00635009\n",
      "Iteration 741, loss = 0.00632551\n",
      "Iteration 742, loss = 0.00630303\n",
      "Iteration 743, loss = 0.00631131\n",
      "Iteration 744, loss = 0.00627222\n",
      "Iteration 745, loss = 0.00629337\n",
      "Iteration 746, loss = 0.00628643\n",
      "Iteration 747, loss = 0.00626197\n",
      "Iteration 748, loss = 0.00632363\n",
      "Iteration 749, loss = 0.00640077\n",
      "Iteration 750, loss = 0.00625906\n",
      "Iteration 751, loss = 0.00620320\n",
      "Iteration 752, loss = 0.00641228\n",
      "Iteration 753, loss = 0.00645604\n",
      "Iteration 754, loss = 0.00631655\n",
      "Iteration 755, loss = 0.00622045\n",
      "Iteration 756, loss = 0.00626047\n",
      "Iteration 757, loss = 0.00616862\n",
      "Iteration 758, loss = 0.00620484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 759, loss = 0.00610350\n",
      "Iteration 760, loss = 0.00615940\n",
      "Iteration 761, loss = 0.00609392\n",
      "Iteration 762, loss = 0.00613161\n",
      "Iteration 763, loss = 0.00608367\n",
      "Iteration 764, loss = 0.00614905\n",
      "Iteration 765, loss = 0.00611100\n",
      "Iteration 766, loss = 0.00606343\n",
      "Iteration 767, loss = 0.00613244\n",
      "Iteration 768, loss = 0.00623284\n",
      "Iteration 769, loss = 0.00609003\n",
      "Iteration 770, loss = 0.00607948\n",
      "Iteration 771, loss = 0.00606272\n",
      "Iteration 772, loss = 0.00601915\n",
      "Iteration 773, loss = 0.00599954\n",
      "Iteration 774, loss = 0.00604930\n",
      "Iteration 775, loss = 0.00598314\n",
      "Iteration 776, loss = 0.00596551\n",
      "Iteration 777, loss = 0.00598228\n",
      "Iteration 778, loss = 0.00602286\n",
      "Iteration 779, loss = 0.00592437\n",
      "Iteration 780, loss = 0.00597365\n",
      "Iteration 781, loss = 0.00593176\n",
      "Iteration 782, loss = 0.00595993\n",
      "Iteration 783, loss = 0.00590760\n",
      "Iteration 784, loss = 0.00590857\n",
      "Iteration 785, loss = 0.00593200\n",
      "Iteration 786, loss = 0.00591649\n",
      "Iteration 787, loss = 0.00588196\n",
      "Iteration 788, loss = 0.00589661\n",
      "Iteration 789, loss = 0.00593466\n",
      "Iteration 790, loss = 0.00585489\n",
      "Iteration 791, loss = 0.00589043\n",
      "Iteration 792, loss = 0.00583791\n",
      "Iteration 793, loss = 0.00582358\n",
      "Iteration 794, loss = 0.00589046\n",
      "Iteration 795, loss = 0.00576944\n",
      "Iteration 796, loss = 0.00593197\n",
      "Iteration 797, loss = 0.00600261\n",
      "Iteration 798, loss = 0.00579665\n",
      "Iteration 799, loss = 0.00587643\n",
      "Iteration 800, loss = 0.00577751\n",
      "Iteration 801, loss = 0.00579715\n",
      "Iteration 802, loss = 0.00576642\n",
      "Iteration 803, loss = 0.00572335\n",
      "Iteration 804, loss = 0.00571993\n",
      "Iteration 805, loss = 0.00574237\n",
      "Iteration 806, loss = 0.00574414\n",
      "Iteration 807, loss = 0.00571098\n",
      "Iteration 808, loss = 0.00569995\n",
      "Iteration 809, loss = 0.00567039\n",
      "Iteration 810, loss = 0.00569729\n",
      "Iteration 811, loss = 0.00566394\n",
      "Iteration 812, loss = 0.00566296\n",
      "Iteration 813, loss = 0.00564287\n",
      "Iteration 814, loss = 0.00561827\n",
      "Iteration 815, loss = 0.00571124\n",
      "Iteration 816, loss = 0.00559333\n",
      "Iteration 817, loss = 0.00557590\n",
      "Iteration 818, loss = 0.00557777\n",
      "Iteration 819, loss = 0.00558308\n",
      "Iteration 820, loss = 0.00556768\n",
      "Iteration 821, loss = 0.00558141\n",
      "Iteration 822, loss = 0.00562442\n",
      "Iteration 823, loss = 0.00558263\n",
      "Iteration 824, loss = 0.00552125\n",
      "Iteration 825, loss = 0.00554191\n",
      "Iteration 826, loss = 0.00555720\n",
      "Iteration 827, loss = 0.00559590\n",
      "Iteration 828, loss = 0.00554317\n",
      "Iteration 829, loss = 0.00553464\n",
      "Iteration 830, loss = 0.00550751\n",
      "Iteration 831, loss = 0.00555364\n",
      "Iteration 832, loss = 0.00549917\n",
      "Iteration 833, loss = 0.00552771\n",
      "Iteration 834, loss = 0.00542807\n",
      "Iteration 835, loss = 0.00544751\n",
      "Iteration 836, loss = 0.00550703\n",
      "Iteration 837, loss = 0.00548008\n",
      "Iteration 838, loss = 0.00540866\n",
      "Iteration 839, loss = 0.00542424\n",
      "Iteration 840, loss = 0.00547585\n",
      "Iteration 841, loss = 0.00546564\n",
      "Iteration 842, loss = 0.00539162\n",
      "Iteration 843, loss = 0.00548518\n",
      "Iteration 844, loss = 0.00534678\n",
      "Iteration 845, loss = 0.00537942\n",
      "Iteration 846, loss = 0.00544836\n",
      "Iteration 847, loss = 0.00540878\n",
      "Iteration 848, loss = 0.00536321\n",
      "Iteration 849, loss = 0.00534884\n",
      "Iteration 850, loss = 0.00531977\n",
      "Iteration 851, loss = 0.00533811\n",
      "Iteration 852, loss = 0.00531085\n",
      "Iteration 853, loss = 0.00533771\n",
      "Iteration 854, loss = 0.00530356\n",
      "Iteration 855, loss = 0.00533024\n",
      "Iteration 856, loss = 0.00535843\n",
      "Iteration 857, loss = 0.00531988\n",
      "Iteration 858, loss = 0.00530619\n",
      "Iteration 859, loss = 0.00530236\n",
      "Iteration 860, loss = 0.00525612\n",
      "Iteration 861, loss = 0.00526355\n",
      "Iteration 862, loss = 0.00527578\n",
      "Iteration 863, loss = 0.00522203\n",
      "Iteration 864, loss = 0.00525530\n",
      "Iteration 865, loss = 0.00524129\n",
      "Iteration 866, loss = 0.00527269\n",
      "Iteration 867, loss = 0.00526267\n",
      "Iteration 868, loss = 0.00521416\n",
      "Iteration 869, loss = 0.00519694\n",
      "Iteration 870, loss = 0.00517772\n",
      "Iteration 871, loss = 0.00518528\n",
      "Iteration 872, loss = 0.00517106\n",
      "Iteration 873, loss = 0.00525588\n",
      "Iteration 874, loss = 0.00520818\n",
      "Iteration 875, loss = 0.00515268\n",
      "Iteration 876, loss = 0.00508526\n",
      "Iteration 877, loss = 0.00516973\n",
      "Iteration 878, loss = 0.00510841\n",
      "Iteration 879, loss = 0.00507531\n",
      "Iteration 880, loss = 0.00511788\n",
      "Iteration 881, loss = 0.00508391\n",
      "Iteration 882, loss = 0.00508442\n",
      "Iteration 883, loss = 0.00508820\n",
      "Iteration 884, loss = 0.00508216\n",
      "Iteration 885, loss = 0.00506026\n",
      "Iteration 886, loss = 0.00508735\n",
      "Iteration 887, loss = 0.00512938\n",
      "Iteration 888, loss = 0.00512193\n",
      "Iteration 889, loss = 0.00523850\n",
      "Iteration 890, loss = 0.00507438\n",
      "Iteration 891, loss = 0.00522905\n",
      "Iteration 892, loss = 0.00501592\n",
      "Iteration 893, loss = 0.00501650\n",
      "Iteration 894, loss = 0.00510533\n",
      "Iteration 895, loss = 0.00511282\n",
      "Iteration 896, loss = 0.00498596\n",
      "Iteration 897, loss = 0.00497374\n",
      "Iteration 898, loss = 0.00501541\n",
      "Iteration 899, loss = 0.00498189\n",
      "Iteration 900, loss = 0.00496550\n",
      "Iteration 901, loss = 0.00492572\n",
      "Iteration 902, loss = 0.00499085\n",
      "Iteration 903, loss = 0.00492170\n",
      "Iteration 904, loss = 0.00491424\n",
      "Iteration 905, loss = 0.00493302\n",
      "Iteration 906, loss = 0.00489295\n",
      "Iteration 907, loss = 0.00492698\n",
      "Iteration 908, loss = 0.00492964\n",
      "Iteration 909, loss = 0.00489964\n",
      "Iteration 910, loss = 0.00488622\n",
      "Iteration 911, loss = 0.00498227\n",
      "Iteration 912, loss = 0.00490600\n",
      "Iteration 913, loss = 0.00488818\n",
      "Iteration 914, loss = 0.00483087\n",
      "Iteration 915, loss = 0.00484152\n",
      "Iteration 916, loss = 0.00485923\n",
      "Iteration 917, loss = 0.00482971\n",
      "Iteration 918, loss = 0.00484522\n",
      "Iteration 919, loss = 0.00483268\n",
      "Iteration 920, loss = 0.00491359\n",
      "Iteration 921, loss = 0.00477216\n",
      "Iteration 922, loss = 0.00478982\n",
      "Iteration 923, loss = 0.00487100\n",
      "Iteration 924, loss = 0.00498983\n",
      "Iteration 925, loss = 0.00489092\n",
      "Iteration 926, loss = 0.00483470\n",
      "Iteration 927, loss = 0.00477010\n",
      "Iteration 928, loss = 0.00484654\n",
      "Iteration 929, loss = 0.00474354\n",
      "Iteration 930, loss = 0.00479874\n",
      "Iteration 931, loss = 0.00477449\n",
      "Iteration 932, loss = 0.00474439\n",
      "Iteration 933, loss = 0.00477894\n",
      "Iteration 934, loss = 0.00476254\n",
      "Iteration 935, loss = 0.00472293\n",
      "Iteration 936, loss = 0.00477703\n",
      "Iteration 937, loss = 0.00482720\n",
      "Iteration 938, loss = 0.00481360\n",
      "Iteration 939, loss = 0.00467702\n",
      "Iteration 940, loss = 0.00466270\n",
      "Iteration 941, loss = 0.00476552\n",
      "Iteration 942, loss = 0.00471534\n",
      "Iteration 943, loss = 0.00474178\n",
      "Iteration 944, loss = 0.00474339\n",
      "Iteration 945, loss = 0.00467803\n",
      "Iteration 946, loss = 0.00464757\n",
      "Iteration 947, loss = 0.00464265\n",
      "Iteration 948, loss = 0.00466666\n",
      "Iteration 949, loss = 0.00469557\n",
      "Iteration 950, loss = 0.00470965\n",
      "Iteration 951, loss = 0.00458775\n",
      "Iteration 952, loss = 0.00463471\n",
      "Iteration 953, loss = 0.00460178\n",
      "Iteration 954, loss = 0.00461735\n",
      "Iteration 955, loss = 0.00458802\n",
      "Iteration 956, loss = 0.00461487\n",
      "Iteration 957, loss = 0.00459639\n",
      "Iteration 958, loss = 0.00458302\n",
      "Iteration 959, loss = 0.00459531\n",
      "Iteration 960, loss = 0.00454279\n",
      "Iteration 961, loss = 0.00462237\n",
      "Iteration 962, loss = 0.00458807\n",
      "Iteration 963, loss = 0.00459210\n",
      "Iteration 964, loss = 0.00455665\n",
      "Iteration 965, loss = 0.00452799\n",
      "Iteration 966, loss = 0.00454533\n",
      "Iteration 967, loss = 0.00451896\n",
      "Iteration 968, loss = 0.00458902\n",
      "Iteration 969, loss = 0.00454004\n",
      "Iteration 970, loss = 0.00461382\n",
      "Iteration 971, loss = 0.00451381\n",
      "Iteration 972, loss = 0.00453035\n",
      "Iteration 973, loss = 0.00449466\n",
      "Iteration 974, loss = 0.00446309\n",
      "Iteration 975, loss = 0.00450267\n",
      "Iteration 976, loss = 0.00448257\n",
      "Iteration 977, loss = 0.00451206\n",
      "Iteration 978, loss = 0.00445028\n",
      "Iteration 979, loss = 0.00446963\n",
      "Iteration 980, loss = 0.00448034\n",
      "Iteration 981, loss = 0.00447704\n",
      "Iteration 982, loss = 0.00445593\n",
      "Iteration 983, loss = 0.00447692\n",
      "Iteration 984, loss = 0.00446882\n",
      "Iteration 985, loss = 0.00443896\n",
      "Iteration 986, loss = 0.00446591\n",
      "Iteration 987, loss = 0.00443987\n",
      "Iteration 988, loss = 0.00441079\n",
      "Iteration 989, loss = 0.00441364\n",
      "Iteration 990, loss = 0.00439613\n",
      "Iteration 991, loss = 0.00446090\n",
      "Iteration 992, loss = 0.00437828\n",
      "Iteration 993, loss = 0.00443753\n",
      "Iteration 994, loss = 0.00439881\n",
      "Iteration 995, loss = 0.00457663\n",
      "Iteration 996, loss = 0.00434853\n",
      "Iteration 997, loss = 0.00444151\n",
      "Iteration 998, loss = 0.00433952\n",
      "Iteration 999, loss = 0.00432661\n",
      "Iteration 1000, loss = 0.00432506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lindino/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "MLP = MLPClassifier(verbose = True,\n",
    "                    max_iter = 1000,\n",
    "                    tol = 0.000010,\n",
    "                    solver = 'adam',\n",
    "                    hidden_layer_sizes = (100),\n",
    "                    activation = 'relu')\n",
    "\n",
    "##Divisao normal do dataset\n",
    "\n",
    "'''\n",
    "previsores_train, previsores_test, classe_train, classe_test = dataset_division(previsores, classe)\n",
    "MLP.fit(previsores_train, classe_train)\n",
    "previsoes = MLP.predict(previsores_test)\n",
    "MLP_accuracy = accuracy_score(classe_test, previsoes)\n",
    "matriz = confusion_matrix(classe_test, previsoes)\n",
    "'''\n",
    "\n",
    "##Cross Validation\n",
    "'''\n",
    "MLP_accuracy = cross_validation(MLP, previsores, classe)\n",
    "'''\n",
    "\n",
    "##Stratified Fold\n",
    "MLP_accuracy, _ = stratified_fold(MLP, previsores, classe, 0)\n",
    "save_model(MLP, 'MLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neurais com Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "MLP_keras = Sequential()\n",
    "MLP_keras.add(Dense(units = 2, activation = 'relu', input_dim = 3))\n",
    "MLP_keras.add(Dense(units = 2, activation = 'relu'))\n",
    "MLP_keras.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "MLP_keras.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "MLP_keras.fit(previsores_train, classe_train, batch_size = 10, epochs = 100)\n",
    "\n",
    "previsoes = MLP.predict(previsores_test)\n",
    "previsoes = (previsoes > 0.5)\n",
    "\n",
    "MLP_keras_accuracy = accuracy_score(classe_test, previsoes)\n",
    "matriz = confusion_matrix(classe_test, previsoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste de Friedman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import friedmanchisquare\n",
    "matriz = pd.read_csv('dados_estatisticos.csv')\n",
    "friedmanchisquare(*[row for index, row in matriz.iterrows()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinação de classificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliente não pagaŕa o empŕestimo!\n"
     ]
    }
   ],
   "source": [
    "def paga(resultados, probabilidades, confianca_minima=0.98):\n",
    "    paga = 0\n",
    "    nao_paga = 0\n",
    "    \n",
    "    for i in range(len(resultados)):\n",
    "        if probabilidades[i] >= confianca_minima:\n",
    "            if resultados[i][0] == 1:\n",
    "                paga += 1\n",
    "            else:\n",
    "                nao_paga += 1\n",
    "    \n",
    "    return paga, nao_paga\n",
    "    \n",
    "SVM = load_model('SVM')\n",
    "Random_Forest = load_model('Random_Forest')\n",
    "MLP = load_model('MLP')\n",
    "\n",
    "novo_registro = np.asarray([[50000, 40, 5000]]).reshape(-1,1)\n",
    "scaler = StandardScaler()\n",
    "novo_registro = scaler.fit_transform(novo_registro).reshape(-1,3)\n",
    "\n",
    "resultados = [SVM.predict(novo_registro), Random_Forest.predict(novo_registro), MLP.predict(novo_registro)]\n",
    "probabilidades = [SVM.predict_proba(novo_registro).max(), Random_Forest.predict_proba(novo_registro).max(), MLP.predict_proba(novo_registro).max()]\n",
    "\n",
    "resultado_final = paga(resultados, probabilidades)\n",
    "\n",
    "if resultado_final[0] > resultado_final[1]:\n",
    "    print('Cliente pagará o empréstimo!')\n",
    "elif resultado_final[0] == resultado_final[1]:\n",
    "    print('Resultado empatado!')\n",
    "else:\n",
    "    print('Cliente não pagaŕa o empŕestimo!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mBase Line\u001b[0;0m: \u001b[1;32m87.2%\u001b[0;0m\n",
      "\u001b[1mNaive Bayes\u001b[0;0m: \u001b[1;32m92.5%\u001b[0;0m\n",
      "\u001b[1mDecision Tree\u001b[0;0m: \u001b[1;32m98.5%\u001b[0;0m\n",
      "\u001b[1mRandom Forest\u001b[0;0m: \u001b[1;32m98.8%\u001b[0;0m\n",
      "\u001b[1mKNN\u001b[0;0m: \u001b[1;32m98.1%\u001b[0;0m\n",
      "\u001b[1mLogistic Regression\u001b[0;0m: \u001b[1;32m94.8%\u001b[0;0m\n",
      "\u001b[1mSVM\u001b[0;0m: \u001b[1;32m98.3%\u001b[0;0m\n",
      "\u001b[1mMLP\u001b[0;0m: \u001b[1;32m99.6%\u001b[0;0m\n"
     ]
    }
   ],
   "source": [
    "GREEN = \"\\033[1;32m\" \n",
    "RESET = '\\033[0;0m'\n",
    "NEGRITO = '\\033[1m'\n",
    "models = {'Base Line': base_line,'Naive Bayes': Naive_Bayes_accuracy, 'Decision Tree': Decision_Tree_accuracy,\n",
    "          'Random Forest': Random_Forest_accuracy, 'KNN': KNN_accuracy, 'Logistic Regression': Logistic_Regression_accuracy,\n",
    "          'SVM': SVM_accuracy, 'MLP': MLP_accuracy, 'MLP_Keras': MLP_keras_accuracy}\n",
    "\n",
    "for model in models:\n",
    "    print('{}{}{}: {}{:.1f}%{}'.format(NEGRITO,model,RESET,GREEN,(models[model] * 100), RESET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

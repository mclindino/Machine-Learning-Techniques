{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo e sepando colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "census = pd.read_csv('../dataset/census.csv')\n",
    "\n",
    "previsores = census.iloc[:, 0:14].values\n",
    "classe = census.iloc[:, 14].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformação de variáveis categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_data = LabelEncoder()\n",
    "\n",
    "previsores[:,1] = labelencoder_data.fit_transform(previsores[:,1])\n",
    "previsores[:,3] = labelencoder_data.fit_transform(previsores[:,3])\n",
    "previsores[:,5] = labelencoder_data.fit_transform(previsores[:,5])\n",
    "previsores[:,6] = labelencoder_data.fit_transform(previsores[:,6])\n",
    "previsores[:,7] = labelencoder_data.fit_transform(previsores[:,7])\n",
    "previsores[:,8] = labelencoder_data.fit_transform(previsores[:,8])\n",
    "previsores[:,9] = labelencoder_data.fit_transform(previsores[:,9])\n",
    "previsores[:,13] = labelencoder_data.fit_transform(previsores[:,13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [('one_hot_encoder', \n",
    "      OneHotEncoder(categories='auto'), \n",
    "      [1,3,5,6,7,8,9,13])],   \n",
    "    remainder='passthrough')\n",
    "\n",
    "previsores = ct.fit_transform(previsores).toarray()\n",
    "\n",
    "labelencoder_classe = LabelEncoder()\n",
    "classe = labelencoder_classe.fit_transform(classe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escalonamento dos valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "previsores = scaler.fit_transform(previsores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "previsores_train, previsores_test, classe_train, classe_test = train_test_split(previsores, classe, \n",
    "                                                                    test_size=0.15, \n",
    "                                                                    random_state=0)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes -> Tabela de Probabilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "Naive_Bayes = GaussianNB()\n",
    "Naive_Bayes.fit(previsores_train, classe_train)\n",
    "\n",
    "previsoes = Naive_Bayes.predict(previsores_test)\n",
    "Naive_Bayes_accuracy = accuracy_score(classe_test, previsoes)\n",
    "matriz = confusion_matrix(classe_test, previsoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árvore de Decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "Decision_Tree = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "Decision_Tree.fit(previsores_train, classe_train)\n",
    "\n",
    "previsoes = Decision_Tree.predict(previsores_test)\n",
    "Decision_Tree_accuracy = accuracy_score(classe_test, previsoes)\n",
    "matriz = confusion_matrix(classe_test, previsoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "Random_Forest = RandomForestClassifier(n_estimators = 40,\n",
    "                                      criterion = 'entropy',\n",
    "                                      random_state = 0)\n",
    "Random_Forest.fit(previsores_train, classe_train)\n",
    "\n",
    "previsoes = Random_Forest.predict(previsores_test)\n",
    "Random_Forest_accuracy = accuracy_score(classe_test, previsoes)\n",
    "matriz = confusion_matrix(classe_test, previsoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Line Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## O minimo que o modelo necessita aprender\n",
    "\n",
    "import collections\n",
    "counter = collections.Counter(classe_test)\n",
    "base_line = counter[0] / (counter[0] + counter[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN = KNeighborsClassifier(n_neighbors = 5,\n",
    "                                    metric = 'minkowski',\n",
    "                                    p = 2)\n",
    "KNN.fit(previsores_train, classe_train)\n",
    "previsoes = KNN.predict(previsores_test)\n",
    "\n",
    "KNN_accuracy = accuracy_score(classe_test, previsoes)\n",
    "matriz = confusion_matrix(classe_test, previsoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Logistic_Regression = LogisticRegression(random_state = 0)\n",
    "Logistic_Regression.fit(previsores_train, classe_train)\n",
    "\n",
    "previsoes = Logistic_Regression.predict(previsores_test)\n",
    "\n",
    "Logistic_Regression_accuracy = accuracy_score(classe_test, previsoes)\n",
    "matriz = confusion_matrix(classe_test, previsoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "SVM = SVC(kernel = 'linear', random_state = 0)\n",
    "SVM.fit(previsores_train, classe_train)\n",
    "\n",
    "previsoes = SVM.predict(previsores_test)\n",
    "\n",
    "SVM_accuracy = accuracy_score(classe_test, previsoes)\n",
    "matriz = confusion_matrix(classe_test, previsoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neurais com SkLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39318731\n",
      "Iteration 2, loss = 0.32488956\n",
      "Iteration 3, loss = 0.31468027\n",
      "Iteration 4, loss = 0.30840031\n",
      "Iteration 5, loss = 0.30412810\n",
      "Iteration 6, loss = 0.30060632\n",
      "Iteration 7, loss = 0.29769542\n",
      "Iteration 8, loss = 0.29578873\n",
      "Iteration 9, loss = 0.29395631\n",
      "Iteration 10, loss = 0.29124730\n",
      "Iteration 11, loss = 0.28926160\n",
      "Iteration 12, loss = 0.28772316\n",
      "Iteration 13, loss = 0.28599262\n",
      "Iteration 14, loss = 0.28482784\n",
      "Iteration 15, loss = 0.28323003\n",
      "Iteration 16, loss = 0.28138121\n",
      "Iteration 17, loss = 0.27998739\n",
      "Iteration 18, loss = 0.27796830\n",
      "Iteration 19, loss = 0.27733086\n",
      "Iteration 20, loss = 0.27618096\n",
      "Iteration 21, loss = 0.27498588\n",
      "Iteration 22, loss = 0.27353980\n",
      "Iteration 23, loss = 0.27313299\n",
      "Iteration 24, loss = 0.27182834\n",
      "Iteration 25, loss = 0.27053620\n",
      "Iteration 26, loss = 0.26968592\n",
      "Iteration 27, loss = 0.26871480\n",
      "Iteration 28, loss = 0.26778063\n",
      "Iteration 29, loss = 0.26774988\n",
      "Iteration 30, loss = 0.26621121\n",
      "Iteration 31, loss = 0.26523359\n",
      "Iteration 32, loss = 0.26444413\n",
      "Iteration 33, loss = 0.26371214\n",
      "Iteration 34, loss = 0.26243011\n",
      "Iteration 35, loss = 0.26255292\n",
      "Iteration 36, loss = 0.26108656\n",
      "Iteration 37, loss = 0.26079471\n",
      "Iteration 38, loss = 0.25967962\n",
      "Iteration 39, loss = 0.25903914\n",
      "Iteration 40, loss = 0.25938446\n",
      "Iteration 41, loss = 0.25828591\n",
      "Iteration 42, loss = 0.25759469\n",
      "Iteration 43, loss = 0.25687913\n",
      "Iteration 44, loss = 0.25588679\n",
      "Iteration 45, loss = 0.25478400\n",
      "Iteration 46, loss = 0.25543453\n",
      "Iteration 47, loss = 0.25390788\n",
      "Iteration 48, loss = 0.25316123\n",
      "Iteration 49, loss = 0.25285319\n",
      "Iteration 50, loss = 0.25233860\n",
      "Iteration 51, loss = 0.25167674\n",
      "Iteration 52, loss = 0.25238878\n",
      "Iteration 53, loss = 0.25079397\n",
      "Iteration 54, loss = 0.25051576\n",
      "Iteration 55, loss = 0.25002015\n",
      "Iteration 56, loss = 0.24973869\n",
      "Iteration 57, loss = 0.24894043\n",
      "Iteration 58, loss = 0.24790906\n",
      "Iteration 59, loss = 0.24733068\n",
      "Iteration 60, loss = 0.24730893\n",
      "Iteration 61, loss = 0.24593080\n",
      "Iteration 62, loss = 0.24558740\n",
      "Iteration 63, loss = 0.24500969\n",
      "Iteration 64, loss = 0.24542323\n",
      "Iteration 65, loss = 0.24633317\n",
      "Iteration 66, loss = 0.24368611\n",
      "Iteration 67, loss = 0.24467053\n",
      "Iteration 68, loss = 0.24297137\n",
      "Iteration 69, loss = 0.24230301\n",
      "Iteration 70, loss = 0.24326946\n",
      "Iteration 71, loss = 0.24151651\n",
      "Iteration 72, loss = 0.24270205\n",
      "Iteration 73, loss = 0.24076359\n",
      "Iteration 74, loss = 0.24069989\n",
      "Iteration 75, loss = 0.24088391\n",
      "Iteration 76, loss = 0.24072126\n",
      "Iteration 77, loss = 0.23983341\n",
      "Iteration 78, loss = 0.23962316\n",
      "Iteration 79, loss = 0.23938445\n",
      "Iteration 80, loss = 0.23854722\n",
      "Iteration 81, loss = 0.23730061\n",
      "Iteration 82, loss = 0.23733061\n",
      "Iteration 83, loss = 0.23686874\n",
      "Iteration 84, loss = 0.23708161\n",
      "Iteration 85, loss = 0.23596691\n",
      "Iteration 86, loss = 0.23592682\n",
      "Iteration 87, loss = 0.23613186\n",
      "Iteration 88, loss = 0.23534204\n",
      "Iteration 89, loss = 0.23538414\n",
      "Iteration 90, loss = 0.23454035\n",
      "Iteration 91, loss = 0.23433381\n",
      "Iteration 92, loss = 0.23304446\n",
      "Iteration 93, loss = 0.23339517\n",
      "Iteration 94, loss = 0.23376741\n",
      "Iteration 95, loss = 0.23309806\n",
      "Iteration 96, loss = 0.23176598\n",
      "Iteration 97, loss = 0.23173176\n",
      "Iteration 98, loss = 0.23216026\n",
      "Iteration 99, loss = 0.23201599\n",
      "Iteration 100, loss = 0.23136662\n",
      "Iteration 101, loss = 0.23202473\n",
      "Iteration 102, loss = 0.23040737\n",
      "Iteration 103, loss = 0.23024842\n",
      "Iteration 104, loss = 0.23010912\n",
      "Iteration 105, loss = 0.22916702\n",
      "Iteration 106, loss = 0.22837327\n",
      "Iteration 107, loss = 0.22890654\n",
      "Iteration 108, loss = 0.22829417\n",
      "Iteration 109, loss = 0.22839309\n",
      "Iteration 110, loss = 0.22774288\n",
      "Iteration 111, loss = 0.22821486\n",
      "Iteration 112, loss = 0.22790413\n",
      "Iteration 113, loss = 0.22764031\n",
      "Iteration 114, loss = 0.22747341\n",
      "Iteration 115, loss = 0.22751539\n",
      "Iteration 116, loss = 0.22664120\n",
      "Iteration 117, loss = 0.22686895\n",
      "Iteration 118, loss = 0.22660622\n",
      "Iteration 119, loss = 0.22596604\n",
      "Iteration 120, loss = 0.22478049\n",
      "Iteration 121, loss = 0.22398171\n",
      "Iteration 122, loss = 0.22526076\n",
      "Iteration 123, loss = 0.22473613\n",
      "Iteration 124, loss = 0.22392795\n",
      "Iteration 125, loss = 0.22378349\n",
      "Iteration 126, loss = 0.22377910\n",
      "Iteration 127, loss = 0.22318182\n",
      "Iteration 128, loss = 0.22236330\n",
      "Iteration 129, loss = 0.22310369\n",
      "Iteration 130, loss = 0.22189272\n",
      "Iteration 131, loss = 0.22164146\n",
      "Iteration 132, loss = 0.22160761\n",
      "Iteration 133, loss = 0.22108513\n",
      "Iteration 134, loss = 0.22252807\n",
      "Iteration 135, loss = 0.22157900\n",
      "Iteration 136, loss = 0.22057962\n",
      "Iteration 137, loss = 0.22165398\n",
      "Iteration 138, loss = 0.22186621\n",
      "Iteration 139, loss = 0.21914682\n",
      "Iteration 140, loss = 0.22018536\n",
      "Iteration 141, loss = 0.21936157\n",
      "Iteration 142, loss = 0.21880880\n",
      "Iteration 143, loss = 0.21784922\n",
      "Iteration 144, loss = 0.21824162\n",
      "Iteration 145, loss = 0.21845333\n",
      "Iteration 146, loss = 0.21822451\n",
      "Iteration 147, loss = 0.21811904\n",
      "Iteration 148, loss = 0.21746183\n",
      "Iteration 149, loss = 0.21838464\n",
      "Iteration 150, loss = 0.21769706\n",
      "Iteration 151, loss = 0.21803245\n",
      "Iteration 152, loss = 0.21656372\n",
      "Iteration 153, loss = 0.21626275\n",
      "Iteration 154, loss = 0.21561510\n",
      "Iteration 155, loss = 0.21508306\n",
      "Iteration 156, loss = 0.21559599\n",
      "Iteration 157, loss = 0.21456746\n",
      "Iteration 158, loss = 0.21469974\n",
      "Iteration 159, loss = 0.21666567\n",
      "Iteration 160, loss = 0.21577518\n",
      "Iteration 161, loss = 0.21484909\n",
      "Iteration 162, loss = 0.21529064\n",
      "Iteration 163, loss = 0.21417027\n",
      "Iteration 164, loss = 0.21447780\n",
      "Iteration 165, loss = 0.21326015\n",
      "Iteration 166, loss = 0.21440245\n",
      "Iteration 167, loss = 0.21368907\n",
      "Iteration 168, loss = 0.21290657\n",
      "Iteration 169, loss = 0.21380744\n",
      "Iteration 170, loss = 0.21421459\n",
      "Iteration 171, loss = 0.21238643\n",
      "Iteration 172, loss = 0.21267078\n",
      "Iteration 173, loss = 0.21285768\n",
      "Iteration 174, loss = 0.21241730\n",
      "Iteration 175, loss = 0.21106805\n",
      "Iteration 176, loss = 0.21171615\n",
      "Iteration 177, loss = 0.21152683\n",
      "Iteration 178, loss = 0.21245373\n",
      "Iteration 179, loss = 0.21088896\n",
      "Iteration 180, loss = 0.21051675\n",
      "Iteration 181, loss = 0.20968588\n",
      "Iteration 182, loss = 0.20968167\n",
      "Iteration 183, loss = 0.21004005\n",
      "Iteration 184, loss = 0.21057477\n",
      "Iteration 185, loss = 0.20999836\n",
      "Iteration 186, loss = 0.20939518\n",
      "Iteration 187, loss = 0.21014313\n",
      "Iteration 188, loss = 0.20895921\n",
      "Iteration 189, loss = 0.20857619\n",
      "Iteration 190, loss = 0.20947177\n",
      "Iteration 191, loss = 0.20834667\n",
      "Iteration 192, loss = 0.20755148\n",
      "Iteration 193, loss = 0.20829219\n",
      "Iteration 194, loss = 0.20880297\n",
      "Iteration 195, loss = 0.20717336\n",
      "Iteration 196, loss = 0.20910557\n",
      "Iteration 197, loss = 0.20833224\n",
      "Iteration 198, loss = 0.20701739\n",
      "Iteration 199, loss = 0.20648980\n",
      "Iteration 200, loss = 0.20771855\n",
      "Iteration 201, loss = 0.20645832\n",
      "Iteration 202, loss = 0.20617870\n",
      "Iteration 203, loss = 0.20749126\n",
      "Iteration 204, loss = 0.20783241\n",
      "Iteration 205, loss = 0.20605883\n",
      "Iteration 206, loss = 0.20561661\n",
      "Iteration 207, loss = 0.20532609\n",
      "Iteration 208, loss = 0.20626074\n",
      "Iteration 209, loss = 0.20580981\n",
      "Iteration 210, loss = 0.20554553\n",
      "Iteration 211, loss = 0.20456253\n",
      "Iteration 212, loss = 0.20580878\n",
      "Iteration 213, loss = 0.20471371\n",
      "Iteration 214, loss = 0.20521182\n",
      "Iteration 215, loss = 0.20432949\n",
      "Iteration 216, loss = 0.20330077\n",
      "Iteration 217, loss = 0.20414175\n",
      "Iteration 218, loss = 0.20329276\n",
      "Iteration 219, loss = 0.20571586\n",
      "Iteration 220, loss = 0.20362568\n",
      "Iteration 221, loss = 0.20335210\n",
      "Iteration 222, loss = 0.20261974\n",
      "Iteration 223, loss = 0.20377084\n",
      "Iteration 224, loss = 0.20323191\n",
      "Iteration 225, loss = 0.20283453\n",
      "Iteration 226, loss = 0.20151737\n",
      "Iteration 227, loss = 0.20174514\n",
      "Iteration 228, loss = 0.20151497\n",
      "Iteration 229, loss = 0.20101195\n",
      "Iteration 230, loss = 0.20188916\n",
      "Iteration 231, loss = 0.20147000\n",
      "Iteration 232, loss = 0.20251747\n",
      "Iteration 233, loss = 0.20141836\n",
      "Iteration 234, loss = 0.20166322\n",
      "Iteration 235, loss = 0.20153286\n",
      "Iteration 236, loss = 0.20176265\n",
      "Iteration 237, loss = 0.19987759\n",
      "Iteration 238, loss = 0.20050184\n",
      "Iteration 239, loss = 0.20052003\n",
      "Iteration 240, loss = 0.20049869\n",
      "Iteration 241, loss = 0.19975863\n",
      "Iteration 242, loss = 0.20034767\n",
      "Iteration 243, loss = 0.19867518\n",
      "Iteration 244, loss = 0.20068410\n",
      "Iteration 245, loss = 0.19943428\n",
      "Iteration 246, loss = 0.19975349\n",
      "Iteration 247, loss = 0.20015494\n",
      "Iteration 248, loss = 0.19979573\n",
      "Iteration 249, loss = 0.19946523\n",
      "Iteration 250, loss = 0.19897746\n",
      "Iteration 251, loss = 0.19925359\n",
      "Iteration 252, loss = 0.19951808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.19842409\n",
      "Iteration 254, loss = 0.19753229\n",
      "Iteration 255, loss = 0.19846978\n",
      "Iteration 256, loss = 0.19909455\n",
      "Iteration 257, loss = 0.19835120\n",
      "Iteration 258, loss = 0.19881358\n",
      "Iteration 259, loss = 0.19801677\n",
      "Iteration 260, loss = 0.19707134\n",
      "Iteration 261, loss = 0.19854853\n",
      "Iteration 262, loss = 0.19860782\n",
      "Iteration 263, loss = 0.19788023\n",
      "Iteration 264, loss = 0.19865773\n",
      "Iteration 265, loss = 0.19714293\n",
      "Iteration 266, loss = 0.19701137\n",
      "Iteration 267, loss = 0.19692665\n",
      "Iteration 268, loss = 0.19744973\n",
      "Iteration 269, loss = 0.19703660\n",
      "Iteration 270, loss = 0.19467217\n",
      "Iteration 271, loss = 0.19657876\n",
      "Iteration 272, loss = 0.19567877\n",
      "Iteration 273, loss = 0.19563419\n",
      "Iteration 274, loss = 0.19613500\n",
      "Iteration 275, loss = 0.19589444\n",
      "Iteration 276, loss = 0.19608934\n",
      "Iteration 277, loss = 0.19430079\n",
      "Iteration 278, loss = 0.19551273\n",
      "Iteration 279, loss = 0.19549576\n",
      "Iteration 280, loss = 0.19626400\n",
      "Iteration 281, loss = 0.19554901\n",
      "Iteration 282, loss = 0.19572990\n",
      "Iteration 283, loss = 0.19639772\n",
      "Iteration 284, loss = 0.19529082\n",
      "Iteration 285, loss = 0.19501082\n",
      "Iteration 286, loss = 0.19539906\n",
      "Iteration 287, loss = 0.19498297\n",
      "Iteration 288, loss = 0.19376963\n",
      "Iteration 289, loss = 0.19351079\n",
      "Iteration 290, loss = 0.19456376\n",
      "Iteration 291, loss = 0.19345196\n",
      "Iteration 292, loss = 0.19463132\n",
      "Iteration 293, loss = 0.19516005\n",
      "Iteration 294, loss = 0.19513725\n",
      "Iteration 295, loss = 0.19568034\n",
      "Iteration 296, loss = 0.19443570\n",
      "Iteration 297, loss = 0.19333051\n",
      "Iteration 298, loss = 0.19276021\n",
      "Iteration 299, loss = 0.19126203\n",
      "Iteration 300, loss = 0.19341484\n",
      "Iteration 301, loss = 0.19299016\n",
      "Iteration 302, loss = 0.19285352\n",
      "Iteration 303, loss = 0.19276001\n",
      "Iteration 304, loss = 0.19343406\n",
      "Iteration 305, loss = 0.19353687\n",
      "Iteration 306, loss = 0.19344984\n",
      "Iteration 307, loss = 0.19114994\n",
      "Iteration 308, loss = 0.19211286\n",
      "Iteration 309, loss = 0.19205899\n",
      "Iteration 310, loss = 0.19149805\n",
      "Iteration 311, loss = 0.19242139\n",
      "Iteration 312, loss = 0.19205099\n",
      "Iteration 313, loss = 0.19370177\n",
      "Iteration 314, loss = 0.19225279\n",
      "Iteration 315, loss = 0.19107115\n",
      "Iteration 316, loss = 0.19206959\n",
      "Iteration 317, loss = 0.19087721\n",
      "Iteration 318, loss = 0.19158809\n",
      "Iteration 319, loss = 0.19042418\n",
      "Iteration 320, loss = 0.19183107\n",
      "Iteration 321, loss = 0.19059257\n",
      "Iteration 322, loss = 0.19015539\n",
      "Iteration 323, loss = 0.19133481\n",
      "Iteration 324, loss = 0.19238248\n",
      "Iteration 325, loss = 0.19100516\n",
      "Iteration 326, loss = 0.19058975\n",
      "Iteration 327, loss = 0.18999659\n",
      "Iteration 328, loss = 0.19313988\n",
      "Iteration 329, loss = 0.19168889\n",
      "Iteration 330, loss = 0.19041795\n",
      "Iteration 331, loss = 0.19065930\n",
      "Iteration 332, loss = 0.18970304\n",
      "Iteration 333, loss = 0.18878226\n",
      "Iteration 334, loss = 0.18826789\n",
      "Iteration 335, loss = 0.18965376\n",
      "Iteration 336, loss = 0.18883522\n",
      "Iteration 337, loss = 0.18902415\n",
      "Iteration 338, loss = 0.18960753\n",
      "Iteration 339, loss = 0.18976251\n",
      "Iteration 340, loss = 0.18841538\n",
      "Iteration 341, loss = 0.18910464\n",
      "Iteration 342, loss = 0.18909869\n",
      "Iteration 343, loss = 0.18721527\n",
      "Iteration 344, loss = 0.18855656\n",
      "Iteration 345, loss = 0.18859919\n",
      "Iteration 346, loss = 0.18765422\n",
      "Iteration 347, loss = 0.18815919\n",
      "Iteration 348, loss = 0.18883732\n",
      "Iteration 349, loss = 0.18943854\n",
      "Iteration 350, loss = 0.18926199\n",
      "Iteration 351, loss = 0.18807904\n",
      "Iteration 352, loss = 0.18749253\n",
      "Iteration 353, loss = 0.18596415\n",
      "Iteration 354, loss = 0.18777596\n",
      "Iteration 355, loss = 0.18658150\n",
      "Iteration 356, loss = 0.18814816\n",
      "Iteration 357, loss = 0.18761303\n",
      "Iteration 358, loss = 0.18769317\n",
      "Iteration 359, loss = 0.18641982\n",
      "Iteration 360, loss = 0.18855013\n",
      "Iteration 361, loss = 0.18561371\n",
      "Iteration 362, loss = 0.18685259\n",
      "Iteration 363, loss = 0.18664529\n",
      "Iteration 364, loss = 0.18644432\n",
      "Iteration 365, loss = 0.18762284\n",
      "Iteration 366, loss = 0.18605039\n",
      "Iteration 367, loss = 0.18651685\n",
      "Iteration 368, loss = 0.18694535\n",
      "Iteration 369, loss = 0.18826621\n",
      "Iteration 370, loss = 0.18615122\n",
      "Iteration 371, loss = 0.18688139\n",
      "Iteration 372, loss = 0.18853808\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "MLP = MLPClassifier(verbose = True,\n",
    "                    max_iter = 1000,\n",
    "                    tol = 0.000010,\n",
    "                    solver = 'adam',\n",
    "                    hidden_layer_sizes = (100),\n",
    "                    activation = 'relu')\n",
    "MLP.fit(previsores_train, classe_train)\n",
    "\n",
    "previsoes = MLP.predict(previsores_test)\n",
    "\n",
    "MLP_accuracy = accuracy_score(classe_test, previsoes)\n",
    "matriz = confusion_matrix(classe_test, previsoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neurais com Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lindino/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "27676/27676 [==============================] - 26s 952us/step - loss: 0.3388 - accuracy: 0.8455\n",
      "Epoch 2/100\n",
      "27676/27676 [==============================] - 26s 957us/step - loss: 0.3190 - accuracy: 0.8540\n",
      "Epoch 3/100\n",
      "27676/27676 [==============================] - 26s 934us/step - loss: 0.3138 - accuracy: 0.8593\n",
      "Epoch 4/100\n",
      "27676/27676 [==============================] - 31s 1ms/step - loss: 0.3090 - accuracy: 0.8609\n",
      "Epoch 5/100\n",
      "27676/27676 [==============================] - 29s 1ms/step - loss: 0.3075 - accuracy: 0.8615\n",
      "Epoch 6/100\n",
      "27676/27676 [==============================] - 28s 1ms/step - loss: 0.3039 - accuracy: 0.8628\n",
      "Epoch 7/100\n",
      "27676/27676 [==============================] - 26s 951us/step - loss: 0.3032 - accuracy: 0.8646\n",
      "Epoch 8/100\n",
      "27676/27676 [==============================] - 27s 990us/step - loss: 0.3018 - accuracy: 0.8642\n",
      "Epoch 9/100\n",
      "27676/27676 [==============================] - 27s 984us/step - loss: 0.2999 - accuracy: 0.8658\n",
      "Epoch 10/100\n",
      "27676/27676 [==============================] - 28s 998us/step - loss: 0.3003 - accuracy: 0.8661\n",
      "Epoch 11/100\n",
      "27676/27676 [==============================] - 27s 984us/step - loss: 0.2983 - accuracy: 0.8682\n",
      "Epoch 12/100\n",
      "27676/27676 [==============================] - 27s 969us/step - loss: 0.2976 - accuracy: 0.8686\n",
      "Epoch 13/100\n",
      "27676/27676 [==============================] - 27s 985us/step - loss: 0.2947 - accuracy: 0.8679\n",
      "Epoch 14/100\n",
      "27676/27676 [==============================] - 27s 985us/step - loss: 0.2939 - accuracy: 0.8699\n",
      "Epoch 15/100\n",
      "27676/27676 [==============================] - 28s 1ms/step - loss: 0.2927 - accuracy: 0.8691\n",
      "Epoch 16/100\n",
      "27676/27676 [==============================] - 27s 968us/step - loss: 0.2897 - accuracy: 0.8714\n",
      "Epoch 17/100\n",
      "27676/27676 [==============================] - 27s 985us/step - loss: 0.2908 - accuracy: 0.8705\n",
      "Epoch 18/100\n",
      "27676/27676 [==============================] - 27s 991us/step - loss: 0.2905 - accuracy: 0.8727\n",
      "Epoch 19/100\n",
      "27676/27676 [==============================] - 28s 1ms/step - loss: 0.2968 - accuracy: 0.8718\n",
      "Epoch 20/100\n",
      "27676/27676 [==============================] - 27s 977us/step - loss: 0.2906 - accuracy: 0.8739\n",
      "Epoch 21/100\n",
      "27676/27676 [==============================] - 28s 1ms/step - loss: 0.2855 - accuracy: 0.8729\n",
      "Epoch 22/100\n",
      "27676/27676 [==============================] - 27s 985us/step - loss: 0.2853 - accuracy: 0.8745\n",
      "Epoch 23/100\n",
      "27676/27676 [==============================] - 27s 987us/step - loss: 0.2845 - accuracy: 0.8743\n",
      "Epoch 24/100\n",
      "27676/27676 [==============================] - 27s 987us/step - loss: 0.2880 - accuracy: 0.8742\n",
      "Epoch 25/100\n",
      "27676/27676 [==============================] - 28s 1ms/step - loss: 0.2837 - accuracy: 0.8750\n",
      "Epoch 26/100\n",
      "27676/27676 [==============================] - 27s 990us/step - loss: 0.2833 - accuracy: 0.8749\n",
      "Epoch 27/100\n",
      "27676/27676 [==============================] - 27s 980us/step - loss: 0.2847 - accuracy: 0.8764\n",
      "Epoch 28/100\n",
      "27676/27676 [==============================] - 28s 1ms/step - loss: 0.2831 - accuracy: 0.8747\n",
      "Epoch 29/100\n",
      "27676/27676 [==============================] - 28s 1ms/step - loss: 0.2836 - accuracy: 0.8764\n",
      "Epoch 30/100\n",
      "27676/27676 [==============================] - 28s 995us/step - loss: 0.2808 - accuracy: 0.8745\n",
      "Epoch 31/100\n",
      "27676/27676 [==============================] - 27s 966us/step - loss: 0.2793 - accuracy: 0.8772\n",
      "Epoch 32/100\n",
      "27676/27676 [==============================] - 27s 990us/step - loss: 0.2828 - accuracy: 0.8779\n",
      "Epoch 33/100\n",
      "27676/27676 [==============================] - 27s 969us/step - loss: 0.2783 - accuracy: 0.8770\n",
      "Epoch 34/100\n",
      "27676/27676 [==============================] - 27s 986us/step - loss: 0.2811 - accuracy: 0.8788\n",
      "Epoch 35/100\n",
      "27676/27676 [==============================] - 27s 967us/step - loss: 0.2752 - accuracy: 0.8777\n",
      "Epoch 36/100\n",
      "27676/27676 [==============================] - 27s 964us/step - loss: 0.2761 - accuracy: 0.8780\n",
      "Epoch 37/100\n",
      "27676/27676 [==============================] - 27s 970us/step - loss: 0.2752 - accuracy: 0.8790\n",
      "Epoch 38/100\n",
      "27676/27676 [==============================] - 27s 972us/step - loss: 0.2750 - accuracy: 0.8778\n",
      "Epoch 39/100\n",
      "27676/27676 [==============================] - 28s 994us/step - loss: 0.2747 - accuracy: 0.8783\n",
      "Epoch 40/100\n",
      "27676/27676 [==============================] - 28s 1ms/step - loss: 0.2705 - accuracy: 0.8803\n",
      "Epoch 41/100\n",
      "27676/27676 [==============================] - 27s 990us/step - loss: 0.2709 - accuracy: 0.8805\n",
      "Epoch 42/100\n",
      "27676/27676 [==============================] - 27s 988us/step - loss: 0.2719 - accuracy: 0.8807\n",
      "Epoch 43/100\n",
      "27676/27676 [==============================] - 27s 982us/step - loss: 0.2706 - accuracy: 0.8792\n",
      "Epoch 44/100\n",
      "27676/27676 [==============================] - 28s 997us/step - loss: 0.2678 - accuracy: 0.8818\n",
      "Epoch 45/100\n",
      "27676/27676 [==============================] - 30s 1ms/step - loss: 0.2693 - accuracy: 0.8805\n",
      "Epoch 46/100\n",
      "27676/27676 [==============================] - 29s 1ms/step - loss: 0.2702 - accuracy: 0.8815 0s - loss: 0.2702 - accuracy: 0.88\n",
      "Epoch 47/100\n",
      "27676/27676 [==============================] - 33s 1ms/step - loss: 0.2732 - accuracy: 0.8800\n",
      "Epoch 48/100\n",
      "27676/27676 [==============================] - 35s 1ms/step - loss: 0.2706 - accuracy: 0.8798\n",
      "Epoch 49/100\n",
      "27676/27676 [==============================] - 36s 1ms/step - loss: 0.2695 - accuracy: 0.8807\n",
      "Epoch 50/100\n",
      "27676/27676 [==============================] - 27s 967us/step - loss: 0.2661 - accuracy: 0.8807\n",
      "Epoch 51/100\n",
      "27676/27676 [==============================] - 23s 824us/step - loss: 0.2678 - accuracy: 0.8814\n",
      "Epoch 52/100\n",
      "27676/27676 [==============================] - 22s 780us/step - loss: 0.2634 - accuracy: 0.8820\n",
      "Epoch 53/100\n",
      "27676/27676 [==============================] - 26s 957us/step - loss: 0.2652 - accuracy: 0.8809\n",
      "Epoch 54/100\n",
      "27676/27676 [==============================] - 31s 1ms/step - loss: 0.2624 - accuracy: 0.8819\n",
      "Epoch 55/100\n",
      "27676/27676 [==============================] - 33s 1ms/step - loss: 0.2659 - accuracy: 0.8822\n",
      "Epoch 56/100\n",
      "27676/27676 [==============================] - 22s 788us/step - loss: 0.2711 - accuracy: 0.8820\n",
      "Epoch 57/100\n",
      "27676/27676 [==============================] - 23s 835us/step - loss: 0.2632 - accuracy: 0.8829\n",
      "Epoch 58/100\n",
      "27676/27676 [==============================] - 24s 881us/step - loss: 0.2640 - accuracy: 0.8836\n",
      "Epoch 59/100\n",
      "27676/27676 [==============================] - 22s 778us/step - loss: 0.2624 - accuracy: 0.8820\n",
      "Epoch 60/100\n",
      "27676/27676 [==============================] - 23s 837us/step - loss: 0.2604 - accuracy: 0.8832\n",
      "Epoch 61/100\n",
      "27676/27676 [==============================] - 24s 853us/step - loss: 0.2604 - accuracy: 0.8832\n",
      "Epoch 62/100\n",
      "27676/27676 [==============================] - 30s 1ms/step - loss: 0.2631 - accuracy: 0.8822\n",
      "Epoch 63/100\n",
      "27676/27676 [==============================] - 30s 1ms/step - loss: 0.2574 - accuracy: 0.8833\n",
      "Epoch 64/100\n",
      "27676/27676 [==============================] - 25s 888us/step - loss: 0.2592 - accuracy: 0.8847\n",
      "Epoch 65/100\n",
      "27676/27676 [==============================] - 30s 1ms/step - loss: 0.2597 - accuracy: 0.8833\n",
      "Epoch 66/100\n",
      "27676/27676 [==============================] - 29s 1ms/step - loss: 0.2571 - accuracy: 0.8841\n",
      "Epoch 67/100\n",
      "27676/27676 [==============================] - 33s 1ms/step - loss: 0.2595 - accuracy: 0.8856\n",
      "Epoch 68/100\n",
      "27676/27676 [==============================] - 26s 947us/step - loss: 0.2600 - accuracy: 0.8849\n",
      "Epoch 69/100\n",
      "27676/27676 [==============================] - 26s 924us/step - loss: 0.2550 - accuracy: 0.8834\n",
      "Epoch 70/100\n",
      "27676/27676 [==============================] - 28s 1ms/step - loss: 0.2566 - accuracy: 0.8852\n",
      "Epoch 71/100\n",
      "27676/27676 [==============================] - 28s 1ms/step - loss: 0.2522 - accuracy: 0.8843\n",
      "Epoch 72/100\n",
      "27676/27676 [==============================] - 28s 1ms/step - loss: 0.2513 - accuracy: 0.8847\n",
      "Epoch 73/100\n",
      "27676/27676 [==============================] - 23s 826us/step - loss: 0.2544 - accuracy: 0.8835\n",
      "Epoch 74/100\n",
      "27676/27676 [==============================] - 23s 830us/step - loss: 0.2510 - accuracy: 0.8850\n",
      "Epoch 75/100\n",
      "27676/27676 [==============================] - 23s 839us/step - loss: 0.2502 - accuracy: 0.8843\n",
      "Epoch 76/100\n",
      "27676/27676 [==============================] - 22s 802us/step - loss: 0.2505 - accuracy: 0.8841\n",
      "Epoch 77/100\n",
      "27676/27676 [==============================] - 22s 794us/step - loss: 0.2508 - accuracy: 0.8847\n",
      "Epoch 78/100\n",
      "27676/27676 [==============================] - 22s 794us/step - loss: 0.2519 - accuracy: 0.8866\n",
      "Epoch 79/100\n",
      "27676/27676 [==============================] - 22s 788us/step - loss: 0.2514 - accuracy: 0.8859\n",
      "Epoch 80/100\n",
      "27676/27676 [==============================] - 22s 792us/step - loss: 0.2500 - accuracy: 0.8866\n",
      "Epoch 81/100\n",
      "27676/27676 [==============================] - 22s 780us/step - loss: 0.2515 - accuracy: 0.8861\n",
      "Epoch 82/100\n",
      "27676/27676 [==============================] - 22s 804us/step - loss: 0.2470 - accuracy: 0.8860\n",
      "Epoch 83/100\n",
      "27676/27676 [==============================] - 22s 790us/step - loss: 0.2502 - accuracy: 0.8860\n",
      "Epoch 84/100\n",
      "27676/27676 [==============================] - 22s 793us/step - loss: 0.2468 - accuracy: 0.8861\n",
      "Epoch 85/100\n",
      "27676/27676 [==============================] - 22s 779us/step - loss: 0.2482 - accuracy: 0.8875\n",
      "Epoch 86/100\n",
      "27676/27676 [==============================] - 22s 787us/step - loss: 0.2494 - accuracy: 0.8875\n",
      "Epoch 87/100\n",
      "27676/27676 [==============================] - 22s 783us/step - loss: 0.2467 - accuracy: 0.8875\n",
      "Epoch 88/100\n",
      "27676/27676 [==============================] - 21s 771us/step - loss: 0.2482 - accuracy: 0.8870\n",
      "Epoch 89/100\n",
      "27676/27676 [==============================] - 21s 773us/step - loss: 0.2455 - accuracy: 0.8873\n",
      "Epoch 90/100\n",
      "27676/27676 [==============================] - 21s 767us/step - loss: 0.2454 - accuracy: 0.8865\n",
      "Epoch 91/100\n",
      "27676/27676 [==============================] - 21s 771us/step - loss: 0.2536 - accuracy: 0.8874\n",
      "Epoch 92/100\n",
      "27676/27676 [==============================] - 21s 769us/step - loss: 0.2439 - accuracy: 0.8881\n",
      "Epoch 93/100\n",
      "27676/27676 [==============================] - 22s 780us/step - loss: 0.2445 - accuracy: 0.8894\n",
      "Epoch 94/100\n",
      "27676/27676 [==============================] - 21s 772us/step - loss: 0.2462 - accuracy: 0.8870\n",
      "Epoch 95/100\n",
      "27676/27676 [==============================] - 21s 773us/step - loss: 0.2511 - accuracy: 0.8883\n",
      "Epoch 96/100\n",
      "27676/27676 [==============================] - 21s 770us/step - loss: 0.2453 - accuracy: 0.8872\n",
      "Epoch 97/100\n",
      "27676/27676 [==============================] - 21s 772us/step - loss: 0.2463 - accuracy: 0.8887\n",
      "Epoch 98/100\n",
      "27676/27676 [==============================] - 22s 787us/step - loss: 0.2427 - accuracy: 0.8868\n",
      "Epoch 99/100\n",
      "27676/27676 [==============================] - 22s 798us/step - loss: 0.2451 - accuracy: 0.8874\n",
      "Epoch 100/100\n",
      "27676/27676 [==============================] - 22s 795us/step - loss: 0.2441 - accuracy: 0.8888\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "MLP_keras = Sequential()\n",
    "MLP_keras.add(Dense(units = 55, activation = 'relu', input_dim = 108))\n",
    "MLP_keras.add(Dense(units = 55, activation = 'relu'))\n",
    "MLP_keras.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "MLP_keras.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "MLP_keras.fit(previsores_train, classe_train, batch_size = 1, nb_epoch = 100)\n",
    "\n",
    "previsoes = MLP.predict(previsores_test)\n",
    "previsoes = (previsoes > 0.5)\n",
    "\n",
    "MLP_keras_accuracy = accuracy_score(classe_test, previsoes)\n",
    "matriz = confusion_matrix(classe_test, previsoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mBase Line\u001b[0;0m: \u001b[1;32m0.7560%\u001b[0;0m\n",
      "\u001b[1mNaive Bayes\u001b[0;0m: \u001b[1;32m0.4768%\u001b[0;0m\n",
      "\u001b[1mDecision Tree\u001b[0;0m: \u001b[1;32m0.8104%\u001b[0;0m\n",
      "\u001b[1mRandom Forest\u001b[0;0m: \u001b[1;32m0.8477%\u001b[0;0m\n",
      "\u001b[1mKNN\u001b[0;0m: \u001b[1;32m0.8223%\u001b[0;0m\n",
      "\u001b[1mLogistic Regression\u001b[0;0m: \u001b[1;32m0.8495%\u001b[0;0m\n",
      "\u001b[1mSVM\u001b[0;0m: \u001b[1;32m0.8508%\u001b[0;0m\n",
      "\u001b[1mMLP\u001b[0;0m: \u001b[1;32m0.8197%\u001b[0;0m\n",
      "\u001b[1mMLP_Keras\u001b[0;0m: \u001b[1;32m0.8197%\u001b[0;0m\n"
     ]
    }
   ],
   "source": [
    "GREEN = \"\\033[1;32m\" \n",
    "RESET = '\\033[0;0m'\n",
    "NEGRITO = '\\033[1m'\n",
    "models = {'Base Line': base_line,'Naive Bayes': Naive_Bayes_accuracy, 'Decision Tree': Decision_Tree_accuracy,\n",
    "          'Random Forest': Random_Forest_accuracy, 'KNN': KNN_accuracy, 'Logistic Regression': Logistic_Regression_accuracy,\n",
    "          'SVM': SVM_accuracy, 'MLP': MLP_accuracy, 'MLP_Keras': MLP_keras_accuracy}\n",
    "\n",
    "for model in models:\n",
    "    print('{}{}{}: {}{:.4f}%{}'.format(NEGRITO,model,RESET,GREEN,models[model], RESET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
